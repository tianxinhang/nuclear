{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexhang/project/Nuclear'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kinf_1</th>\n",
       "      <th>kinf_2</th>\n",
       "      <th>kinf_3</th>\n",
       "      <th>kinf_4</th>\n",
       "      <th>kinf_5</th>\n",
       "      <th>kinf_6</th>\n",
       "      <th>kinf_7</th>\n",
       "      <th>kinf_8</th>\n",
       "      <th>kinf_9</th>\n",
       "      <th>kinf_10</th>\n",
       "      <th>...</th>\n",
       "      <th>NODE2DBU_95</th>\n",
       "      <th>NODE2DBU_96</th>\n",
       "      <th>NODE2DBU_97</th>\n",
       "      <th>NODE2DBU_98</th>\n",
       "      <th>NODE2DBU_99</th>\n",
       "      <th>NODE2DBU_100</th>\n",
       "      <th>NODE2DBU_101</th>\n",
       "      <th>NODE2DBU_102</th>\n",
       "      <th>NODE2DBU_103</th>\n",
       "      <th>NODE2DBU_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.16708</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.18090</td>\n",
       "      <td>1.16216</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.08632</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26837.0</td>\n",
       "      <td>26864.0</td>\n",
       "      <td>26834.0</td>\n",
       "      <td>26862.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1.15988</td>\n",
       "      <td>1.07458</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.15534</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>1.19730</td>\n",
       "      <td>1.15417</td>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.20422</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.06296</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1.06622</td>\n",
       "      <td>1.16655</td>\n",
       "      <td>1.20028</td>\n",
       "      <td>1.19485</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15935</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>43835.0</td>\n",
       "      <td>39276.0</td>\n",
       "      <td>42082.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         kinf_1   kinf_2   kinf_3   kinf_4  kinf_5  kinf_6   kinf_7  kinf_8  \\\n",
       "0       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "1       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "2       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "3       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "4       1.16708  1.08099  1.18090  1.16216  1.4279  1.1821  1.08632  1.4279   \n",
       "...         ...      ...      ...      ...     ...     ...      ...     ...   \n",
       "119995  1.15988  1.07458  1.09861  1.15534  1.4279  1.1821  1.07427  1.4279   \n",
       "119996  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119997  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119998  1.19730  1.15417  1.21509  1.20422  1.4279  1.1821  1.06296  1.4279   \n",
       "119999  1.06622  1.16655  1.20028  1.19485  1.4279  1.1821  1.15935  1.4279   \n",
       "\n",
       "        kinf_9  kinf_10  ...  NODE2DBU_95  NODE2DBU_96  NODE2DBU_97  \\\n",
       "0       1.1821   1.1821  ...          0.0          0.0      36445.0   \n",
       "1       1.1821   1.1821  ...          0.0          0.0      36809.0   \n",
       "2       1.1821   1.1821  ...          0.0          0.0      31836.0   \n",
       "3       1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "4       1.1821   1.1821  ...          0.0          0.0      26837.0   \n",
       "...        ...      ...  ...          ...          ...          ...   \n",
       "119995  1.1821   1.1821  ...          0.0          0.0      26191.0   \n",
       "119996  1.1821   1.1821  ...          0.0          0.0      43151.0   \n",
       "119997  1.1821   1.1821  ...          0.0          0.0      43091.0   \n",
       "119998  1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "119999  1.1821   1.1821  ...          0.0          0.0      42090.0   \n",
       "\n",
       "        NODE2DBU_98  NODE2DBU_99  NODE2DBU_100  NODE2DBU_101  NODE2DBU_102  \\\n",
       "0           31803.0      36809.0       31836.0       20806.0       20806.0   \n",
       "1           36445.0      31836.0       31803.0       20806.0       20806.0   \n",
       "2           36809.0      31803.0       36445.0       20806.0       20806.0   \n",
       "3           31836.0      36445.0       36809.0       20806.0       20806.0   \n",
       "4           26864.0      26834.0       26862.0       22008.0       22008.0   \n",
       "...             ...          ...           ...           ...           ...   \n",
       "119995      26191.0      26200.0       26200.0       36146.0       36146.0   \n",
       "119996      45258.0      39658.0       43091.0       36186.0       36186.0   \n",
       "119997      39658.0      45258.0       43151.0       36186.0       36186.0   \n",
       "119998      31836.0      36445.0       36809.0       25238.0       25238.0   \n",
       "119999      43835.0      39276.0       42082.0       21568.0       21568.0   \n",
       "\n",
       "        NODE2DBU_103  NODE2DBU_104  \n",
       "0            20806.0       20806.0  \n",
       "1            20806.0       20806.0  \n",
       "2            20806.0       20806.0  \n",
       "3            20806.0       20806.0  \n",
       "4            22008.0       22008.0  \n",
       "...              ...           ...  \n",
       "119995       36146.0       36146.0  \n",
       "119996       36186.0       36186.0  \n",
       "119997       36186.0       36186.0  \n",
       "119998       25238.0       25238.0  \n",
       "119999       21568.0       21568.0  \n",
       "\n",
       "[120000 rows x 156 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入处理好的数据\n",
    "file_base_name = 'nuclear_burnup_data_20201215.csv'\n",
    "file_input_name = 'df.csv'\n",
    "\n",
    "pre_data_X = pd.read_csv(os.path.join(path, file_input_name), index_col=0)\n",
    "pre_data_base = pd.read_csv(os.path.join(path, file_base_name))\n",
    "pre_data_X.columns.values.tolist()  \n",
    "pre_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAssBurnupCal</th>\n",
       "      <th>MaxPinBurnupCal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56624</td>\n",
       "      <td>62467.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56812</td>\n",
       "      <td>61980.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56724</td>\n",
       "      <td>62521.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56537</td>\n",
       "      <td>62195.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57424</td>\n",
       "      <td>64025.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>57359</td>\n",
       "      <td>63599.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>59562</td>\n",
       "      <td>64865.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>59631</td>\n",
       "      <td>63716.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>61714</td>\n",
       "      <td>65601.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>61843</td>\n",
       "      <td>66948.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAssBurnupCal  MaxPinBurnupCal\n",
       "0                 56624          62467.5\n",
       "1                 56812          61980.1\n",
       "2                 56724          62521.4\n",
       "3                 56537          62195.7\n",
       "4                 57424          64025.7\n",
       "...                 ...              ...\n",
       "119995            57359          63599.1\n",
       "119996            59562          64865.4\n",
       "119997            59631          63716.2\n",
       "119998            61714          65601.8\n",
       "119999            61843          66948.3\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_y = pre_data_base.iloc[:,-2:]\n",
    "pre_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = pd.concat([pre_data_X, pre_data_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 156)\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pre_data_array = pre_data.values\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "pre_data_Normalized = mm.fit_transform(pre_data_array)\n",
    "pre_data_Normalized = pre_data_Normalized[:,:-2]\n",
    "# pre_data_y = pre_data_Normalized[:,-2:]\n",
    "print(pre_data_Normalized.shape)\n",
    "print(pre_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "map1 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [2,6,11,16,20,23,25,26,25,23,20,16,11,6,2],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0]\n",
    "    \n",
    "]\n",
    "m = np.array(map1)\n",
    "print(m.shape)\n",
    "map2 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['2/0','2/2',\"6/0\",\"6/2\",'11/0','11/2','16/0','16/2',\"20/0\",'20/2','23/0','23/2','25/0','25/2','26/0','26/1',\"25/2\",'25/0','23/2','23/0','20/2','20/0','16/2','16/0','11/2','11/0','6/2','6/0','2/2','2/0'],\n",
    "    ['2/1','2/3',\"6/1\",\"6/3\",'11/1','11/3','16/1','16/3',\"20/1\",'20/3','23/1','23/3','25/1','25/3','26/2','26/3',\"25/3\",'25/1','23/3','23/1','20/3','20/1','16/3','16/1','11/3','11/1','6/3','6/1','2/3','2/1'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "      \n",
    "]\n",
    "m = np.array(map2)\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(a,alist):\n",
    "    res = []\n",
    "    for i in range(len(alist)):\n",
    "        for j in range(len(alist[i])):\n",
    "            if alist[i][j] == a:\n",
    "                res.append((i,j))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "#排布input\n",
    "def search(a,alist):\n",
    "    res = []\n",
    "    for i in range(len(alist)):\n",
    "        for j in range(len(alist[i])):\n",
    "            if alist[i][j] == a:\n",
    "                res.append((i,j))\n",
    "    return res\n",
    "new = []\n",
    "for i,item in enumerate(pre_data_Normalized):\n",
    "    print(\"正在处理第 \"+i+' 个图像..')\n",
    "    layer1 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "    layer2 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "    layer3 = [[0 for _ in range(30)] for _ in range(30)]\n",
    "#     print(item)\n",
    "    for j in range(item.size): #len = 156\n",
    "        if j < 26:\n",
    "            res = search(j+1,map1)\n",
    "            for u in res:\n",
    "                h,w = u[0],u[1]\n",
    "                layer1[h][w] = item[j]\n",
    "        elif j < 52:\n",
    "            j_ = j-26\n",
    "            res = search(j_+1,map1)\n",
    "            for u in res:\n",
    "                h,w = u[0],u[1]\n",
    "                layer2[h][w] = item[j]\n",
    "            \n",
    "        else:\n",
    "            j_ = j-52\n",
    "            number = str(j_ // 4 + 1)\n",
    "            corner = str(j_ % 4)\n",
    "            number_corner = number + '/' + corner\n",
    "            res = search(number_corner,map2)\n",
    "            for u in res:\n",
    "                h,w = u[0],u[1]\n",
    "                layer3[h][w] = item[j]\n",
    "    layer1_array = np.array(layer1)\n",
    "    layer2_array = np.array(layer2)\n",
    "    layer3_array = np.array(layer3)\n",
    "    \n",
    "    #从1*1扩展成2*2\n",
    "    layer1_array = np.repeat(layer1_array,2,axis = 1) \n",
    "    layer1_array = np.repeat(layer1_array,2,axis = 0)\n",
    "    layer2_array = np.repeat(layer2_array,2,axis = 1)\n",
    "    layer2_array = np.repeat(layer2_array,2,axis = 0)\n",
    "    \n",
    "    #channel 拼接\n",
    "    tmp = np.stack((layer1_array,layer2_array,layer3_array),axis = 2)\n",
    "    new.append(tmp)\n",
    "\n",
    "pre_data_x = np.array(new)\n",
    "print(pre_data_x.shape)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'a']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,\"a\"]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 6, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "# pre_data_x = pre_data_x.reshape((pre_data_x.shape[0], pre_data_x.shape[1], pre_data_x.shape[2], 1))\n",
    "# print(pre_data_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 30, 30, 3)\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(pre_data_x.shape)\n",
    "print(pre_data_y.shape)\n",
    "pre_data_y = np.array(pre_data_y)\n",
    "data = (pre_data_x,pre_data_y)\n",
    "# pre_data_y.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 12 16:56:10 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    Off  | 00000000:17:00.0 Off |                  N/A |\r\n",
      "| 29%   34C    P8     8W / 175W |   1116MiB /  7982MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     13674      C   /home/adt/anaconda3/bin/python              1105MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 16)        208       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 15, 15, 64)        8256      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              14401000  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 14,511,745\n",
      "Trainable params: 14,511,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.applications.resnet18 import ResNet18\n",
    "# from keras.applications.resnet18 import preprocess_input as preprocess_input_resnet\n",
    "import keras\n",
    "def deeper_conv2D(h,w):\n",
    "    new_model = keras.Sequential()\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=(2,2), padding=\"same\", activation=\"relu\", \\\n",
    "                                         input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "    # Flatten will take our convolution filters and lay them out end to end so our dense layer can predict based on the outcomes of each\n",
    "    new_model.add(tf.keras.layers.Flatten())\n",
    "    new_model.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
    "    new_model.add(tf.keras.layers.Dense(100))\n",
    "    new_model.add(tf.keras.layers.Dense(1))\n",
    "    new_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")    \n",
    "    return new_model\n",
    "m = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "seed = 2020\n",
    "x_pre_train, x_test, y_pre_train, y_test = train_test_split(pre_data_x, pre_data_y, \n",
    "                                                           random_state=seed, train_size=0.9, \n",
    "                                                           test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3375/3375 [==============================] - 204s 61ms/step - loss: 27701910.0000\n",
      "Epoch 2/150\n",
      "3375/3375 [==============================] - 271s 80ms/step - loss: 2284708.5000\n",
      "Epoch 3/150\n",
      "3375/3375 [==============================] - 270s 80ms/step - loss: 1823101.6250\n",
      "Epoch 4/150\n",
      "3375/3375 [==============================] - 270s 80ms/step - loss: 1329517.2500\n",
      "Epoch 5/150\n",
      "3375/3375 [==============================] - 269s 80ms/step - loss: 1017234.0625\n",
      "Epoch 6/150\n",
      "3375/3375 [==============================] - 268s 80ms/step - loss: 814674.1250\n",
      "Epoch 7/150\n",
      "3375/3375 [==============================] - 265s 79ms/step - loss: 575187.5000\n",
      "Epoch 8/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 397913.4062\n",
      "Epoch 9/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 277159.1562\n",
      "Epoch 10/150\n",
      "3375/3375 [==============================] - 265s 79ms/step - loss: 233927.9062\n",
      "Epoch 11/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 205184.3281\n",
      "Epoch 12/150\n",
      "3375/3375 [==============================] - 268s 79ms/step - loss: 174534.6250\n",
      "Epoch 13/150\n",
      "3375/3375 [==============================] - 269s 80ms/step - loss: 160603.0938\n",
      "Epoch 14/150\n",
      "3375/3375 [==============================] - 269s 80ms/step - loss: 139212.6406\n",
      "Epoch 15/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 127923.4688\n",
      "Epoch 16/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 115649.4219\n",
      "Epoch 17/150\n",
      "3375/3375 [==============================] - 268s 79ms/step - loss: 105105.5469\n",
      "Epoch 18/150\n",
      "3375/3375 [==============================] - 269s 80ms/step - loss: 97976.0859\n",
      "Epoch 19/150\n",
      "3375/3375 [==============================] - 269s 80ms/step - loss: 91187.8516\n",
      "Epoch 20/150\n",
      "3375/3375 [==============================] - 267s 79ms/step - loss: 85817.2422\n",
      "Epoch 21/150\n",
      "3375/3375 [==============================] - 268s 79ms/step - loss: 81028.1406\n",
      "Epoch 22/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 76538.5156\n",
      "Epoch 23/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 71257.3594\n",
      "Epoch 24/150\n",
      "3375/3375 [==============================] - 268s 79ms/step - loss: 67311.6875\n",
      "Epoch 25/150\n",
      "3375/3375 [==============================] - 269s 80ms/step - loss: 63685.4570\n",
      "Epoch 26/150\n",
      "3375/3375 [==============================] - 268s 79ms/step - loss: 60198.5820\n",
      "Epoch 27/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 57739.9844\n",
      "Epoch 28/150\n",
      "3375/3375 [==============================] - 265s 78ms/step - loss: 54670.6680\n",
      "Epoch 29/150\n",
      "3375/3375 [==============================] - 265s 79ms/step - loss: 54842.5195\n",
      "Epoch 30/150\n",
      "3375/3375 [==============================] - 264s 78ms/step - loss: 53367.8047\n",
      "Epoch 31/150\n",
      "3375/3375 [==============================] - 265s 78ms/step - loss: 51696.7500\n",
      "Epoch 32/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 47794.4219\n",
      "Epoch 33/150\n",
      "3375/3375 [==============================] - 267s 79ms/step - loss: 47577.8672\n",
      "Epoch 34/150\n",
      "3375/3375 [==============================] - 266s 79ms/step - loss: 46667.7539\n",
      "Epoch 35/150\n",
      "3375/3375 [==============================] - 212s 63ms/step - loss: 45167.7734\n",
      "Epoch 36/150\n",
      "3375/3375 [==============================] - 199s 59ms/step - loss: 46397.5781\n",
      "Epoch 37/150\n",
      "3375/3375 [==============================] - 196s 58ms/step - loss: 43850.4453\n",
      "Epoch 38/150\n",
      "3375/3375 [==============================] - 197s 58ms/step - loss: 41318.5820\n",
      "Epoch 39/150\n",
      "3375/3375 [==============================] - 199s 59ms/step - loss: 39201.0508\n",
      "Epoch 40/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 42046.7109\n",
      "Epoch 41/150\n",
      "3375/3375 [==============================] - 201s 60ms/step - loss: 38807.4766\n",
      "Epoch 42/150\n",
      "3375/3375 [==============================] - 204s 60ms/step - loss: 37938.4141\n",
      "Epoch 43/150\n",
      "3375/3375 [==============================] - 202s 60ms/step - loss: 38970.9336\n",
      "Epoch 44/150\n",
      "3375/3375 [==============================] - 203s 60ms/step - loss: 37599.8203\n",
      "Epoch 45/150\n",
      "3375/3375 [==============================] - 202s 60ms/step - loss: 39250.2695\n",
      "Epoch 46/150\n",
      "3375/3375 [==============================] - 203s 60ms/step - loss: 36081.6133\n",
      "Epoch 47/150\n",
      "3375/3375 [==============================] - 202s 60ms/step - loss: 34752.4531\n",
      "Epoch 48/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 33354.8906\n",
      "Epoch 49/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 34966.0117\n",
      "Epoch 50/150\n",
      "3375/3375 [==============================] - 201s 59ms/step - loss: 33425.4141\n",
      "Epoch 51/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 33217.2773\n",
      "Epoch 52/150\n",
      "3375/3375 [==============================] - 201s 59ms/step - loss: 31728.6465\n",
      "Epoch 53/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 31904.7988\n",
      "Epoch 54/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 30922.0664\n",
      "Epoch 55/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 31539.3770\n",
      "Epoch 56/150\n",
      "3375/3375 [==============================] - 200s 59ms/step - loss: 30173.9180\n",
      "Epoch 57/150\n",
      "3375/3375 [==============================] - 198s 59ms/step - loss: 29373.1348\n",
      "Epoch 58/150\n",
      "3375/3375 [==============================] - 199s 59ms/step - loss: 28627.0742\n",
      "Epoch 59/150\n",
      "3375/3375 [==============================] - 199s 59ms/step - loss: 28736.4062\n",
      "Epoch 60/150\n",
      "3375/3375 [==============================] - 198s 59ms/step - loss: 29485.5625\n",
      "Epoch 61/150\n",
      "3375/3375 [==============================] - 202s 60ms/step - loss: 29023.6914\n",
      "Epoch 62/150\n",
      "3375/3375 [==============================] - 201s 60ms/step - loss: 27213.0664\n",
      "Epoch 63/150\n",
      "3375/3375 [==============================] - 201s 60ms/step - loss: 27534.5664\n",
      "Epoch 64/150\n",
      "3375/3375 [==============================] - 201s 60ms/step - loss: 25253.4668\n",
      "Epoch 65/150\n",
      "3375/3375 [==============================] - 276s 82ms/step - loss: 25179.6875\n",
      "Epoch 66/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 25588.9180\n",
      "Epoch 67/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 25740.5820\n",
      "Epoch 68/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 24795.4258\n",
      "Epoch 69/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 23685.9258\n",
      "Epoch 70/150\n",
      "3375/3375 [==============================] - 296s 88ms/step - loss: 24565.4355\n",
      "Epoch 71/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 24148.6797\n",
      "Epoch 72/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 22267.2754\n",
      "Epoch 73/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 24542.0176\n",
      "Epoch 74/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 22274.9043\n",
      "Epoch 75/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 22933.8379\n",
      "Epoch 76/150\n",
      "3375/3375 [==============================] - 296s 88ms/step - loss: 21574.3496\n",
      "Epoch 77/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 21439.1719\n",
      "Epoch 78/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 21517.2441\n",
      "Epoch 79/150\n",
      "3375/3375 [==============================] - 299s 88ms/step - loss: 20933.5918\n",
      "Epoch 80/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 20829.6504\n",
      "Epoch 81/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 20903.1465\n",
      "Epoch 82/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 19889.7715\n",
      "Epoch 83/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 21832.4336\n",
      "Epoch 84/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 19495.3320\n",
      "Epoch 85/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 20443.8613\n",
      "Epoch 86/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 18416.6387\n",
      "Epoch 87/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 19194.0547\n",
      "Epoch 88/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 19790.3301\n",
      "Epoch 89/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 19255.0117\n",
      "Epoch 90/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 18710.9336\n",
      "Epoch 91/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 18522.7422\n",
      "Epoch 92/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 18018.9863\n",
      "Epoch 93/150\n",
      "3375/3375 [==============================] - 299s 88ms/step - loss: 18181.8594\n",
      "Epoch 94/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 17622.9434\n",
      "Epoch 95/150\n",
      "3375/3375 [==============================] - 299s 88ms/step - loss: 18025.9609\n",
      "Epoch 96/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 17340.3301\n",
      "Epoch 97/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 17887.8418\n",
      "Epoch 98/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 16736.3145\n",
      "Epoch 99/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 17304.0352\n",
      "Epoch 100/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 16934.7168\n",
      "Epoch 101/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 17064.4727\n",
      "Epoch 102/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 16836.5977\n",
      "Epoch 103/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 16862.9863\n",
      "Epoch 104/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 16541.1914\n",
      "Epoch 105/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 15659.1904\n",
      "Epoch 106/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 16756.3203\n",
      "Epoch 107/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 14962.7012\n",
      "Epoch 108/150\n",
      "3375/3375 [==============================] - 302s 89ms/step - loss: 16020.9863\n",
      "Epoch 109/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 15744.7080\n",
      "Epoch 110/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 15835.7617\n",
      "Epoch 111/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 16116.8252\n",
      "Epoch 112/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 15457.7607\n",
      "Epoch 113/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 15158.7803\n",
      "Epoch 114/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 14735.8799\n",
      "Epoch 115/150\n",
      "3375/3375 [==============================] - 302s 89ms/step - loss: 15065.6123\n",
      "Epoch 116/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 15079.4727\n",
      "Epoch 117/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 14845.9971\n",
      "Epoch 118/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 13773.6631\n",
      "Epoch 119/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 14503.8740\n",
      "Epoch 120/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 15101.2646\n",
      "Epoch 121/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 14572.7031\n",
      "Epoch 122/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 15329.7734\n",
      "Epoch 123/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 13931.1533\n",
      "Epoch 124/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 14125.2598\n",
      "Epoch 125/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 13977.0547\n",
      "Epoch 126/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 14373.1836\n",
      "Epoch 127/150\n",
      "3375/3375 [==============================] - 299s 88ms/step - loss: 13655.0674\n",
      "Epoch 128/150\n",
      "3375/3375 [==============================] - 297s 88ms/step - loss: 13523.4863\n",
      "Epoch 129/150\n",
      "3375/3375 [==============================] - 303s 90ms/step - loss: 13536.5283\n",
      "Epoch 130/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 13522.7617\n",
      "Epoch 131/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 13224.2334\n",
      "Epoch 132/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 13914.2129\n",
      "Epoch 133/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 12686.9756\n",
      "Epoch 134/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 13461.3594\n",
      "Epoch 135/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 13681.2461\n",
      "Epoch 136/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 12690.6523\n",
      "Epoch 137/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 13282.1914\n",
      "Epoch 138/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 12248.0879\n",
      "Epoch 139/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 12994.2510\n",
      "Epoch 140/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 13686.9814\n",
      "Epoch 141/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 13543.2773\n",
      "Epoch 142/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 12113.3916\n",
      "Epoch 143/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 12679.9238\n",
      "Epoch 144/150\n",
      "3375/3375 [==============================] - 298s 88ms/step - loss: 12046.3096\n",
      "Epoch 145/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 12807.1660\n",
      "Epoch 146/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 12558.1348\n",
      "Epoch 147/150\n",
      "3375/3375 [==============================] - 299s 89ms/step - loss: 11722.6289\n",
      "Epoch 148/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 11966.4189\n",
      "Epoch 149/150\n",
      "3375/3375 [==============================] - 301s 89ms/step - loss: 12904.3672\n",
      "Epoch 150/150\n",
      "3375/3375 [==============================] - 300s 89ms/step - loss: 12159.8848\n"
     ]
    }
   ],
   "source": [
    "model = m.fit(x_pre_train,  y_pre_train[:,0], epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57732., 49679., 54567., ..., 57982., 57080., 56720.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:,0]\n",
    "# array([57732., 49679., 54567., ..., 57982., 57080., 56720.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f01705321d0>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAWUUlEQVR4nO3df7DddX3n8ef7e26IAWwBcwuYEIOW6oJbCs0gaNdhu6JAGZjO2mkYR2trJ7MubnWHmR3RGer6R3fddu1qUShVaukw0CmyNjJBS4VZabdlSVh+QySKSpjQBNAAm2C4ue/94/u9955f90du7s05n+PzMXMm5/v9fs457/km53U++Xw/3+83MhNJUvmqQRcgSVoaBrokjQgDXZJGhIEuSSPCQJekEWGgS9KIGGigR8QNEbE7Ih5ZQNs/jogHmsd3IuLHR6JGSSpFDHIeekS8E3gZuDEz33oIr/sPwFmZ+TvLVpwkFWagPfTM/DbwQvu6iHhTRHwjIrZFxD0R8ZY+L70cuPmIFClJhRgbdAF9XA/8u8x8MiLeBnwR+NWpjRHxBuBU4K4B1SdJQ2moAj0ijgXeDvx1REytXtnVbCNwa2YePJK1SdKwG6pApx4C+nFm/tIcbTYCVxyheiSpGEM1bTEzXwSeiojfAIjamVPbm/H044F/HFCJkjS0Bj1t8WbqcH5zROyMiA8B7wM+FBEPAo8Cl7W9ZCNwS3qJSEnqMdBpi5KkpTNUQy6SpMUb2EHR1atX5/r16wf18ZJUpG3btj2XmeP9tg0s0NevX8/WrVsH9fGSVKSI+MFs2xxykaQRYaBL0ogw0CVpRBjokjQiDHRJGhEGuiSNCANdkkZEcYG+/dmX+O9/u53nXv7JoEuRpKFSXKDv2P0yf3LXDp5/+cCgS5GkoVJcoLeaig9OelExSWpXXKBXzZ2MJr1KpCR1KC7QW1Ud6PbQJalTcYFuD12S+isv0CsDXZL6KS7QWzE15DLgQiRpyBQX6JWzXCSpr+ICveUYuiT1VV6gO8tFkvoqLtCnDooetIcuSR2KC/TpIRd76JLUobxAn562OOBCJGnIFBfoTQfdMXRJ6lJcoLc8sUiS+iov0MNZLpLUz7yBHhGnRMTdEfFYRDwaER/t0+b8iNgbEQ80j6uXp1xP/Zek2YwtoM0EcGVm3h8RrwW2RcSdmflYV7t7MvOSpS+xkz10Sepv3h56Zu7KzPub5y8BjwNrlruw2XhikST1d0hj6BGxHjgLuLfP5vMi4sGIuCMizliC2vpyyEWS+lvIkAsAEXEs8FXgY5n5Ytfm+4E3ZObLEXEx8DXgtD7vsQnYBLBu3bpFFTxzLZdFvVySRtaCeugRsYI6zG/KzNu6t2fmi5n5cvN8C7AiIlb3aXd9Zm7IzA3j4+OLK9h56JLU10JmuQTwZeDxzPzsLG1OatoREec07/v8UhY6xSEXSepvIUMu7wDeDzwcEQ806z4BrAPIzOuA9wIfjogJYD+wMXN5EtdZLpLU37yBnpl/D8Q8ba4BrlmqouZSOctFkvoq70xRh1wkqa/yAt17ikpSX8UF+tQ9Re2hS1Kn4gLdG1xIUn/FBXoV3oJOkvopL9Are+iS1E9xgQ71TBd76JLUqcxAj3CWiyR1KTLQq8pZLpLUrchAr3voBroktSsy0KvKQJekbkUGeqsKh1wkqUuZgR4GuiR1KzLQw1kuktSjyEBvVZ5YJEndygz08MQiSepWZKBXVdhDl6QuRQa6p/5LUq8yA90TiySpR5GBXjkPXZJ6FBnorQgmnbYoSR2KDPQIb3AhSd2KDPSWs1wkqUexgW4PXZI6FRnolbNcJKlHkYHu1RYlqVeZgW4PXZJ6zBvoEXFKRNwdEY9FxKMR8dE+bSIiPh8ROyLioYg4e3nKrVUVTluUpC5jC2gzAVyZmfdHxGuBbRFxZ2Y+1tbmIuC05vE24Nrmz2XRqoIJr58rSR3m7aFn5q7MvL95/hLwOLCmq9llwI1Z+yfguIg4ecmrbVRebVGSehzSGHpErAfOAu7t2rQGeLpteSe9oU9EbIqIrRGxdc+ePYdWaZsqnIcuSd0WHOgRcSzwVeBjmfniYj4sM6/PzA2ZuWF8fHwxbwE4D12S+llQoEfECuowvykzb+vT5BnglLbltc26ZVF5CzpJ6rGQWS4BfBl4PDM/O0uzzcAHmtku5wJ7M3PXEtbZwVvQSVKvhcxyeQfwfuDhiHigWfcJYB1AZl4HbAEuBnYA+4DfXvpSZzjkIkm95g30zPx7IOZpk8AVS1XUfDwoKkm9yjxT1B66JPUoM9DDa7lIUrciAz28Y5Ek9Sgy0FsVXpxLkroUGuiOoUtStyID3VkuktSryEC3hy5JvYoMdG9BJ0m9igz0VuWQiyR1KzfQzXNJ6lBkoHuDC0nqVWige7VFSepWZKA7y0WSehUZ6FUEmZCGuiRNKzLQW1V9NV+nLkrSjLID3R66JE0rMtCrqAPdKy5K0owiA73VVO010SVpRpGBPtVDd8hFkmYUHejORZekGUUGurNcJKlXkYFeOctFknoUGegtZ7lIUo8yA72p2h66JM0oMtA9KCpJvYoM9KmDos5Dl6QZRQe6s1wkaca8gR4RN0TE7oh4ZJbt50fE3oh4oHlcvfRldpoecrGHLknTxhbQ5ivANcCNc7S5JzMvWZKKFmD6TFFnuUjStHl76Jn5beCFI1DLgk3PcnHIRZKmLdUY+nkR8WBE3BERZ8zWKCI2RcTWiNi6Z8+eRX+YQy6S1GspAv1+4A2ZeSbwJ8DXZmuYmddn5obM3DA+Pr7oD/SgqCT1OuxAz8wXM/Pl5vkWYEVErD7syubgqf+S1OuwAz0iToqox0Ai4pzmPZ8/3PedS8sTiySpx7yzXCLiZuB8YHVE7AR+H1gBkJnXAe8FPhwRE8B+YGMu892bZ04sWs5PkaSyzBvomXn5PNuvoZ7WeMTMTFs00SVpSpFnijYddGe5SFKbIgPdWS6S1KvIQHeWiyT1KjLQneUiSb3KDHSHXCSpR5GB7qn/ktSryEB3Hrok9So00Os/HXKRpBlFBno45CJJPYoM9JZnikpSjzID3VkuktSjyECvKodcJKlbkYHe8p6iktSjyECvpma52EOXpGlFBvpUD32ZL7suSUUpM9A9KCpJPYoM9MpAl6QeZQa6JxZJUo8iA91ZLpLUq8hAn5rlYg9dkmYUGeie+i9JvcoMdA+KSlKPIgM9IohwyEWS2hUZ6FAPuxjokjSj2ECvqnCWiyS1KTfQHXKRpA7zBnpE3BARuyPikVm2R0R8PiJ2RMRDEXH20pfZqxXhQVFJarOQHvpXgAvn2H4RcFrz2ARce/hlza8ecjHQJWnKvIGemd8GXpijyWXAjVn7J+C4iDh5qQqcTavyoKgktVuKMfQ1wNNtyzubdcvKIRdJ6nRED4pGxKaI2BoRW/fs2XNY71XZQ5ekDksR6M8Ap7Qtr23W9cjM6zNzQ2ZuGB8fP6wPbUUw6bRFSZq2FIG+GfhAM9vlXGBvZu5agvedU6sKb0EnSW3G5msQETcD5wOrI2In8PvACoDMvA7YAlwM7AD2Ab+9XMW2qyqYdAxdkqbNG+iZefk82xO4YskqWqAq7KFLUrtizxR1loskdSo20J3lIkmdig10e+iS1KnYQPdqi5LUqdhAb1WQDrlI0rRyA91ZLpLUodhA92qLktSp3ED3FnSS1KHYQHeWiyR1KjbQ61P/B12FJA2PYgPdi3NJUqdiA71yyEWSOhQb6K0qnIcuSW3KDXTnoUtSh2ID3VP/JalTsYFe34LOHrokTSk20KsKh1wkqU25gW4PXZI6FBvozkOXpE7lBrrz0CWpQ7GBXlUOuUhSu2IDvRWBeS5JM4oN9MoxdEnqUGygtyoccpGkNsUGeuWp/5LUoexAt4cuSdOKDfSWs1wkqcOCAj0iLoyI7RGxIyI+3mf7ByNiT0Q80Dx+d+lL7eSJRZLUaWy+BhHRAr4AXADsBO6LiM2Z+VhX07/KzI8sQ4191af+H6lPk6Tht5Ae+jnAjsz8XmYeAG4BLlvesubXqmDSHrokTVtIoK8Bnm5b3tms6/ZvI+KhiLg1Ik7p90YRsSkitkbE1j179iyi3Bne4EKSOi3VQdGvA+sz8xeBO4G/6NcoM6/PzA2ZuWF8fPywPrCqgky8DZ0kNRYS6M8A7T3utc26aZn5fGb+pFn8EvDLS1Pe7KoIAKcuSlJjIYF+H3BaRJwaEUcBG4HN7Q0i4uS2xUuBx5euxP5aVRPo9tAlCVjALJfMnIiIjwDfBFrADZn5aER8GtiamZuB34uIS4EJ4AXgg8tYMzDTQ3emiyTV5g10gMzcAmzpWnd12/OrgKuWtrS5tZr/W9hDl6RasWeKOoYuSZ2KDfSpMXRnuUhSrfhAt4cuSbViA316yMUeuiQBBQf6VA/dWS6SVCs20Js8t4cuSY2CA32qh26gSxIUHOgeFJWkTuUHukMukgQUHOgOuUhSp2IDfXqWi3kuSUDBge6p/5LUqdhAn+mhG+iSBAUH+opWHei7X3plwJVI0nAoNtDPOfUE1hy3is/csZ1XD3q6qCQVG+hHHzXGpy49g+3//BJfuuepQZcjSQNXbKADXHD6ibz79BP53Le+w9Mv7Bt0OZI0UEUHOsCnLj2DIPgvdyz7bUwlaagVH+ivP24VHz7/TWx5+Fn+93efG3Q5kjQwxQc6wKZ3vpE1x63i019/jAkPkEr6KTUSgf6aFS0++Wv/gieefYk/2PKEt6WT9FNpJAId4KK3nsQH376eG/7hKa79X98ddDmSdMSNDbqApRIRXH3J6fxo3wH+2ze2s3ffq1z57jdz1NjI/GZJ0pxGJtABqir4w/eeyTErx/jTb3+Pe558ji++72zWrz5m0KVJ0rIbue7rUWMVf/Dr/5I/+8AGdu3dz2Vf+Af+8bvPD7osSVp2IxfoUy44/UT+5opfYfy1K3n/l+/lmrue5MCEM2Akja6RDXSAda87mtv+/dt5zxkn8Ud/+x0u/vw9bHl4l9d+kTSSYiFT/CLiQuBzQAv4Umb+167tK4EbgV8Gngd+MzO/P9d7btiwIbdu3brIsg/d3U/s5lNff5QfPL+Pk37mNbztjSew9vhVrD3+aNYev4rXH7eKk37mNRyzcqQOK0gaMRGxLTM39Ns2b3pFRAv4AnABsBO4LyI2Z+Zjbc0+BPwoM38+IjYCnwF+8/BLXzr/+i0/xzt/YZy7n9jNLff9kG0/+BG3P7Sr5wYZK1rBqhUtVh3V4uijxtqet7qej7HqqGq6zcoVFWNV0KoqWhW0qnq5iqjXt4LW1PM+j7G21wUQMXMTj4h6Fk8VEES9DNC0qdvXf1bNxqk2VdTt6+Yzz6fet3v91HvNPJ9q2/ZCSUNpId3Rc4Admfk9gIi4BbgMaA/0y4BPNc9vBa6JiMghO8OnVQXvOv1E3nX6iQBMHJzk2Rdf4ekX9vPsi/vZtfcVXtw/wSuvHmTfgQn2HTjYPD/IS69MsOeln7DvwEH2v3qQ/QfqNj9tN0xqD/7ebZ1r+7eZ2tb5RjHL9o4foK7Pia4NPdtnW9/1WX2q7L92jt+0uX7u5n7d7BsX83mL/eGd87OWof7+73WI7Q/xAw55zyxj/Zefs47f/VdvPLQPWICFBPoa4Om25Z3A22Zrk5kTEbEXeB3QcXGViNgEbAJYt27dIkteOmOtqhlyOXpRr89MDhycZP+Bg7zy6iQHM5mcTCYmk4PNY2JykslJmJicnF53cKpNJgcP1s8nc+p1k2RSP2juyJSQJJn1PVSnnmddRL0uk6wXp+/i1PG8Wa6fJ+0/tdnWPqfX1e061re9V+++6Fru02rm87uXOzfMbJ95j/le2/P5XbXO136m7lnWz/nDPfvGuV4357ZFvOdcJS72sxa56ZDP1j7UftGhdhUP/f2Xt/7Vx648xFcszBEdMM7M64HroR5DP5KfvRwigpVjLVaOtQZdiiQtaJbLM8Apbctrm3V920TEGPCz1AdHJUlHyEIC/T7gtIg4NSKOAjYCm7vabAZ+q3n+XuCuYRs/l6RRN++QSzMm/hHgm9TTFm/IzEcj4tPA1szcDHwZ+MuI2AG8QB36kqQjaEFj6Jm5BdjSte7qtuevAL+xtKVJkg7FSJ8pKkk/TQx0SRoRBrokjQgDXZJGxIIuzrUsHxyxB/jBIl++mq6zUIeQNS4Na1wa1nj4hqW+N2TmeL8NAwv0wxERW2e72tiwsMalYY1LwxoP37DXBw65SNLIMNAlaUSUGujXD7qABbDGpWGNS8MaD9+w11fmGLokqVepPXRJUhcDXZJGRHGBHhEXRsT2iNgRER8fdD0AEXFKRNwdEY9FxKMR8dFm/QkRcWdEPNn8efyA62xFxP+NiNub5VMj4t5mX/5Vc3nkQdZ3XETcGhFPRMTjEXHeEO7D/9j8HT8SETdHxGsGvR8j4oaI2B0Rj7St67vfovb5ptaHIuLsAdb4h83f9UMR8T8j4ri2bVc1NW6PiPcMqsa2bVdGREbE6mZ5IPtxPkUFetsNqy8CTgcuj4jTB1sVABPAlZl5OnAucEVT18eBb2XmacC3muVB+ijweNvyZ4A/zsyfB35EfbPvQfoc8I3MfAtwJnWtQ7MPI2IN8HvAhsx8K/XlpKduij7I/fgV4MKudbPtt4uA05rHJuDaAdZ4J/DWzPxF4DvAVQDNd2cjcEbzmi823/1B1EhEnAK8G/hh2+pB7ce5ZWYxD+A84Jtty1cBVw26rj51/g1wAbAdOLlZdzKwfYA1raX+Yv8qcDv1PW2fA8b67dsB1PezwFM0B+rb1g/TPpy6d+4J1Jeevh14zzDsR2A98Mh8+w34U+Dyfu2OdI1d234duKl53vG9pr4Xw3mDqpH6xvdnAt8HVg96P871KKqHTv8bVq8ZUC19RcR64CzgXuDEzNzVbHoWOHFAZQH8D+A/AZPN8uuAH2fmRLM86H15KrAH+PNmWOhLEXEMQ7QPM/MZ4I+oe2q7gL3ANoZrP06Zbb8N63fod4A7mudDU2NEXAY8k5kPdm0amhrblRboQy0ijgW+CnwsM19s35b1z/hA5ohGxCXA7szcNojPX6Ax4Gzg2sw8C/h/dA2vDHIfAjTj0JdR//i8HjiGPv9FHzaD3m/ziYhPUg9b3jToWtpFxNHAJ4Cr52s7LEoL9IXcsHogImIFdZjflJm3Nav/OSJObrafDOweUHnvAC6NiO8Dt1APu3wOOK65qTcMfl/uBHZm5r3N8q3UAT8s+xDgXcBTmbknM18FbqPet8O0H6fMtt+G6jsUER8ELgHe1/zwwPDU+CbqH+8Hm+/OWuD+iDiJ4amxQ2mBvpAbVh9xERHU91V9PDM/27ap/ebZv0U9tn7EZeZVmbk2M9dT77O7MvN9wN3UN/UeaH0Amfks8HREvLlZ9W+AxxiSfdj4IXBuRBzd/J1P1Tg0+7HNbPttM/CBZpbGucDetqGZIyoiLqQeBrw0M/e1bdoMbIyIlRFxKvWBx/9zpOvLzIcz8+cyc33z3dkJnN38Wx2a/dhh0IP4izhocTH1EfHvAp8cdD1NTb9C/V/ah4AHmsfF1OPU3wKeBP4OOGEIaj0fuL15/kbqL8oO4K+BlQOu7ZeArc1+/Bpw/LDtQ+A/A08AjwB/Cawc9H4EbqYe03+VOnQ+NNt+oz4Y/oXm+/Mw9YydQdW4g3oceuo7c11b+082NW4HLhpUjV3bv8/MQdGB7Mf5Hp76L0kjorQhF0nSLAx0SRoRBrokjQgDXZJGhIEuSSPCQJekEWGgS9KI+P80TbBdf2p4KQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14448786.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54567.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>59847.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>57746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>57982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>57080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>56720.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v\n",
       "0      57732.0\n",
       "1      49679.0\n",
       "2      54567.0\n",
       "3      56139.0\n",
       "4      62291.0\n",
       "...        ...\n",
       "11995  59847.0\n",
       "11996  57746.0\n",
       "11997  57982.0\n",
       "11998  57080.0\n",
       "11999  56720.0\n",
       "\n",
       "[12000 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test = pd.DataFrame(y_test[:,0],columns=output_col)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAssBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      57721.691406  57732.0\n",
      "1      49521.343750  49679.0\n",
      "2      54464.921875  54567.0\n",
      "3      55912.726562  56139.0\n",
      "4      62067.667969  62291.0\n",
      "...             ...      ...\n",
      "11995  59567.546875  59847.0\n",
      "11996  57577.855469  57746.0\n",
      "11997  57917.683594  57982.0\n",
      "11998  57017.367188  57080.0\n",
      "11999  56538.316406  56720.0\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(100.0, 200.0]       5248\n",
      "(200.0, 300.0]       3786\n",
      "(0.0, 100.0]         2059\n",
      "(300.0, 400.0]        800\n",
      "(400.0, 500.0]         92\n",
      "(500.0, 600.0]         11\n",
      "(3000.0, 4000.0]        1\n",
      "(1000.0, 2000.0]        1\n",
      "(800.0, 900.0]          1\n",
      "(600.0, 700.0]          1\n",
      "(5000.0, 10000.0]       0\n",
      "(4000.0, 5000.0]        0\n",
      "(2000.0, 3000.0]        0\n",
      "(900.0, 1000.0]         0\n",
      "(700.0, 800.0]          0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57721.691406</td>\n",
       "      <td>57732.0</td>\n",
       "      <td>-10.308594</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49521.343750</td>\n",
       "      <td>49679.0</td>\n",
       "      <td>-157.656250</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54464.921875</td>\n",
       "      <td>54567.0</td>\n",
       "      <td>-102.078125</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55912.726562</td>\n",
       "      <td>56139.0</td>\n",
       "      <td>-226.273438</td>\n",
       "      <td>(200.0, 300.0]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62067.667969</td>\n",
       "      <td>62291.0</td>\n",
       "      <td>-223.332031</td>\n",
       "      <td>(200.0, 300.0]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>59567.546875</td>\n",
       "      <td>59847.0</td>\n",
       "      <td>-279.453125</td>\n",
       "      <td>(200.0, 300.0]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>57577.855469</td>\n",
       "      <td>57746.0</td>\n",
       "      <td>-168.144531</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>57917.683594</td>\n",
       "      <td>57982.0</td>\n",
       "      <td>-64.316406</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>57017.367188</td>\n",
       "      <td>57080.0</td>\n",
       "      <td>-62.632812</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>56538.316406</td>\n",
       "      <td>56720.0</td>\n",
       "      <td>-181.683594</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test     v_error   v_error_range v_error_label\n",
       "0      57721.691406  57732.0  -10.308594    (0.0, 100.0]           1.0\n",
       "1      49521.343750  49679.0 -157.656250  (100.0, 200.0]           2.0\n",
       "2      54464.921875  54567.0 -102.078125  (100.0, 200.0]           2.0\n",
       "3      55912.726562  56139.0 -226.273438  (200.0, 300.0]           3.0\n",
       "4      62067.667969  62291.0 -223.332031  (200.0, 300.0]           3.0\n",
       "...             ...      ...         ...             ...           ...\n",
       "11995  59567.546875  59847.0 -279.453125  (200.0, 300.0]           3.0\n",
       "11996  57577.855469  57746.0 -168.144531  (100.0, 200.0]           2.0\n",
       "11997  57917.683594  57982.0  -64.316406    (0.0, 100.0]           1.0\n",
       "11998  57017.367188  57080.0  -62.632812    (0.0, 100.0]           1.0\n",
       "11999  56538.316406  56720.0 -181.683594  (100.0, 200.0]           2.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxAssBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
