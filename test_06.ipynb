{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexhang/project/Nuclear'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kinf_1</th>\n",
       "      <th>kinf_2</th>\n",
       "      <th>kinf_3</th>\n",
       "      <th>kinf_4</th>\n",
       "      <th>kinf_5</th>\n",
       "      <th>kinf_6</th>\n",
       "      <th>kinf_7</th>\n",
       "      <th>kinf_8</th>\n",
       "      <th>kinf_9</th>\n",
       "      <th>kinf_10</th>\n",
       "      <th>...</th>\n",
       "      <th>NODE2DBU_95</th>\n",
       "      <th>NODE2DBU_96</th>\n",
       "      <th>NODE2DBU_97</th>\n",
       "      <th>NODE2DBU_98</th>\n",
       "      <th>NODE2DBU_99</th>\n",
       "      <th>NODE2DBU_100</th>\n",
       "      <th>NODE2DBU_101</th>\n",
       "      <th>NODE2DBU_102</th>\n",
       "      <th>NODE2DBU_103</th>\n",
       "      <th>NODE2DBU_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.16708</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.18090</td>\n",
       "      <td>1.16216</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.08632</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26837.0</td>\n",
       "      <td>26864.0</td>\n",
       "      <td>26834.0</td>\n",
       "      <td>26862.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1.15988</td>\n",
       "      <td>1.07458</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.15534</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>1.19730</td>\n",
       "      <td>1.15417</td>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.20422</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.06296</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1.06622</td>\n",
       "      <td>1.16655</td>\n",
       "      <td>1.20028</td>\n",
       "      <td>1.19485</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15935</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>43835.0</td>\n",
       "      <td>39276.0</td>\n",
       "      <td>42082.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         kinf_1   kinf_2   kinf_3   kinf_4  kinf_5  kinf_6   kinf_7  kinf_8  \\\n",
       "0       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "1       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "2       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "3       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "4       1.16708  1.08099  1.18090  1.16216  1.4279  1.1821  1.08632  1.4279   \n",
       "...         ...      ...      ...      ...     ...     ...      ...     ...   \n",
       "119995  1.15988  1.07458  1.09861  1.15534  1.4279  1.1821  1.07427  1.4279   \n",
       "119996  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119997  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119998  1.19730  1.15417  1.21509  1.20422  1.4279  1.1821  1.06296  1.4279   \n",
       "119999  1.06622  1.16655  1.20028  1.19485  1.4279  1.1821  1.15935  1.4279   \n",
       "\n",
       "        kinf_9  kinf_10  ...  NODE2DBU_95  NODE2DBU_96  NODE2DBU_97  \\\n",
       "0       1.1821   1.1821  ...          0.0          0.0      36445.0   \n",
       "1       1.1821   1.1821  ...          0.0          0.0      36809.0   \n",
       "2       1.1821   1.1821  ...          0.0          0.0      31836.0   \n",
       "3       1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "4       1.1821   1.1821  ...          0.0          0.0      26837.0   \n",
       "...        ...      ...  ...          ...          ...          ...   \n",
       "119995  1.1821   1.1821  ...          0.0          0.0      26191.0   \n",
       "119996  1.1821   1.1821  ...          0.0          0.0      43151.0   \n",
       "119997  1.1821   1.1821  ...          0.0          0.0      43091.0   \n",
       "119998  1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "119999  1.1821   1.1821  ...          0.0          0.0      42090.0   \n",
       "\n",
       "        NODE2DBU_98  NODE2DBU_99  NODE2DBU_100  NODE2DBU_101  NODE2DBU_102  \\\n",
       "0           31803.0      36809.0       31836.0       20806.0       20806.0   \n",
       "1           36445.0      31836.0       31803.0       20806.0       20806.0   \n",
       "2           36809.0      31803.0       36445.0       20806.0       20806.0   \n",
       "3           31836.0      36445.0       36809.0       20806.0       20806.0   \n",
       "4           26864.0      26834.0       26862.0       22008.0       22008.0   \n",
       "...             ...          ...           ...           ...           ...   \n",
       "119995      26191.0      26200.0       26200.0       36146.0       36146.0   \n",
       "119996      45258.0      39658.0       43091.0       36186.0       36186.0   \n",
       "119997      39658.0      45258.0       43151.0       36186.0       36186.0   \n",
       "119998      31836.0      36445.0       36809.0       25238.0       25238.0   \n",
       "119999      43835.0      39276.0       42082.0       21568.0       21568.0   \n",
       "\n",
       "        NODE2DBU_103  NODE2DBU_104  \n",
       "0            20806.0       20806.0  \n",
       "1            20806.0       20806.0  \n",
       "2            20806.0       20806.0  \n",
       "3            20806.0       20806.0  \n",
       "4            22008.0       22008.0  \n",
       "...              ...           ...  \n",
       "119995       36146.0       36146.0  \n",
       "119996       36186.0       36186.0  \n",
       "119997       36186.0       36186.0  \n",
       "119998       25238.0       25238.0  \n",
       "119999       21568.0       21568.0  \n",
       "\n",
       "[120000 rows x 156 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入处理好的数据\n",
    "file_base_name = 'nuclear_burnup_data_20201215.csv'\n",
    "file_input_name = 'df.csv'\n",
    "\n",
    "pre_data_X = pd.read_csv(os.path.join(path, file_input_name), index_col=0)\n",
    "pre_data_base = pd.read_csv(os.path.join(path, file_base_name))\n",
    "pre_data_X.columns.values.tolist()  \n",
    "pre_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAssBurnupCal</th>\n",
       "      <th>MaxPinBurnupCal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56624</td>\n",
       "      <td>62467.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56812</td>\n",
       "      <td>61980.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56724</td>\n",
       "      <td>62521.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56537</td>\n",
       "      <td>62195.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57424</td>\n",
       "      <td>64025.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>57359</td>\n",
       "      <td>63599.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>59562</td>\n",
       "      <td>64865.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>59631</td>\n",
       "      <td>63716.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>61714</td>\n",
       "      <td>65601.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>61843</td>\n",
       "      <td>66948.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAssBurnupCal  MaxPinBurnupCal\n",
       "0                 56624          62467.5\n",
       "1                 56812          61980.1\n",
       "2                 56724          62521.4\n",
       "3                 56537          62195.7\n",
       "4                 57424          64025.7\n",
       "...                 ...              ...\n",
       "119995            57359          63599.1\n",
       "119996            59562          64865.4\n",
       "119997            59631          63716.2\n",
       "119998            61714          65601.8\n",
       "119999            61843          66948.3\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_y = pre_data_base.iloc[:,-2:]\n",
    "pre_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = pd.concat([pre_data_X, pre_data_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 156)\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pre_data_array = pre_data.values\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "pre_data_Normalized = mm.fit_transform(pre_data_array)\n",
    "pre_data_Normalized = pre_data_Normalized[:,:-2]\n",
    "# pre_data_y = pre_data_Normalized[:,-2:]\n",
    "print(pre_data_Normalized.shape)\n",
    "print(pre_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "map1 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [2,6,11,16,20,23,25,26,25,23,20,16,11,6,2],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0]\n",
    "    \n",
    "]\n",
    "m = np.array(map1)\n",
    "print(m.shape)\n",
    "map2 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['2/0','2/2',\"6/0\",\"6/2\",'11/0','11/2','16/0','16/2',\"20/0\",'20/2','23/0','23/2','25/0','25/2','26/0','26/1',\"25/2\",'25/0','23/2','23/0','20/2','20/0','16/2','16/0','11/2','11/0','6/2','6/0','2/2','2/0'],\n",
    "    ['2/1','2/3',\"6/1\",\"6/3\",'11/1','11/3','16/1','16/3',\"20/1\",'20/3','23/1','23/3','25/1','25/3','26/2','26/3',\"25/3\",'25/1','23/3','23/1','20/3','20/1','16/3','16/1','11/3','11/1','6/3','6/1','2/3','2/1'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "      \n",
    "]\n",
    "m = np.array(map2)\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(a,alist):\n",
    "    res = []\n",
    "    for i in range(len(alist)):\n",
    "        for j in range(len(alist[i])):\n",
    "            if alist[i][j] == a:\n",
    "                res.append((i,j))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #排布input\n",
    "# def search(a,alist):\n",
    "#     res = []\n",
    "#     for i in range(len(alist)):\n",
    "#         for j in range(len(alist[i])):\n",
    "#             if alist[i][j] == a:\n",
    "#                 res.append((i,j))\n",
    "#     return res\n",
    "# new = []\n",
    "# for i,item in enumerate(pre_data_Normalized):\n",
    "#     print(\"正在处理第 \"+str(i)+' 个图像..')\n",
    "#     layer1 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer2 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer3 = [[0 for _ in range(30)] for _ in range(30)]\n",
    "# #     print(item)\n",
    "#     for j in range(item.size): #len = 156\n",
    "#         if j < 26:\n",
    "#             res = search(j+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer1[h][w] = item[j]\n",
    "#         elif j < 52:\n",
    "#             j_ = j-26\n",
    "#             res = search(j_+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer2[h][w] = item[j]\n",
    "            \n",
    "#         else:\n",
    "#             j_ = j-52\n",
    "#             number = str(j_ // 4 + 1)\n",
    "#             corner = str(j_ % 4)\n",
    "#             number_corner = number + '/' + corner\n",
    "#             res = search(number_corner,map2)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer3[h][w] = item[j]\n",
    "#     layer1_array = np.array(layer1)\n",
    "#     layer2_array = np.array(layer2)\n",
    "#     layer3_array = np.array(layer3)\n",
    "    \n",
    "#     #从1*1扩展成2*2\n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 1) \n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 0)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 1)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 0)\n",
    "    \n",
    "#     #channel 拼接\n",
    "#     tmp = np.stack((layer1_array,layer2_array,layer3_array),axis = 2)\n",
    "#     new.append(tmp)\n",
    "\n",
    "# pre_data_x = np.array(new)\n",
    "# print(pre_data_x.shape)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"pre_data_x_tmp.npy\",pre_data_x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_x = np.load(\"pre_data_x_tmp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pre_data_x.shape)\n",
    "pre_data_y = np.array(pre_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    " \n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 30, 30, 16)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 30, 30, 16)        16400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 30, 30, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 30, 30, 32)        65568     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 30, 30, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 30, 30, 64)        262208    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              57601000  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,143,393\n",
      "Trainable params: 58,142,721\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.applications.resnet18 import ResNet18\n",
    "# from keras.applications.resnet18 import preprocess_input as preprocess_input_resnet\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "# import tensorflow.keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "def deeper_conv2D(h,w):\n",
    "    new_model = tf.keras.Sequential()\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\", activation=\"relu\"))\n",
    "    # Flatten will take our convolution filters and lay them out end to end so our dense layer can predict based on the outcomes of each\n",
    "    new_model.add(tf.keras.layers.Flatten())\n",
    "#     new_model.add(tf.keras.layers.Dense(1000,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "    new_model.add(tf.keras.layers.Dense(1000))\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(100))\n",
    "#     new_model.add(tf.keras.layers.Dense(100,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(1))\n",
    "    new_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")    \n",
    "    return new_model\n",
    "# m = deeper_conv2D(30,30)\n",
    "m = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m2 = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "seed = 2020\n",
    "x_pre_train, x_test, y_pre_train, y_test = train_test_split(pre_data_x, pre_data_y, \n",
    "                                                           random_state=seed, train_size=0.9, \n",
    "                                                           test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    min_delta=0.0001, \n",
    "    cooldown=0, \n",
    "    min_lr=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 46830100.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 52s 61ms/step - loss: 761344.1250 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 53s 63ms/step - loss: 706294.6250 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 54s 64ms/step - loss: 605505.6250 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 599694.6875 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 55s 66ms/step - loss: 520676.1562 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 463628.0625 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 420776.3438 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 407031.6250 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 365342.5625 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 282116.2500 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 360450.9688 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 330853.7188 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 263376.3125 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 333028.8125 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 306061.5312 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 258086.5312 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 256315.2969 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 234700.1406 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 188778.0469 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 209349.4688 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 251339.1562 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 136076.9688 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 186486.9219 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 199358.5938 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 173091.7500 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 154484.5312 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 155602.9375 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 191323.8750 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 169777.3594 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 143813.7812 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 173812.1719 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 129845.4844 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 137240.0000 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 136918.1094 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 135188.7969 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 131920.8125 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 142612.0312 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 133839.0156 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 129290.0469 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 106185.1016 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 119194.4609 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 131333.8906 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 100846.0312 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 108396.0781 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 120560.0234 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 124080.3672 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 115538.4609 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 97890.5781 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 93354.9531 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 85244.8047 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 94996.4219 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 97593.1953 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 85011.3594 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 105191.2422 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 91148.1016 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 91912.0703 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 83136.7266 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 76325.5156 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 97863.0703 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 95234.6562 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 78422.3516 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 77928.6172 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 91671.2422 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 80235.8984 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 70750.6250 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 68986.5625 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 72937.8281 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 70486.3125 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 100158.2969 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 69680.7188 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 78556.1562 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 69415.2656 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 78365.4062 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 80860.7031 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 66717.6172 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 77142.6250 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 64984.4297 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 74741.2969 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "844/844 [==============================] - 55s 65ms/step - loss: 75543.8516 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 70321.8516 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 67993.4688 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 51785.3945 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 73923.0703 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 57425.0273 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 67197.3359 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 59053.6758 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 56321.7461 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 72835.4844 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 53555.5469 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 55561.0586 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 62220.6875 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 46633.2539 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 60114.8906 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 55167.7188 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 58952.9258 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 57859.8359 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 49613.6680 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 63625.8711 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 61208.5430 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 49005.1172 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 53724.5820 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 60225.0156 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 14382.0400 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 13800.8203 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 13852.5820 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 13257.3818 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 14774.7168 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12977.5752 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12995.6875 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 14488.4014 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 14539.0557 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11782.1914 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 13004.0703 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 13513.0977 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12919.1348 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12318.6797 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12644.9375 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12217.2656 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12976.7363 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 14404.8604 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11129.0615 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11073.9297 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11768.3613 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12264.7744 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10921.4219 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11794.5977 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11381.1709 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11165.1387 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11375.3789 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11808.9189 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11068.2500 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 12046.4893 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10739.7764 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10987.0469 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10586.7393 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10527.4844 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10738.1572 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10022.9971 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10765.0996 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10159.9326 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10360.3604 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 11230.7344 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10338.3691 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 9864.7949 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 9602.9473 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10978.0518 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10121.5400 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 9776.0322 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 9623.6406 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 9978.5303 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10798.0957 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10000.0645 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10272.3672 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 10486.3359 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 9792.4727 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6426.0361 - lr: 1.0000e-05\n",
      "Epoch 158/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 51s 61ms/step - loss: 6133.9590 - lr: 1.0000e-05\n",
      "Epoch 159/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6304.9702 - lr: 1.0000e-05\n",
      "Epoch 160/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6517.5591 - lr: 1.0000e-05\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6287.9102 - lr: 1.0000e-05\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6335.2725 - lr: 1.0000e-05\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6397.6958 - lr: 1.0000e-05\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6267.6523 - lr: 1.0000e-05\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6332.6519 - lr: 1.0000e-05\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6354.8745 - lr: 1.0000e-05\n",
      "Epoch 167/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6163.6641 - lr: 1.0000e-05\n",
      "Epoch 168/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6429.3169 - lr: 1.0000e-05\n",
      "Epoch 169/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5717.6831 - lr: 1.0000e-06\n",
      "Epoch 170/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5774.9570 - lr: 1.0000e-06\n",
      "Epoch 171/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5726.3491 - lr: 1.0000e-06\n",
      "Epoch 172/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5818.2715 - lr: 1.0000e-06\n",
      "Epoch 173/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5740.8848 - lr: 1.0000e-06\n",
      "Epoch 174/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6037.7417 - lr: 1.0000e-06\n",
      "Epoch 175/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5883.7261 - lr: 1.0000e-06\n",
      "Epoch 176/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5754.3506 - lr: 1.0000e-06\n",
      "Epoch 177/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5941.2979 - lr: 1.0000e-06\n",
      "Epoch 178/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5869.3628 - lr: 1.0000e-06\n",
      "Epoch 179/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5847.7637 - lr: 1.0000e-06\n",
      "Epoch 180/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5678.9077 - lr: 1.0000e-07\n",
      "Epoch 181/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5843.9824 - lr: 1.0000e-07\n",
      "Epoch 182/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5768.4976 - lr: 1.0000e-07\n",
      "Epoch 183/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5734.0479 - lr: 1.0000e-07\n",
      "Epoch 184/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5584.8506 - lr: 1.0000e-07\n",
      "Epoch 185/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5744.8203 - lr: 1.0000e-07\n",
      "Epoch 186/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5723.8457 - lr: 1.0000e-07\n",
      "Epoch 187/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5765.6914 - lr: 1.0000e-07\n",
      "Epoch 188/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5762.4746 - lr: 1.0000e-07\n",
      "Epoch 189/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5842.0312 - lr: 1.0000e-07\n",
      "Epoch 190/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5773.1963 - lr: 1.0000e-07\n",
      "Epoch 191/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5761.6528 - lr: 1.0000e-07\n",
      "Epoch 192/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5785.2056 - lr: 1.0000e-07\n",
      "Epoch 193/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5528.1157 - lr: 1.0000e-07\n",
      "Epoch 194/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5753.2500 - lr: 1.0000e-07\n",
      "Epoch 195/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5782.7261 - lr: 1.0000e-07\n",
      "Epoch 196/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5636.8691 - lr: 1.0000e-07\n",
      "Epoch 197/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5648.6055 - lr: 1.0000e-07\n",
      "Epoch 198/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5763.1792 - lr: 1.0000e-07\n",
      "Epoch 199/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5649.6660 - lr: 1.0000e-07\n",
      "Epoch 200/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5713.5928 - lr: 1.0000e-07\n",
      "Epoch 201/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5853.4307 - lr: 1.0000e-07\n",
      "Epoch 202/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5733.6001 - lr: 1.0000e-07\n",
      "Epoch 203/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5734.8193 - lr: 1.0000e-07\n",
      "Epoch 204/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5714.7124 - lr: 1.0000e-08\n",
      "Epoch 205/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5866.2368 - lr: 1.0000e-08\n",
      "Epoch 206/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5700.6475 - lr: 1.0000e-08\n",
      "Epoch 207/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5863.7104 - lr: 1.0000e-08\n",
      "Epoch 208/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5644.7856 - lr: 1.0000e-08\n",
      "Epoch 209/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5632.5718 - lr: 1.0000e-08\n",
      "Epoch 210/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5799.3032 - lr: 1.0000e-08\n",
      "Epoch 211/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5590.3862 - lr: 1.0000e-08\n",
      "Epoch 212/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5799.3677 - lr: 1.0000e-08\n",
      "Epoch 213/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5769.7871 - lr: 1.0000e-08\n",
      "Epoch 214/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5657.7915 - lr: 1.0000e-09\n",
      "Epoch 215/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5759.7515 - lr: 1.0000e-09\n",
      "Epoch 216/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5651.8496 - lr: 1.0000e-09\n",
      "Epoch 217/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5689.2212 - lr: 1.0000e-09\n",
      "Epoch 218/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5684.8911 - lr: 1.0000e-09\n",
      "Epoch 219/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5736.9653 - lr: 1.0000e-09\n",
      "Epoch 220/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5560.3271 - lr: 1.0000e-09\n",
      "Epoch 221/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5675.2900 - lr: 1.0000e-09\n",
      "Epoch 222/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5730.2061 - lr: 1.0000e-09\n",
      "Epoch 223/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5793.3623 - lr: 1.0000e-09\n",
      "Epoch 224/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5770.9937 - lr: 1.0000e-10\n",
      "Epoch 225/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5840.4067 - lr: 1.0000e-10\n",
      "Epoch 226/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5777.9287 - lr: 1.0000e-10\n",
      "Epoch 227/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5790.2266 - lr: 1.0000e-10\n",
      "Epoch 228/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5777.6816 - lr: 1.0000e-10\n",
      "Epoch 229/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5750.1118 - lr: 1.0000e-10\n",
      "Epoch 230/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5716.3501 - lr: 1.0000e-10\n",
      "Epoch 231/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5814.3301 - lr: 1.0000e-10\n",
      "Epoch 232/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5915.9854 - lr: 1.0000e-10\n",
      "Epoch 233/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5586.6465 - lr: 1.0000e-10\n",
      "Epoch 234/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5790.9248 - lr: 1.0000e-11\n",
      "Epoch 235/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 51s 61ms/step - loss: 5759.5239 - lr: 1.0000e-11\n",
      "Epoch 236/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5778.7798 - lr: 1.0000e-11\n",
      "Epoch 237/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5814.8218 - lr: 1.0000e-11\n",
      "Epoch 238/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5621.3096 - lr: 1.0000e-11\n",
      "Epoch 239/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5817.9844 - lr: 1.0000e-11\n",
      "Epoch 240/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5774.8188 - lr: 1.0000e-11\n",
      "Epoch 241/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5815.1899 - lr: 1.0000e-11\n",
      "Epoch 242/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5821.5503 - lr: 1.0000e-11\n",
      "Epoch 243/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5676.8638 - lr: 1.0000e-11\n",
      "Epoch 244/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5831.0439 - lr: 1.0000e-12\n",
      "Epoch 245/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5844.4429 - lr: 1.0000e-12\n",
      "Epoch 246/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5680.5947 - lr: 1.0000e-12\n",
      "Epoch 247/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5729.7881 - lr: 1.0000e-12\n",
      "Epoch 248/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5714.6650 - lr: 1.0000e-12\n",
      "Epoch 249/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5665.6143 - lr: 1.0000e-12\n",
      "Epoch 250/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5756.5967 - lr: 1.0000e-12\n",
      "Epoch 251/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5847.8921 - lr: 1.0000e-12\n",
      "Epoch 252/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5758.0952 - lr: 1.0000e-12\n",
      "Epoch 253/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5693.6099 - lr: 1.0000e-12\n",
      "Epoch 254/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5770.6470 - lr: 1.0000e-13\n",
      "Epoch 255/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5967.4814 - lr: 1.0000e-13\n",
      "Epoch 256/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5736.0962 - lr: 1.0000e-13\n",
      "Epoch 257/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5747.7964 - lr: 1.0000e-13\n",
      "Epoch 258/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5732.6777 - lr: 1.0000e-13\n",
      "Epoch 259/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5766.1543 - lr: 1.0000e-13\n",
      "Epoch 260/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5862.7290 - lr: 1.0000e-13\n",
      "Epoch 261/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5704.2080 - lr: 1.0000e-13\n",
      "Epoch 262/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5746.8433 - lr: 1.0000e-13\n",
      "Epoch 263/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5804.6304 - lr: 1.0000e-13\n",
      "Epoch 264/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5725.0386 - lr: 1.0000e-14\n",
      "Epoch 265/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5779.9312 - lr: 1.0000e-14\n",
      "Epoch 266/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5789.5459 - lr: 1.0000e-14\n",
      "Epoch 267/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5751.6758 - lr: 1.0000e-14\n",
      "Epoch 268/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5783.7134 - lr: 1.0000e-14\n",
      "Epoch 269/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5673.7373 - lr: 1.0000e-14\n",
      "Epoch 270/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5823.3174 - lr: 1.0000e-14\n",
      "Epoch 271/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5837.0366 - lr: 1.0000e-14\n",
      "Epoch 272/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5862.4663 - lr: 1.0000e-14\n",
      "Epoch 273/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5713.9585 - lr: 1.0000e-14\n",
      "Epoch 274/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5632.3975 - lr: 1.0000e-15\n",
      "Epoch 275/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5628.0884 - lr: 1.0000e-15\n",
      "Epoch 276/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5739.6724 - lr: 1.0000e-15\n",
      "Epoch 277/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5627.8589 - lr: 1.0000e-15\n",
      "Epoch 278/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5808.5493 - lr: 1.0000e-15\n",
      "Epoch 279/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5654.5825 - lr: 1.0000e-15\n",
      "Epoch 280/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5694.7510 - lr: 1.0000e-15\n",
      "Epoch 281/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5818.7646 - lr: 1.0000e-15\n",
      "Epoch 282/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5752.1772 - lr: 1.0000e-15\n",
      "Epoch 283/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5825.8062 - lr: 1.0000e-15\n",
      "Epoch 284/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5689.0234 - lr: 1.0000e-16\n",
      "Epoch 285/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5731.9863 - lr: 1.0000e-16\n",
      "Epoch 286/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5768.2222 - lr: 1.0000e-16\n",
      "Epoch 287/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5738.3394 - lr: 1.0000e-16\n",
      "Epoch 288/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5894.9966 - lr: 1.0000e-16\n",
      "Epoch 289/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5672.9351 - lr: 1.0000e-16\n",
      "Epoch 290/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5730.2129 - lr: 1.0000e-16\n",
      "Epoch 291/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5827.3628 - lr: 1.0000e-16\n",
      "Epoch 292/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5716.9629 - lr: 1.0000e-16\n",
      "Epoch 293/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5706.2476 - lr: 1.0000e-16\n",
      "Epoch 294/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5874.4941 - lr: 1.0000e-17\n",
      "Epoch 295/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5648.4180 - lr: 1.0000e-17\n",
      "Epoch 296/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5783.2632 - lr: 1.0000e-17\n",
      "Epoch 297/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5848.3516 - lr: 1.0000e-17\n",
      "Epoch 298/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5777.0864 - lr: 1.0000e-17\n",
      "Epoch 299/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5831.3311 - lr: 1.0000e-17\n",
      "Epoch 300/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5739.3027 - lr: 1.0000e-17\n"
     ]
    }
   ],
   "source": [
    "model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m2.save('test_05_model_2.h5')\n",
    "\n",
    "\n",
    "#batch size: 调小\n",
    "#删除所有正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "# m2.save('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m2.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_2 = pd.DataFrame(y_test[:,1],columns=output_col)\n",
    "# y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPinBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      63723.007812  63664.2\n",
      "1      52866.988281  53156.4\n",
      "2      60749.804688  60857.7\n",
      "3      63810.355469  63804.1\n",
      "4      65973.140625  66056.1\n",
      "...             ...      ...\n",
      "11995  63041.769531  63212.1\n",
      "11996  63986.652344  63966.7\n",
      "11997  63368.746094  63355.0\n",
      "11998  62906.574219  63106.4\n",
      "11999  62770.851562  62843.1\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(0.0, 100.0]         7413\n",
      "(100.0, 200.0]       3082\n",
      "(200.0, 300.0]        944\n",
      "(300.0, 400.0]        320\n",
      "(400.0, 500.0]        113\n",
      "(500.0, 600.0]         73\n",
      "(600.0, 700.0]         25\n",
      "(700.0, 800.0]         13\n",
      "(800.0, 900.0]          6\n",
      "(1000.0, 2000.0]        6\n",
      "(900.0, 1000.0]         5\n",
      "(2000.0, 3000.0]        0\n",
      "(3000.0, 4000.0]        0\n",
      "(4000.0, 5000.0]        0\n",
      "(5000.0, 10000.0]       0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63723.007812</td>\n",
       "      <td>63664.2</td>\n",
       "      <td>58.807813</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52866.988281</td>\n",
       "      <td>53156.4</td>\n",
       "      <td>-289.411719</td>\n",
       "      <td>(200.0, 300.0]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60749.804688</td>\n",
       "      <td>60857.7</td>\n",
       "      <td>-107.895312</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63810.355469</td>\n",
       "      <td>63804.1</td>\n",
       "      <td>6.255469</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65973.140625</td>\n",
       "      <td>66056.1</td>\n",
       "      <td>-82.959375</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>63041.769531</td>\n",
       "      <td>63212.1</td>\n",
       "      <td>-170.330469</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>63986.652344</td>\n",
       "      <td>63966.7</td>\n",
       "      <td>19.952344</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>63368.746094</td>\n",
       "      <td>63355.0</td>\n",
       "      <td>13.746094</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>62906.574219</td>\n",
       "      <td>63106.4</td>\n",
       "      <td>-199.825781</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>62770.851562</td>\n",
       "      <td>62843.1</td>\n",
       "      <td>-72.248437</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test     v_error   v_error_range v_error_label\n",
       "0      63723.007812  63664.2   58.807813    (0.0, 100.0]           1.0\n",
       "1      52866.988281  53156.4 -289.411719  (200.0, 300.0]           3.0\n",
       "2      60749.804688  60857.7 -107.895312  (100.0, 200.0]           2.0\n",
       "3      63810.355469  63804.1    6.255469    (0.0, 100.0]           1.0\n",
       "4      65973.140625  66056.1  -82.959375    (0.0, 100.0]           1.0\n",
       "...             ...      ...         ...             ...           ...\n",
       "11995  63041.769531  63212.1 -170.330469  (100.0, 200.0]           2.0\n",
       "11996  63986.652344  63966.7   19.952344    (0.0, 100.0]           1.0\n",
       "11997  63368.746094  63355.0   13.746094    (0.0, 100.0]           1.0\n",
       "11998  62906.574219  63106.4 -199.825781  (100.0, 200.0]           2.0\n",
       "11999  62770.851562  62843.1  -72.248437    (0.0, 100.0]           1.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxPinBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_2[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 40996216.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 640152.4375 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 527912.7500 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 359361.6875 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 394063.3438 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 377093.0938 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 415398.5000 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 320692.4375 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 330512.8125 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 301962.0000 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 285273.7188 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 241401.0938 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 273520.7500 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 213551.4688 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 236206.5469 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 226687.2031 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 236155.9531 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 227673.8125 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 154844.9844 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 210671.6719 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 149512.2812 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 117616.5625 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 125376.5391 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 122332.2422 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 115075.0000 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 109766.5625 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 128264.5625 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 108091.2422 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 80127.6797 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 92843.6250 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 116356.4922 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 89324.3438 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 91392.8984 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 77551.6953 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 79755.9844 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 120940.0391 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 89806.2734 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 86638.9062 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 81832.4766 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 58076.5117 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 66565.8828 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 66138.9297 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 58107.0273 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 77863.3828 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 66376.4609 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 56853.4453 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 53681.0273 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 54767.9531 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 64305.5742 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 48179.7422 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 80054.8594 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 51461.2539 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 53033.4062 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 59960.2188 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 54110.7734 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 44665.2031 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 57976.1055 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 42900.9844 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 51117.6758 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 47320.1211 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 43759.6016 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 52170.0547 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 55422.6914 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 36530.9141 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 43740.8203 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 42225.6836 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 48338.1523 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 45407.7344 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 44293.9180 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 38791.4219 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 53362.5820 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 43175.7070 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 49369.8672 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 42579.8086 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 7667.5806 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6768.8101 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 7241.3418 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 7476.0195 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6994.2061 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6839.7646 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6695.2598 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 7599.2959 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6988.3223 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 7057.2847 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6617.4556 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6456.4463 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5431.9478 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6474.2437 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6169.0190 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5915.4404 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6262.8774 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5766.3569 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5633.2368 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 6670.7300 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5359.0425 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5581.9492 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5332.1504 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5450.7373 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5035.9717 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5885.6606 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4774.5483 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4331.9434 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5639.2646 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5238.6714 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4428.9883 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4804.8828 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5190.8330 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5375.6191 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4311.1211 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4315.0635 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5060.0396 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4282.9995 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5190.4961 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5006.8091 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4449.6094 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4989.5518 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4082.0664 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4467.7764 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5003.5117 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4585.5264 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 3719.7590 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4533.8062 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 3756.3359 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4388.9746 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 5103.4170 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4644.6641 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4807.2754 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4526.9912 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4479.5649 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 3990.6233 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 4177.7192 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2244.2368 - lr: 1.0000e-05\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2204.3281 - lr: 1.0000e-05\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2317.2681 - lr: 1.0000e-05\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2295.7673 - lr: 1.0000e-05\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2284.3311 - lr: 1.0000e-05\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2258.9619 - lr: 1.0000e-05\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2245.6404 - lr: 1.0000e-05\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2176.5142 - lr: 1.0000e-05\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2314.9639 - lr: 1.0000e-05\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2223.2166 - lr: 1.0000e-05\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2295.6626 - lr: 1.0000e-05\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2259.7493 - lr: 1.0000e-05\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2150.8086 - lr: 1.0000e-05\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2290.9233 - lr: 1.0000e-05\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2185.5837 - lr: 1.0000e-05\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2213.1448 - lr: 1.0000e-05\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2326.7000 - lr: 1.0000e-05\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2182.7620 - lr: 1.0000e-05\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2182.4229 - lr: 1.0000e-05\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2177.7876 - lr: 1.0000e-05\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2203.0872 - lr: 1.0000e-05\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2232.3611 - lr: 1.0000e-05\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 2181.7168 - lr: 1.0000e-05\n",
      "Epoch 155/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1958.2510 - lr: 1.0000e-06\n",
      "Epoch 156/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1899.6538 - lr: 1.0000e-06\n",
      "Epoch 157/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1957.0159 - lr: 1.0000e-06\n",
      "Epoch 158/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 51s 61ms/step - loss: 1898.4078 - lr: 1.0000e-06\n",
      "Epoch 159/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1984.0520 - lr: 1.0000e-06\n",
      "Epoch 160/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1929.1299 - lr: 1.0000e-06\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1967.5591 - lr: 1.0000e-06\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1951.0289 - lr: 1.0000e-06\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1925.7310 - lr: 1.0000e-06\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1925.5194 - lr: 1.0000e-06\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1974.3960 - lr: 1.0000e-06\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1958.7603 - lr: 1.0000e-06\n",
      "Epoch 167/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1926.9379 - lr: 1.0000e-06\n",
      "Epoch 168/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1969.8938 - lr: 1.0000e-06\n",
      "Epoch 169/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1956.4667 - lr: 1.0000e-07\n",
      "Epoch 170/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1915.8362 - lr: 1.0000e-07\n",
      "Epoch 171/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1939.7404 - lr: 1.0000e-07\n",
      "Epoch 172/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1902.8624 - lr: 1.0000e-07\n",
      "Epoch 173/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1942.6500 - lr: 1.0000e-07\n",
      "Epoch 174/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1924.5037 - lr: 1.0000e-07\n",
      "Epoch 175/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1949.6317 - lr: 1.0000e-07\n",
      "Epoch 176/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1910.1808 - lr: 1.0000e-07\n",
      "Epoch 177/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1926.0873 - lr: 1.0000e-07\n",
      "Epoch 178/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1919.7291 - lr: 1.0000e-07\n",
      "Epoch 179/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1912.0750 - lr: 1.0000e-08\n",
      "Epoch 180/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1892.0636 - lr: 1.0000e-08\n",
      "Epoch 181/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1937.5049 - lr: 1.0000e-08\n",
      "Epoch 182/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1885.6655 - lr: 1.0000e-08\n",
      "Epoch 183/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1915.6128 - lr: 1.0000e-08\n",
      "Epoch 184/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1993.3921 - lr: 1.0000e-08\n",
      "Epoch 185/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1928.2609 - lr: 1.0000e-08\n",
      "Epoch 186/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1933.4662 - lr: 1.0000e-08\n",
      "Epoch 187/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1877.2899 - lr: 1.0000e-08\n",
      "Epoch 188/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1946.2953 - lr: 1.0000e-08\n",
      "Epoch 189/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1919.8826 - lr: 1.0000e-08\n",
      "Epoch 190/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1948.4528 - lr: 1.0000e-08\n",
      "Epoch 191/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1941.5381 - lr: 1.0000e-08\n",
      "Epoch 192/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1905.0081 - lr: 1.0000e-08\n",
      "Epoch 193/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1948.3885 - lr: 1.0000e-08\n",
      "Epoch 194/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1914.3654 - lr: 1.0000e-08\n",
      "Epoch 195/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1960.4556 - lr: 1.0000e-08\n",
      "Epoch 196/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1929.4862 - lr: 1.0000e-08\n",
      "Epoch 197/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1901.7672 - lr: 1.0000e-08\n",
      "Epoch 198/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1910.7903 - lr: 1.0000e-09\n",
      "Epoch 199/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1933.9146 - lr: 1.0000e-09\n",
      "Epoch 200/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1913.1858 - lr: 1.0000e-09\n",
      "Epoch 201/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1933.9392 - lr: 1.0000e-09\n",
      "Epoch 202/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1940.3290 - lr: 1.0000e-09\n",
      "Epoch 203/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1959.1406 - lr: 1.0000e-09\n",
      "Epoch 204/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1905.9895 - lr: 1.0000e-09\n",
      "Epoch 205/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1927.3767 - lr: 1.0000e-09\n",
      "Epoch 206/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1886.6898 - lr: 1.0000e-09\n",
      "Epoch 207/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1889.6157 - lr: 1.0000e-09\n",
      "Epoch 208/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1933.5262 - lr: 1.0000e-10\n",
      "Epoch 209/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1881.4926 - lr: 1.0000e-10\n",
      "Epoch 210/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1941.1978 - lr: 1.0000e-10\n",
      "Epoch 211/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1900.0929 - lr: 1.0000e-10\n",
      "Epoch 212/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1904.3281 - lr: 1.0000e-10\n",
      "Epoch 213/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1862.3192 - lr: 1.0000e-10\n",
      "Epoch 214/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1901.0446 - lr: 1.0000e-10\n",
      "Epoch 215/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1957.0327 - lr: 1.0000e-10\n",
      "Epoch 216/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1922.9783 - lr: 1.0000e-10\n",
      "Epoch 217/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1911.9244 - lr: 1.0000e-10\n",
      "Epoch 218/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1879.2770 - lr: 1.0000e-10\n",
      "Epoch 219/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1918.1781 - lr: 1.0000e-10\n",
      "Epoch 220/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1917.3108 - lr: 1.0000e-10\n",
      "Epoch 221/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1876.9059 - lr: 1.0000e-10\n",
      "Epoch 222/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1900.2731 - lr: 1.0000e-10\n",
      "Epoch 223/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1914.0054 - lr: 1.0000e-10\n",
      "Epoch 224/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1958.8994 - lr: 1.0000e-11\n",
      "Epoch 225/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1923.8236 - lr: 1.0000e-11\n",
      "Epoch 226/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1920.8528 - lr: 1.0000e-11\n",
      "Epoch 227/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1913.1423 - lr: 1.0000e-11\n",
      "Epoch 228/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1909.2537 - lr: 1.0000e-11\n",
      "Epoch 229/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1938.1854 - lr: 1.0000e-11\n",
      "Epoch 230/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1939.7616 - lr: 1.0000e-11\n",
      "Epoch 231/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1945.5352 - lr: 1.0000e-11\n",
      "Epoch 232/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1910.6185 - lr: 1.0000e-11\n",
      "Epoch 233/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1916.8362 - lr: 1.0000e-11\n",
      "Epoch 234/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1897.2260 - lr: 1.0000e-12\n",
      "Epoch 235/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 51s 61ms/step - loss: 1906.2600 - lr: 1.0000e-12\n",
      "Epoch 236/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1944.9387 - lr: 1.0000e-12\n",
      "Epoch 237/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1964.0133 - lr: 1.0000e-12\n",
      "Epoch 238/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1920.9294 - lr: 1.0000e-12\n",
      "Epoch 239/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1926.4225 - lr: 1.0000e-12\n",
      "Epoch 240/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1889.7689 - lr: 1.0000e-12\n",
      "Epoch 241/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1993.0480 - lr: 1.0000e-12\n",
      "Epoch 242/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1933.5277 - lr: 1.0000e-12\n",
      "Epoch 243/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1944.1652 - lr: 1.0000e-12\n",
      "Epoch 244/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1914.2078 - lr: 1.0000e-13\n",
      "Epoch 245/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1959.7482 - lr: 1.0000e-13\n",
      "Epoch 246/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1899.4377 - lr: 1.0000e-13\n",
      "Epoch 247/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1921.3983 - lr: 1.0000e-13\n",
      "Epoch 248/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1924.1956 - lr: 1.0000e-13\n",
      "Epoch 249/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1915.1316 - lr: 1.0000e-13\n",
      "Epoch 250/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1923.9293 - lr: 1.0000e-13\n",
      "Epoch 251/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1897.8365 - lr: 1.0000e-13\n",
      "Epoch 252/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1957.5074 - lr: 1.0000e-13\n",
      "Epoch 253/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1879.4907 - lr: 1.0000e-13\n",
      "Epoch 254/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1918.0680 - lr: 1.0000e-14\n",
      "Epoch 255/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1892.4517 - lr: 1.0000e-14\n",
      "Epoch 256/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1948.3597 - lr: 1.0000e-14\n",
      "Epoch 257/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1916.0192 - lr: 1.0000e-14\n",
      "Epoch 258/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1898.5133 - lr: 1.0000e-14\n",
      "Epoch 259/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1899.3151 - lr: 1.0000e-14\n",
      "Epoch 260/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1891.3328 - lr: 1.0000e-14\n",
      "Epoch 261/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1966.4792 - lr: 1.0000e-14\n",
      "Epoch 262/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1929.4667 - lr: 1.0000e-14\n",
      "Epoch 263/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1953.6180 - lr: 1.0000e-14\n",
      "Epoch 264/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1911.1757 - lr: 1.0000e-15\n",
      "Epoch 265/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1927.9780 - lr: 1.0000e-15\n",
      "Epoch 266/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1917.4835 - lr: 1.0000e-15\n",
      "Epoch 267/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1898.3615 - lr: 1.0000e-15\n",
      "Epoch 268/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1921.7488 - lr: 1.0000e-15\n",
      "Epoch 269/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1899.0266 - lr: 1.0000e-15\n",
      "Epoch 270/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1890.1454 - lr: 1.0000e-15\n",
      "Epoch 271/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1911.2911 - lr: 1.0000e-15\n",
      "Epoch 272/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1884.8456 - lr: 1.0000e-15\n",
      "Epoch 273/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1926.2408 - lr: 1.0000e-15\n",
      "Epoch 274/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1911.4600 - lr: 1.0000e-16\n",
      "Epoch 275/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1886.6908 - lr: 1.0000e-16\n",
      "Epoch 276/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1926.7620 - lr: 1.0000e-16\n",
      "Epoch 277/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1878.5001 - lr: 1.0000e-16\n",
      "Epoch 278/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1940.5143 - lr: 1.0000e-16\n",
      "Epoch 279/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1922.4899 - lr: 1.0000e-16\n",
      "Epoch 280/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1958.2406 - lr: 1.0000e-16\n",
      "Epoch 281/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1867.2762 - lr: 1.0000e-16\n",
      "Epoch 282/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1932.3369 - lr: 1.0000e-16\n",
      "Epoch 283/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1993.3765 - lr: 1.0000e-16\n",
      "Epoch 284/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1967.6609 - lr: 1.0000e-17\n",
      "Epoch 285/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1947.1821 - lr: 1.0000e-17\n",
      "Epoch 286/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1969.9110 - lr: 1.0000e-17\n",
      "Epoch 287/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1928.6144 - lr: 1.0000e-17\n",
      "Epoch 288/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1879.3593 - lr: 1.0000e-17\n",
      "Epoch 289/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1912.3763 - lr: 1.0000e-17\n",
      "Epoch 290/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1924.1805 - lr: 1.0000e-17\n",
      "Epoch 291/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1896.8904 - lr: 1.0000e-17\n",
      "Epoch 292/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1882.4149 - lr: 1.0000e-17\n",
      "Epoch 293/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1956.5862 - lr: 1.0000e-17\n",
      "Epoch 294/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1939.2880 - lr: 1.0000e-18\n",
      "Epoch 295/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1933.3868 - lr: 1.0000e-18\n",
      "Epoch 296/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1893.5414 - lr: 1.0000e-18\n",
      "Epoch 297/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1948.8358 - lr: 1.0000e-18\n",
      "Epoch 298/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1876.5795 - lr: 1.0000e-18\n",
      "Epoch 299/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1895.8352 - lr: 1.0000e-18\n",
      "Epoch 300/300\n",
      "844/844 [==============================] - 51s 61ms/step - loss: 1915.0289 - lr: 1.0000e-18\n"
     ]
    }
   ],
   "source": [
    "model = m.fit(x_pre_train,  y_pre_train[:,0], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m.save('test_05_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = load_model('test_05_model.h5')\n",
    "# m2 = load_model('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57732. , 63664.2],\n",
       "       [49679. , 53156.4],\n",
       "       [54567. , 60857.7],\n",
       "       ...,\n",
       "       [57982. , 63355. ],\n",
       "       [57080. , 63106.4],\n",
       "       [56720. , 62843.1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_1 = pd.DataFrame(y_test[:,0],columns=output_col)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAssBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      57737.578125  57732.0\n",
      "1      49612.714844  49679.0\n",
      "2      54552.804688  54567.0\n",
      "3      56084.082031  56139.0\n",
      "4      62323.562500  62291.0\n",
      "...             ...      ...\n",
      "11995  59819.457031  59847.0\n",
      "11996  57776.980469  57746.0\n",
      "11997  58009.046875  57982.0\n",
      "11998  57024.722656  57080.0\n",
      "11999  56695.257812  56720.0\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(0.0, 100.0]         11422\n",
      "(100.0, 200.0]         540\n",
      "(200.0, 300.0]          25\n",
      "(300.0, 400.0]           6\n",
      "(400.0, 500.0]           2\n",
      "(500.0, 600.0]           2\n",
      "(600.0, 700.0]           2\n",
      "(3000.0, 4000.0]         1\n",
      "(700.0, 800.0]           0\n",
      "(800.0, 900.0]           0\n",
      "(900.0, 1000.0]          0\n",
      "(1000.0, 2000.0]         0\n",
      "(2000.0, 3000.0]         0\n",
      "(4000.0, 5000.0]         0\n",
      "(5000.0, 10000.0]        0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57737.578125</td>\n",
       "      <td>57732.0</td>\n",
       "      <td>5.578125</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49612.714844</td>\n",
       "      <td>49679.0</td>\n",
       "      <td>-66.285156</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54552.804688</td>\n",
       "      <td>54567.0</td>\n",
       "      <td>-14.195312</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56084.082031</td>\n",
       "      <td>56139.0</td>\n",
       "      <td>-54.917969</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62323.562500</td>\n",
       "      <td>62291.0</td>\n",
       "      <td>32.562500</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>59819.457031</td>\n",
       "      <td>59847.0</td>\n",
       "      <td>-27.542969</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>57776.980469</td>\n",
       "      <td>57746.0</td>\n",
       "      <td>30.980469</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>58009.046875</td>\n",
       "      <td>57982.0</td>\n",
       "      <td>27.046875</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>57024.722656</td>\n",
       "      <td>57080.0</td>\n",
       "      <td>-55.277344</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>56695.257812</td>\n",
       "      <td>56720.0</td>\n",
       "      <td>-24.742188</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test    v_error v_error_range v_error_label\n",
       "0      57737.578125  57732.0   5.578125  (0.0, 100.0]           1.0\n",
       "1      49612.714844  49679.0 -66.285156  (0.0, 100.0]           1.0\n",
       "2      54552.804688  54567.0 -14.195312  (0.0, 100.0]           1.0\n",
       "3      56084.082031  56139.0 -54.917969  (0.0, 100.0]           1.0\n",
       "4      62323.562500  62291.0  32.562500  (0.0, 100.0]           1.0\n",
       "...             ...      ...        ...           ...           ...\n",
       "11995  59819.457031  59847.0 -27.542969  (0.0, 100.0]           1.0\n",
       "11996  57776.980469  57746.0  30.980469  (0.0, 100.0]           1.0\n",
       "11997  58009.046875  57982.0  27.046875  (0.0, 100.0]           1.0\n",
       "11998  57024.722656  57080.0 -55.277344  (0.0, 100.0]           1.0\n",
       "11999  56695.257812  56720.0 -24.742188  (0.0, 100.0]           1.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxAssBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_1[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
