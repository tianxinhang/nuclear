{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexhang/project/Nuclear'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kinf_1</th>\n",
       "      <th>kinf_2</th>\n",
       "      <th>kinf_3</th>\n",
       "      <th>kinf_4</th>\n",
       "      <th>kinf_5</th>\n",
       "      <th>kinf_6</th>\n",
       "      <th>kinf_7</th>\n",
       "      <th>kinf_8</th>\n",
       "      <th>kinf_9</th>\n",
       "      <th>kinf_10</th>\n",
       "      <th>...</th>\n",
       "      <th>NODE2DBU_95</th>\n",
       "      <th>NODE2DBU_96</th>\n",
       "      <th>NODE2DBU_97</th>\n",
       "      <th>NODE2DBU_98</th>\n",
       "      <th>NODE2DBU_99</th>\n",
       "      <th>NODE2DBU_100</th>\n",
       "      <th>NODE2DBU_101</th>\n",
       "      <th>NODE2DBU_102</th>\n",
       "      <th>NODE2DBU_103</th>\n",
       "      <th>NODE2DBU_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.16708</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.18090</td>\n",
       "      <td>1.16216</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.08632</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26837.0</td>\n",
       "      <td>26864.0</td>\n",
       "      <td>26834.0</td>\n",
       "      <td>26862.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1.15988</td>\n",
       "      <td>1.07458</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.15534</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>1.19730</td>\n",
       "      <td>1.15417</td>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.20422</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.06296</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1.06622</td>\n",
       "      <td>1.16655</td>\n",
       "      <td>1.20028</td>\n",
       "      <td>1.19485</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15935</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>43835.0</td>\n",
       "      <td>39276.0</td>\n",
       "      <td>42082.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         kinf_1   kinf_2   kinf_3   kinf_4  kinf_5  kinf_6   kinf_7  kinf_8  \\\n",
       "0       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "1       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "2       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "3       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "4       1.16708  1.08099  1.18090  1.16216  1.4279  1.1821  1.08632  1.4279   \n",
       "...         ...      ...      ...      ...     ...     ...      ...     ...   \n",
       "119995  1.15988  1.07458  1.09861  1.15534  1.4279  1.1821  1.07427  1.4279   \n",
       "119996  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119997  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119998  1.19730  1.15417  1.21509  1.20422  1.4279  1.1821  1.06296  1.4279   \n",
       "119999  1.06622  1.16655  1.20028  1.19485  1.4279  1.1821  1.15935  1.4279   \n",
       "\n",
       "        kinf_9  kinf_10  ...  NODE2DBU_95  NODE2DBU_96  NODE2DBU_97  \\\n",
       "0       1.1821   1.1821  ...          0.0          0.0      36445.0   \n",
       "1       1.1821   1.1821  ...          0.0          0.0      36809.0   \n",
       "2       1.1821   1.1821  ...          0.0          0.0      31836.0   \n",
       "3       1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "4       1.1821   1.1821  ...          0.0          0.0      26837.0   \n",
       "...        ...      ...  ...          ...          ...          ...   \n",
       "119995  1.1821   1.1821  ...          0.0          0.0      26191.0   \n",
       "119996  1.1821   1.1821  ...          0.0          0.0      43151.0   \n",
       "119997  1.1821   1.1821  ...          0.0          0.0      43091.0   \n",
       "119998  1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "119999  1.1821   1.1821  ...          0.0          0.0      42090.0   \n",
       "\n",
       "        NODE2DBU_98  NODE2DBU_99  NODE2DBU_100  NODE2DBU_101  NODE2DBU_102  \\\n",
       "0           31803.0      36809.0       31836.0       20806.0       20806.0   \n",
       "1           36445.0      31836.0       31803.0       20806.0       20806.0   \n",
       "2           36809.0      31803.0       36445.0       20806.0       20806.0   \n",
       "3           31836.0      36445.0       36809.0       20806.0       20806.0   \n",
       "4           26864.0      26834.0       26862.0       22008.0       22008.0   \n",
       "...             ...          ...           ...           ...           ...   \n",
       "119995      26191.0      26200.0       26200.0       36146.0       36146.0   \n",
       "119996      45258.0      39658.0       43091.0       36186.0       36186.0   \n",
       "119997      39658.0      45258.0       43151.0       36186.0       36186.0   \n",
       "119998      31836.0      36445.0       36809.0       25238.0       25238.0   \n",
       "119999      43835.0      39276.0       42082.0       21568.0       21568.0   \n",
       "\n",
       "        NODE2DBU_103  NODE2DBU_104  \n",
       "0            20806.0       20806.0  \n",
       "1            20806.0       20806.0  \n",
       "2            20806.0       20806.0  \n",
       "3            20806.0       20806.0  \n",
       "4            22008.0       22008.0  \n",
       "...              ...           ...  \n",
       "119995       36146.0       36146.0  \n",
       "119996       36186.0       36186.0  \n",
       "119997       36186.0       36186.0  \n",
       "119998       25238.0       25238.0  \n",
       "119999       21568.0       21568.0  \n",
       "\n",
       "[120000 rows x 156 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入处理好的数据\n",
    "file_base_name = 'nuclear_burnup_data_20201215.csv'\n",
    "file_input_name = 'df.csv'\n",
    "\n",
    "pre_data_X = pd.read_csv(os.path.join(path, file_input_name), index_col=0)\n",
    "pre_data_base = pd.read_csv(os.path.join(path, file_base_name))\n",
    "pre_data_X.columns.values.tolist()  \n",
    "pre_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAssBurnupCal</th>\n",
       "      <th>MaxPinBurnupCal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56624</td>\n",
       "      <td>62467.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56812</td>\n",
       "      <td>61980.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56724</td>\n",
       "      <td>62521.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56537</td>\n",
       "      <td>62195.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57424</td>\n",
       "      <td>64025.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>57359</td>\n",
       "      <td>63599.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>59562</td>\n",
       "      <td>64865.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>59631</td>\n",
       "      <td>63716.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>61714</td>\n",
       "      <td>65601.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>61843</td>\n",
       "      <td>66948.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAssBurnupCal  MaxPinBurnupCal\n",
       "0                 56624          62467.5\n",
       "1                 56812          61980.1\n",
       "2                 56724          62521.4\n",
       "3                 56537          62195.7\n",
       "4                 57424          64025.7\n",
       "...                 ...              ...\n",
       "119995            57359          63599.1\n",
       "119996            59562          64865.4\n",
       "119997            59631          63716.2\n",
       "119998            61714          65601.8\n",
       "119999            61843          66948.3\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_y = pre_data_base.iloc[:,-2:]\n",
    "pre_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = pd.concat([pre_data_X, pre_data_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 156)\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pre_data_array = pre_data.values\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "pre_data_Normalized = mm.fit_transform(pre_data_array)\n",
    "pre_data_Normalized = pre_data_Normalized[:,:-2]\n",
    "# pre_data_y = pre_data_Normalized[:,-2:]\n",
    "print(pre_data_Normalized.shape)\n",
    "print(pre_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "map1 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [2,6,11,16,20,23,25,26,25,23,20,16,11,6,2],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0]\n",
    "    \n",
    "]\n",
    "m = np.array(map1)\n",
    "print(m.shape)\n",
    "map2 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['2/0','2/2',\"6/0\",\"6/2\",'11/0','11/2','16/0','16/2',\"20/0\",'20/2','23/0','23/2','25/0','25/2','26/0','26/1',\"25/2\",'25/0','23/2','23/0','20/2','20/0','16/2','16/0','11/2','11/0','6/2','6/0','2/2','2/0'],\n",
    "    ['2/1','2/3',\"6/1\",\"6/3\",'11/1','11/3','16/1','16/3',\"20/1\",'20/3','23/1','23/3','25/1','25/3','26/2','26/3',\"25/3\",'25/1','23/3','23/1','20/3','20/1','16/3','16/1','11/3','11/1','6/3','6/1','2/3','2/1'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "      \n",
    "]\n",
    "m = np.array(map2)\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(a,alist):\n",
    "    res = []\n",
    "    for i in range(len(alist)):\n",
    "        for j in range(len(alist[i])):\n",
    "            if alist[i][j] == a:\n",
    "                res.append((i,j))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #排布input\n",
    "# def search(a,alist):\n",
    "#     res = []\n",
    "#     for i in range(len(alist)):\n",
    "#         for j in range(len(alist[i])):\n",
    "#             if alist[i][j] == a:\n",
    "#                 res.append((i,j))\n",
    "#     return res\n",
    "# new = []\n",
    "# for i,item in enumerate(pre_data_Normalized):\n",
    "#     print(\"正在处理第 \"+str(i)+' 个图像..')\n",
    "#     layer1 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer2 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer3 = [[0 for _ in range(30)] for _ in range(30)]\n",
    "# #     print(item)\n",
    "#     for j in range(item.size): #len = 156\n",
    "#         if j < 26:\n",
    "#             res = search(j+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer1[h][w] = item[j]\n",
    "#         elif j < 52:\n",
    "#             j_ = j-26\n",
    "#             res = search(j_+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer2[h][w] = item[j]\n",
    "            \n",
    "#         else:\n",
    "#             j_ = j-52\n",
    "#             number = str(j_ // 4 + 1)\n",
    "#             corner = str(j_ % 4)\n",
    "#             number_corner = number + '/' + corner\n",
    "#             res = search(number_corner,map2)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer3[h][w] = item[j]\n",
    "#     layer1_array = np.array(layer1)\n",
    "#     layer2_array = np.array(layer2)\n",
    "#     layer3_array = np.array(layer3)\n",
    "    \n",
    "#     #从1*1扩展成2*2\n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 1) \n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 0)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 1)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 0)\n",
    "    \n",
    "#     #channel 拼接\n",
    "#     tmp = np.stack((layer1_array,layer2_array,layer3_array),axis = 2)\n",
    "#     new.append(tmp)\n",
    "\n",
    "# pre_data_x = np.array(new)\n",
    "# print(pre_data_x.shape)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"pre_data_x_tmp.npy\",pre_data_x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_x = np.load(\"pre_data_x_tmp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pre_data_x.shape)\n",
    "pre_data_y = np.array(pre_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    " \n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 16)        16400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 32)        65568     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 64)        262208    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              57601000  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,143,393\n",
      "Trainable params: 58,142,721\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.applications.resnet18 import ResNet18\n",
    "# from keras.applications.resnet18 import preprocess_input as preprocess_input_resnet\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "# import tensorflow.keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "def deeper_conv2D(h,w):\n",
    "    new_model = tf.keras.Sequential()\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\", activation=\"relu\"))\n",
    "    # Flatten will take our convolution filters and lay them out end to end so our dense layer can predict based on the outcomes of each\n",
    "    new_model.add(tf.keras.layers.Flatten())\n",
    "    new_model.add(tf.keras.layers.Dense(1000,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(100))\n",
    "#     new_model.add(tf.keras.layers.Dense(100,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(1))\n",
    "    new_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")    \n",
    "    return new_model\n",
    "# m = deeper_conv2D(30,30)\n",
    "m = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m2 = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "seed = 2020\n",
    "x_pre_train, x_test, y_pre_train, y_test = train_test_split(pre_data_x, pre_data_y, \n",
    "                                                           random_state=seed, train_size=0.9, \n",
    "                                                           test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    min_delta=0.0001, \n",
    "    cooldown=0, \n",
    "    min_lr=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 58s 69ms/step - loss: 46856232.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 987923.3125 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 744336.5000 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 661310.6250 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 60s 71ms/step - loss: 701966.1875 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 535151.4375 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 573714.6875 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 472039.3438 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 481643.3750 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 404127.6562 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 350351.1250 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 312905.4375 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 282391.4688 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 382509.6875 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 286763.9688 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 303328.0000 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 62s 73ms/step - loss: 265933.5938 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 283863.2500 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 278411.0000 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 209644.9219 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 236457.2031 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 240696.3594 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 185933.5156 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 61s 73ms/step - loss: 207919.3906 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 61s 73ms/step - loss: 199076.6250 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 62s 73ms/step - loss: 180605.3281 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 186425.9375 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 192375.7500 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 191518.8750 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 149874.3438 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 156709.0781 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 62s 74ms/step - loss: 130926.4688 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 136195.2188 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 146146.4219 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 160561.9062 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 138094.7500 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 132915.9688 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 165062.2344 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 111833.2031 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 119537.3125 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 153346.1094 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 123378.2109 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 118486.0469 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 118444.1328 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 103355.8906 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 113660.5391 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 91219.2891 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 102693.2344 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 118230.1953 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 63s 75ms/step - loss: 111692.9219 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 98032.8359 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 63s 74ms/step - loss: 97117.9531 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 97617.2969 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 101684.5000 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 102555.5000 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 89646.4688 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 86347.4531 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 81775.9453 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 96531.9219 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 81898.2031 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 76674.1250 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 81820.9297 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 88574.2344 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 74682.8828 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 87246.2344 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 90096.9453 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 77194.0156 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 89964.6953 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 76089.1719 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 71992.0234 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 84804.7656 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 77338.7266 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 84797.7969 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 74420.6172 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 76328.5781 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 73959.8828 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 65345.1719 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 68527.3672 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 64092.5430 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 66639.6562 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 73175.0625 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 76033.4062 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 74893.4297 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 65400.1055 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 63681.9297 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 63687.9180 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 66526.6562 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 74394.9688 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 54587.3984 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 70128.9844 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 61232.2031 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 67895.9297 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 70949.9062 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 60133.3320 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 61116.4297 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 53986.5234 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 61529.4922 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 56317.0469 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 53833.2773 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 60586.8945 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 69226.4219 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 56434.8594 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 62791.8477 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 61005.2773 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 65395.4844 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 55536.0820 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 70391.0625 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 43958.4336 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 68605.6875 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 62704.4062 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 57579.8359 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 62050.5430 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 51381.2617 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 54962.1016 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 55627.3984 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 62857.2266 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 54746.4258 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 52385.1758 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 16262.5459 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 16103.1973 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 16064.0342 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 14236.2754 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 15995.8613 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13904.7090 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 14316.8828 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 15126.2451 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 14129.0645 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13194.5117 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12892.4844 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13927.0352 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 14500.7881 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13318.7578 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13944.4824 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12883.8604 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13613.2188 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13220.9717 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13523.6670 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12545.8154 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13020.9580 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12279.1689 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11963.4561 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13484.6191 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12129.3721 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12346.2041 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12383.2256 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12453.9717 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11542.4141 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12054.6572 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11915.0947 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 13558.9355 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11620.7949 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12067.4043 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11350.8994 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11862.3701 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11592.5605 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10818.5176 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11516.7646 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12423.9326 - lr: 1.0000e-04\n",
      "Epoch 159/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 59s 70ms/step - loss: 12011.0439 - lr: 1.0000e-04\n",
      "Epoch 160/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 12346.8350 - lr: 1.0000e-04\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10684.1650 - lr: 1.0000e-04\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10536.0479 - lr: 1.0000e-04\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11269.6074 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11684.0000 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11116.3213 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10818.3467 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10363.2109 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11609.5293 - lr: 1.0000e-04\n",
      "Epoch 169/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10991.9355 - lr: 1.0000e-04\n",
      "Epoch 170/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10966.7051 - lr: 1.0000e-04\n",
      "Epoch 171/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10996.4395 - lr: 1.0000e-04\n",
      "Epoch 172/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10061.5547 - lr: 1.0000e-04\n",
      "Epoch 173/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11218.4688 - lr: 1.0000e-04\n",
      "Epoch 174/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10317.0254 - lr: 1.0000e-04\n",
      "Epoch 175/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10526.3203 - lr: 1.0000e-04\n",
      "Epoch 176/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11711.4023 - lr: 1.0000e-04\n",
      "Epoch 177/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10585.3379 - lr: 1.0000e-04\n",
      "Epoch 178/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10864.4277 - lr: 1.0000e-04\n",
      "Epoch 179/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11027.3604 - lr: 1.0000e-04\n",
      "Epoch 180/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10956.4805 - lr: 1.0000e-04\n",
      "Epoch 181/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10126.2100 - lr: 1.0000e-04\n",
      "Epoch 182/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9739.4277 - lr: 1.0000e-04\n",
      "Epoch 183/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11540.2568 - lr: 1.0000e-04\n",
      "Epoch 184/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10107.1494 - lr: 1.0000e-04\n",
      "Epoch 185/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10038.2646 - lr: 1.0000e-04\n",
      "Epoch 186/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11009.5938 - lr: 1.0000e-04\n",
      "Epoch 187/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10592.9092 - lr: 1.0000e-04\n",
      "Epoch 188/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10423.2197 - lr: 1.0000e-04\n",
      "Epoch 189/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11194.0488 - lr: 1.0000e-04\n",
      "Epoch 190/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10565.3223 - lr: 1.0000e-04\n",
      "Epoch 191/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11388.3047 - lr: 1.0000e-04\n",
      "Epoch 192/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9511.8867 - lr: 1.0000e-04\n",
      "Epoch 193/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9562.1465 - lr: 1.0000e-04\n",
      "Epoch 194/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 11337.5098 - lr: 1.0000e-04\n",
      "Epoch 195/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10693.5254 - lr: 1.0000e-04\n",
      "Epoch 196/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9757.5068 - lr: 1.0000e-04\n",
      "Epoch 197/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9609.2393 - lr: 1.0000e-04\n",
      "Epoch 198/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9871.7881 - lr: 1.0000e-04\n",
      "Epoch 199/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9940.8584 - lr: 1.0000e-04\n",
      "Epoch 200/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9116.2822 - lr: 1.0000e-04\n",
      "Epoch 201/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10313.7842 - lr: 1.0000e-04\n",
      "Epoch 202/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9218.0117 - lr: 1.0000e-04\n",
      "Epoch 203/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10067.6768 - lr: 1.0000e-04\n",
      "Epoch 204/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9247.4111 - lr: 1.0000e-04\n",
      "Epoch 205/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10031.1885 - lr: 1.0000e-04\n",
      "Epoch 206/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9738.4131 - lr: 1.0000e-04\n",
      "Epoch 207/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9593.4736 - lr: 1.0000e-04\n",
      "Epoch 208/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9692.1738 - lr: 1.0000e-04\n",
      "Epoch 209/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10344.9590 - lr: 1.0000e-04\n",
      "Epoch 210/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8936.8193 - lr: 1.0000e-04\n",
      "Epoch 211/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9670.8037 - lr: 1.0000e-04\n",
      "Epoch 212/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9094.4180 - lr: 1.0000e-04\n",
      "Epoch 213/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9057.0430 - lr: 1.0000e-04\n",
      "Epoch 214/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9663.6953 - lr: 1.0000e-04\n",
      "Epoch 215/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9612.9922 - lr: 1.0000e-04\n",
      "Epoch 216/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9217.2441 - lr: 1.0000e-04\n",
      "Epoch 217/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9052.1201 - lr: 1.0000e-04\n",
      "Epoch 218/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9961.1865 - lr: 1.0000e-04\n",
      "Epoch 219/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10183.2617 - lr: 1.0000e-04\n",
      "Epoch 220/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8982.9111 - lr: 1.0000e-04\n",
      "Epoch 221/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6438.1440 - lr: 1.0000e-05\n",
      "Epoch 222/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6606.4277 - lr: 1.0000e-05\n",
      "Epoch 223/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6478.5791 - lr: 1.0000e-05\n",
      "Epoch 224/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6625.0425 - lr: 1.0000e-05\n",
      "Epoch 225/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6577.6924 - lr: 1.0000e-05\n",
      "Epoch 226/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6647.1738 - lr: 1.0000e-05\n",
      "Epoch 227/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6547.4600 - lr: 1.0000e-05\n",
      "Epoch 228/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6655.7251 - lr: 1.0000e-05\n",
      "Epoch 229/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6643.2896 - lr: 1.0000e-05\n",
      "Epoch 230/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6488.0527 - lr: 1.0000e-05\n",
      "Epoch 231/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6521.7861 - lr: 1.0000e-05\n",
      "Epoch 232/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6096.1143 - lr: 1.0000e-06\n",
      "Epoch 233/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6075.9053 - lr: 1.0000e-06\n",
      "Epoch 234/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6198.9951 - lr: 1.0000e-06\n",
      "Epoch 235/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6146.9995 - lr: 1.0000e-06\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 59s 70ms/step - loss: 6119.5420 - lr: 1.0000e-06\n",
      "Epoch 237/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6203.2070 - lr: 1.0000e-06\n",
      "Epoch 238/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6058.6323 - lr: 1.0000e-06\n",
      "Epoch 239/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6127.2612 - lr: 1.0000e-06\n",
      "Epoch 240/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6130.9951 - lr: 1.0000e-06\n",
      "Epoch 241/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6223.1987 - lr: 1.0000e-06\n",
      "Epoch 242/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6148.6343 - lr: 1.0000e-06\n",
      "Epoch 243/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6204.4824 - lr: 1.0000e-06\n",
      "Epoch 244/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6147.8940 - lr: 1.0000e-06\n",
      "Epoch 245/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6097.5488 - lr: 1.0000e-06\n",
      "Epoch 246/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6065.1543 - lr: 1.0000e-06\n",
      "Epoch 247/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6206.9229 - lr: 1.0000e-06\n",
      "Epoch 248/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6113.8306 - lr: 1.0000e-06\n",
      "Epoch 249/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6149.2847 - lr: 1.0000e-07\n",
      "Epoch 250/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6023.7485 - lr: 1.0000e-07\n",
      "Epoch 251/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6005.9087 - lr: 1.0000e-07\n",
      "Epoch 252/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6278.3135 - lr: 1.0000e-07\n",
      "Epoch 253/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6176.8765 - lr: 1.0000e-07\n",
      "Epoch 254/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6041.9360 - lr: 1.0000e-07\n",
      "Epoch 255/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6155.3838 - lr: 1.0000e-07\n",
      "Epoch 256/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6092.2979 - lr: 1.0000e-07\n",
      "Epoch 257/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6174.5991 - lr: 1.0000e-07\n",
      "Epoch 258/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6181.3174 - lr: 1.0000e-07\n",
      "Epoch 259/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6157.1162 - lr: 1.0000e-07\n",
      "Epoch 260/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5989.2910 - lr: 1.0000e-07\n",
      "Epoch 261/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6080.9238 - lr: 1.0000e-07\n",
      "Epoch 262/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6197.0024 - lr: 1.0000e-07\n",
      "Epoch 263/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6052.7217 - lr: 1.0000e-07\n",
      "Epoch 264/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5922.9097 - lr: 1.0000e-07\n",
      "Epoch 265/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6088.6313 - lr: 1.0000e-07\n",
      "Epoch 266/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6107.6431 - lr: 1.0000e-07\n",
      "Epoch 267/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6074.5571 - lr: 1.0000e-07\n",
      "Epoch 268/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6108.2598 - lr: 1.0000e-07\n",
      "Epoch 269/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6222.7905 - lr: 1.0000e-07\n",
      "Epoch 270/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5936.4502 - lr: 1.0000e-07\n",
      "Epoch 271/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6152.9116 - lr: 1.0000e-07\n",
      "Epoch 272/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6081.2466 - lr: 1.0000e-07\n",
      "Epoch 273/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5976.3667 - lr: 1.0000e-07\n",
      "Epoch 274/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6133.8037 - lr: 1.0000e-07\n",
      "Epoch 275/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6125.2612 - lr: 1.0000e-08\n",
      "Epoch 276/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6102.1660 - lr: 1.0000e-08\n",
      "Epoch 277/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6095.2588 - lr: 1.0000e-08\n",
      "Epoch 278/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6122.9238 - lr: 1.0000e-08\n",
      "Epoch 279/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5984.9980 - lr: 1.0000e-08\n",
      "Epoch 280/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6098.7803 - lr: 1.0000e-08\n",
      "Epoch 281/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5995.7607 - lr: 1.0000e-08\n",
      "Epoch 282/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6074.5059 - lr: 1.0000e-08\n",
      "Epoch 283/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6029.6001 - lr: 1.0000e-08\n",
      "Epoch 284/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6092.6748 - lr: 1.0000e-08\n",
      "Epoch 285/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6150.4150 - lr: 1.0000e-09\n",
      "Epoch 286/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6060.6577 - lr: 1.0000e-09\n",
      "Epoch 287/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6048.9707 - lr: 1.0000e-09\n",
      "Epoch 288/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6138.9741 - lr: 1.0000e-09\n",
      "Epoch 289/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5960.0190 - lr: 1.0000e-09\n",
      "Epoch 290/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6159.1411 - lr: 1.0000e-09\n",
      "Epoch 291/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6074.2368 - lr: 1.0000e-09\n",
      "Epoch 292/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6019.4097 - lr: 1.0000e-09\n",
      "Epoch 293/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5963.2700 - lr: 1.0000e-09\n",
      "Epoch 294/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6095.2236 - lr: 1.0000e-09\n",
      "Epoch 295/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 5991.3281 - lr: 1.0000e-10\n",
      "Epoch 296/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6092.5137 - lr: 1.0000e-10\n",
      "Epoch 297/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6113.0439 - lr: 1.0000e-10\n",
      "Epoch 298/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6054.8774 - lr: 1.0000e-10\n",
      "Epoch 299/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6175.1768 - lr: 1.0000e-10\n",
      "Epoch 300/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 6121.3560 - lr: 1.0000e-10\n"
     ]
    }
   ],
   "source": [
    "model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m2.save('test_05_model_2.h5')\n",
    "\n",
    "\n",
    "#batch size: 调小\n",
    "#删除所有正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "# m2.save('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m2.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_2 = pd.DataFrame(y_test[:,1],columns=output_col)\n",
    "# y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAssBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      63689.441406  63664.2\n",
      "1      53102.320312  53156.4\n",
      "2      61009.785156  60857.7\n",
      "3      63837.988281  63804.1\n",
      "4      66185.070312  66056.1\n",
      "...             ...      ...\n",
      "11995  62965.394531  63212.1\n",
      "11996  63918.566406  63966.7\n",
      "11997  63358.246094  63355.0\n",
      "11998  62936.406250  63106.4\n",
      "11999  62718.027344  62843.1\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(0.0, 100.0]         7552\n",
      "(100.0, 200.0]       2952\n",
      "(200.0, 300.0]        942\n",
      "(300.0, 400.0]        337\n",
      "(400.0, 500.0]         98\n",
      "(500.0, 600.0]         57\n",
      "(600.0, 700.0]         21\n",
      "(700.0, 800.0]         17\n",
      "(800.0, 900.0]         12\n",
      "(1000.0, 2000.0]        8\n",
      "(900.0, 1000.0]         4\n",
      "(2000.0, 3000.0]        0\n",
      "(3000.0, 4000.0]        0\n",
      "(4000.0, 5000.0]        0\n",
      "(5000.0, 10000.0]       0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63689.441406</td>\n",
       "      <td>63664.2</td>\n",
       "      <td>25.241406</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53102.320312</td>\n",
       "      <td>53156.4</td>\n",
       "      <td>-54.079688</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61009.785156</td>\n",
       "      <td>60857.7</td>\n",
       "      <td>152.085156</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63837.988281</td>\n",
       "      <td>63804.1</td>\n",
       "      <td>33.888281</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66185.070312</td>\n",
       "      <td>66056.1</td>\n",
       "      <td>128.970312</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>62965.394531</td>\n",
       "      <td>63212.1</td>\n",
       "      <td>-246.705469</td>\n",
       "      <td>(200.0, 300.0]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>63918.566406</td>\n",
       "      <td>63966.7</td>\n",
       "      <td>-48.133594</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>63358.246094</td>\n",
       "      <td>63355.0</td>\n",
       "      <td>3.246094</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>62936.406250</td>\n",
       "      <td>63106.4</td>\n",
       "      <td>-169.993750</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>62718.027344</td>\n",
       "      <td>62843.1</td>\n",
       "      <td>-125.072656</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test     v_error   v_error_range v_error_label\n",
       "0      63689.441406  63664.2   25.241406    (0.0, 100.0]           1.0\n",
       "1      53102.320312  53156.4  -54.079688    (0.0, 100.0]           1.0\n",
       "2      61009.785156  60857.7  152.085156  (100.0, 200.0]           2.0\n",
       "3      63837.988281  63804.1   33.888281    (0.0, 100.0]           1.0\n",
       "4      66185.070312  66056.1  128.970312  (100.0, 200.0]           2.0\n",
       "...             ...      ...         ...             ...           ...\n",
       "11995  62965.394531  63212.1 -246.705469  (200.0, 300.0]           3.0\n",
       "11996  63918.566406  63966.7  -48.133594    (0.0, 100.0]           1.0\n",
       "11997  63358.246094  63355.0    3.246094    (0.0, 100.0]           1.0\n",
       "11998  62936.406250  63106.4 -169.993750  (100.0, 200.0]           2.0\n",
       "11999  62718.027344  62843.1 -125.072656  (100.0, 200.0]           2.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxPinBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_2[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 39053308.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 873556.3750 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 613291.2500 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 557919.0000 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 686203.4375 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 419067.8750 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 493098.5625 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 419450.8125 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 332978.9688 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 398639.9062 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 380857.9062 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 295207.8125 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 385363.9375 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 305067.7812 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 250729.6875 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 275834.8125 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 219926.7500 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 235695.8438 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 193487.1562 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 158215.1250 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 282004.0312 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 160408.6406 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 174266.5312 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 181844.6562 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 151903.0000 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 133897.5781 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 131508.5781 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 115033.6094 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 117098.5781 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 120702.3750 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 111030.6875 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 113420.1250 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 122179.6797 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 94636.1016 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 110539.7031 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 82722.0547 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 90894.0234 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 90002.1484 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 84480.1953 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 70344.0625 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 81104.6328 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 72989.5312 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 64217.4023 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 78557.7969 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 86798.8125 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 65296.3594 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 65081.7227 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 61125.3750 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 68511.9141 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 63670.5195 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 74892.0781 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 61040.3711 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 52965.0781 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 65742.2734 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 57917.4570 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 57421.3164 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 60s 71ms/step - loss: 66520.1797 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 60s 71ms/step - loss: 54825.3789 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 54919.0156 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 67597.2812 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 56414.8320 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 60s 71ms/step - loss: 42387.8125 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 57822.5312 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 59370.2461 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 47318.1992 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 56128.0664 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 47966.6719 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 51683.2461 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 46086.6211 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 44420.2656 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 52294.6523 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 53794.5156 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10985.2354 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10157.8145 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9995.8984 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10720.1797 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9655.8682 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 10825.2490 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 9735.5771 - lr: 1.0000e-04\n",
      "Epoch 80/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 59s 69ms/step - loss: 10171.9619 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9978.3223 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9448.7070 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9055.6387 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9041.9434 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 10459.6191 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9701.4199 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8403.9248 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8897.0586 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9908.3096 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9189.9385 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8765.0723 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 8837.6436 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8318.5195 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8579.5947 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7875.9863 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8402.5225 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8610.9062 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9011.2441 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7296.8208 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 9443.3359 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8053.1904 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7490.0205 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7456.3755 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7566.1826 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7940.5898 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8412.8008 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7745.5557 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 7429.7495 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 8133.8315 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4854.4043 - lr: 1.0000e-05\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4875.3931 - lr: 1.0000e-05\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4806.3198 - lr: 1.0000e-05\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4936.9966 - lr: 1.0000e-05\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4923.7778 - lr: 1.0000e-05\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4910.7671 - lr: 1.0000e-05\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4996.2329 - lr: 1.0000e-05\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4636.1147 - lr: 1.0000e-05\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4986.6675 - lr: 1.0000e-05\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4825.6016 - lr: 1.0000e-05\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 5033.2974 - lr: 1.0000e-05\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4886.5273 - lr: 1.0000e-05\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4741.1997 - lr: 1.0000e-05\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4879.2925 - lr: 1.0000e-05\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4707.9595 - lr: 1.0000e-05\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4697.3931 - lr: 1.0000e-05\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4784.0361 - lr: 1.0000e-05\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4774.4849 - lr: 1.0000e-05\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4364.4800 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4380.5635 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4420.5933 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4364.9912 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4401.8950 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4433.2407 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4346.0161 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4355.7080 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4443.2876 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4356.2119 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4348.1899 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4443.7876 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4333.0820 - lr: 1.0000e-06\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4343.6328 - lr: 1.0000e-06\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4424.7021 - lr: 1.0000e-06\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4380.1118 - lr: 1.0000e-06\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4368.7153 - lr: 1.0000e-06\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4266.2808 - lr: 1.0000e-06\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4389.6011 - lr: 1.0000e-06\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4429.2227 - lr: 1.0000e-06\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4397.2544 - lr: 1.0000e-06\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4393.6055 - lr: 1.0000e-06\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4348.3325 - lr: 1.0000e-06\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4354.7285 - lr: 1.0000e-06\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4498.8247 - lr: 1.0000e-06\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4240.5518 - lr: 1.0000e-06\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4329.0781 - lr: 1.0000e-06\n",
      "Epoch 155/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4221.7598 - lr: 1.0000e-06\n",
      "Epoch 156/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4321.0923 - lr: 1.0000e-06\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 59s 69ms/step - loss: 4259.6147 - lr: 1.0000e-06\n",
      "Epoch 158/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4449.7056 - lr: 1.0000e-06\n",
      "Epoch 159/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4235.6929 - lr: 1.0000e-06\n",
      "Epoch 160/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4426.9185 - lr: 1.0000e-06\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4352.0718 - lr: 1.0000e-06\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4335.7373 - lr: 1.0000e-06\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4372.4600 - lr: 1.0000e-06\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4385.3086 - lr: 1.0000e-06\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4377.8110 - lr: 1.0000e-06\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4337.0547 - lr: 1.0000e-07\n",
      "Epoch 167/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4311.5747 - lr: 1.0000e-07\n",
      "Epoch 168/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4274.1182 - lr: 1.0000e-07\n",
      "Epoch 169/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4370.4111 - lr: 1.0000e-07\n",
      "Epoch 170/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4364.1284 - lr: 1.0000e-07\n",
      "Epoch 171/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4222.2559 - lr: 1.0000e-07\n",
      "Epoch 172/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4238.5474 - lr: 1.0000e-07\n",
      "Epoch 173/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4294.5688 - lr: 1.0000e-07\n",
      "Epoch 174/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4243.0552 - lr: 1.0000e-07\n",
      "Epoch 175/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4225.5981 - lr: 1.0000e-07\n",
      "Epoch 176/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4262.3408 - lr: 1.0000e-08\n",
      "Epoch 177/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4306.1772 - lr: 1.0000e-08\n",
      "Epoch 178/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4413.2646 - lr: 1.0000e-08\n",
      "Epoch 179/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4373.5068 - lr: 1.0000e-08\n",
      "Epoch 180/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4207.5640 - lr: 1.0000e-08\n",
      "Epoch 181/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4232.3135 - lr: 1.0000e-08\n",
      "Epoch 182/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4134.4277 - lr: 1.0000e-08\n",
      "Epoch 183/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4214.7773 - lr: 1.0000e-08\n",
      "Epoch 184/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4182.1855 - lr: 1.0000e-08\n",
      "Epoch 185/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4324.2510 - lr: 1.0000e-08\n",
      "Epoch 186/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4269.9854 - lr: 1.0000e-08\n",
      "Epoch 187/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4342.3130 - lr: 1.0000e-08\n",
      "Epoch 188/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4293.3325 - lr: 1.0000e-08\n",
      "Epoch 189/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4157.4678 - lr: 1.0000e-08\n",
      "Epoch 190/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4301.3755 - lr: 1.0000e-08\n",
      "Epoch 191/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4245.4858 - lr: 1.0000e-08\n",
      "Epoch 192/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4303.8413 - lr: 1.0000e-08\n",
      "Epoch 193/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4302.6123 - lr: 1.0000e-09\n",
      "Epoch 194/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4304.1924 - lr: 1.0000e-09\n",
      "Epoch 195/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4276.8354 - lr: 1.0000e-09\n",
      "Epoch 196/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4296.7437 - lr: 1.0000e-09\n",
      "Epoch 197/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4334.7388 - lr: 1.0000e-09\n",
      "Epoch 198/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4353.7319 - lr: 1.0000e-09\n",
      "Epoch 199/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4245.3003 - lr: 1.0000e-09\n",
      "Epoch 200/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4300.6235 - lr: 1.0000e-09\n",
      "Epoch 201/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4286.4150 - lr: 1.0000e-09\n",
      "Epoch 202/300\n",
      "844/844 [==============================] - ETA: 0s - loss: 4328.34 - 59s 70ms/step - loss: 4328.3452 - lr: 1.0000e-09\n",
      "Epoch 203/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4232.6763 - lr: 1.0000e-10\n",
      "Epoch 204/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4316.1284 - lr: 1.0000e-10\n",
      "Epoch 205/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4402.6719 - lr: 1.0000e-10\n",
      "Epoch 206/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4371.9893 - lr: 1.0000e-10\n",
      "Epoch 207/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4255.9136 - lr: 1.0000e-10\n",
      "Epoch 208/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4415.1831 - lr: 1.0000e-10\n",
      "Epoch 209/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4294.0034 - lr: 1.0000e-10\n",
      "Epoch 210/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4276.1714 - lr: 1.0000e-10\n",
      "Epoch 211/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4263.1953 - lr: 1.0000e-10\n",
      "Epoch 212/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4220.3652 - lr: 1.0000e-10\n",
      "Epoch 213/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4238.2046 - lr: 1.0000e-11\n",
      "Epoch 214/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4261.6968 - lr: 1.0000e-11\n",
      "Epoch 215/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4353.3271 - lr: 1.0000e-11\n",
      "Epoch 216/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4261.1738 - lr: 1.0000e-11\n",
      "Epoch 217/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4272.6709 - lr: 1.0000e-11\n",
      "Epoch 218/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4278.8857 - lr: 1.0000e-11\n",
      "Epoch 219/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4325.1855 - lr: 1.0000e-11\n",
      "Epoch 220/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4312.5190 - lr: 1.0000e-11\n",
      "Epoch 221/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4251.7354 - lr: 1.0000e-11\n",
      "Epoch 222/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4326.1514 - lr: 1.0000e-11\n",
      "Epoch 223/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4222.4155 - lr: 1.0000e-12\n",
      "Epoch 224/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4336.3813 - lr: 1.0000e-12\n",
      "Epoch 225/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4281.8569 - lr: 1.0000e-12\n",
      "Epoch 226/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4297.7920 - lr: 1.0000e-12\n",
      "Epoch 227/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4331.4033 - lr: 1.0000e-12\n",
      "Epoch 228/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4317.8091 - lr: 1.0000e-12\n",
      "Epoch 229/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4295.8691 - lr: 1.0000e-12\n",
      "Epoch 230/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4363.3086 - lr: 1.0000e-12\n",
      "Epoch 231/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4184.7354 - lr: 1.0000e-12\n",
      "Epoch 232/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4235.1616 - lr: 1.0000e-12\n",
      "Epoch 233/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4368.0186 - lr: 1.0000e-13\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 59s 70ms/step - loss: 4270.6655 - lr: 1.0000e-13\n",
      "Epoch 235/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4246.1250 - lr: 1.0000e-13\n",
      "Epoch 236/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4225.7881 - lr: 1.0000e-13\n",
      "Epoch 237/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4352.1104 - lr: 1.0000e-13\n",
      "Epoch 238/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4271.5928 - lr: 1.0000e-13\n",
      "Epoch 239/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4243.2334 - lr: 1.0000e-13\n",
      "Epoch 240/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4418.4668 - lr: 1.0000e-13\n",
      "Epoch 241/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4315.2153 - lr: 1.0000e-13\n",
      "Epoch 242/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4238.2549 - lr: 1.0000e-13\n",
      "Epoch 243/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4301.2700 - lr: 1.0000e-14\n",
      "Epoch 244/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4145.5610 - lr: 1.0000e-14\n",
      "Epoch 245/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4366.4697 - lr: 1.0000e-14\n",
      "Epoch 246/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4378.6528 - lr: 1.0000e-14\n",
      "Epoch 247/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4225.2671 - lr: 1.0000e-14\n",
      "Epoch 248/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4291.7051 - lr: 1.0000e-14\n",
      "Epoch 249/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4347.3931 - lr: 1.0000e-14\n",
      "Epoch 250/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4284.0425 - lr: 1.0000e-14\n",
      "Epoch 251/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4254.2349 - lr: 1.0000e-14\n",
      "Epoch 252/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4395.1548 - lr: 1.0000e-14\n",
      "Epoch 253/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4304.6289 - lr: 1.0000e-15\n",
      "Epoch 254/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4377.0410 - lr: 1.0000e-15\n",
      "Epoch 255/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4346.4292 - lr: 1.0000e-15\n",
      "Epoch 256/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4207.2847 - lr: 1.0000e-15\n",
      "Epoch 257/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4296.7031 - lr: 1.0000e-15\n",
      "Epoch 258/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4305.0952 - lr: 1.0000e-15\n",
      "Epoch 259/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4348.8076 - lr: 1.0000e-15\n",
      "Epoch 260/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4293.0469 - lr: 1.0000e-15\n",
      "Epoch 261/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4264.7090 - lr: 1.0000e-15\n",
      "Epoch 262/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4198.7856 - lr: 1.0000e-15\n",
      "Epoch 263/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4246.1299 - lr: 1.0000e-16\n",
      "Epoch 264/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4299.3042 - lr: 1.0000e-16\n",
      "Epoch 265/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4248.3491 - lr: 1.0000e-16\n",
      "Epoch 266/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4324.1592 - lr: 1.0000e-16\n",
      "Epoch 267/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4214.4541 - lr: 1.0000e-16\n",
      "Epoch 268/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4283.5195 - lr: 1.0000e-16\n",
      "Epoch 269/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4382.2466 - lr: 1.0000e-16\n",
      "Epoch 270/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4245.0649 - lr: 1.0000e-16\n",
      "Epoch 271/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4277.1831 - lr: 1.0000e-16\n",
      "Epoch 272/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4347.8750 - lr: 1.0000e-16\n",
      "Epoch 273/300\n",
      "844/844 [==============================] - 59s 69ms/step - loss: 4213.9941 - lr: 1.0000e-17\n",
      "Epoch 274/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4257.0654 - lr: 1.0000e-17\n",
      "Epoch 275/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4292.4360 - lr: 1.0000e-17\n",
      "Epoch 276/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4343.3950 - lr: 1.0000e-17\n",
      "Epoch 277/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4341.3398 - lr: 1.0000e-17\n",
      "Epoch 278/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4324.7617 - lr: 1.0000e-17\n",
      "Epoch 279/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4283.7725 - lr: 1.0000e-17\n",
      "Epoch 280/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4247.8364 - lr: 1.0000e-17\n",
      "Epoch 281/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4305.2197 - lr: 1.0000e-17\n",
      "Epoch 282/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4241.5776 - lr: 1.0000e-17\n",
      "Epoch 283/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4331.0718 - lr: 1.0000e-18\n",
      "Epoch 284/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4254.7471 - lr: 1.0000e-18\n",
      "Epoch 285/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4139.4492 - lr: 1.0000e-18\n",
      "Epoch 286/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4219.5728 - lr: 1.0000e-18\n",
      "Epoch 287/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4350.8496 - lr: 1.0000e-18\n",
      "Epoch 288/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4248.3809 - lr: 1.0000e-18\n",
      "Epoch 289/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4245.3779 - lr: 1.0000e-18\n",
      "Epoch 290/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4376.8774 - lr: 1.0000e-18\n",
      "Epoch 291/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4368.9307 - lr: 1.0000e-18\n",
      "Epoch 292/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4376.6953 - lr: 1.0000e-18\n",
      "Epoch 293/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4157.7847 - lr: 1.0000e-19\n",
      "Epoch 294/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4305.8696 - lr: 1.0000e-19\n",
      "Epoch 295/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4249.7422 - lr: 1.0000e-19\n",
      "Epoch 296/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4245.3672 - lr: 1.0000e-19\n",
      "Epoch 297/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4296.6743 - lr: 1.0000e-19\n",
      "Epoch 298/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4240.0659 - lr: 1.0000e-19\n",
      "Epoch 299/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4210.3110 - lr: 1.0000e-19\n",
      "Epoch 300/300\n",
      "844/844 [==============================] - 59s 70ms/step - loss: 4352.8301 - lr: 1.0000e-19\n"
     ]
    }
   ],
   "source": [
    "model = m.fit(x_pre_train,  y_pre_train[:,0], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m.save('test_05_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = load_model('test_05_model.h5')\n",
    "# m2 = load_model('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57732. , 63664.2],\n",
       "       [49679. , 53156.4],\n",
       "       [54567. , 60857.7],\n",
       "       ...,\n",
       "       [57982. , 63355. ],\n",
       "       [57080. , 63106.4],\n",
       "       [56720. , 62843.1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_1 = pd.DataFrame(y_test[:,0],columns=output_col)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAssBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      57727.910156  57732.0\n",
      "1      49723.781250  49679.0\n",
      "2      54576.582031  54567.0\n",
      "3      56116.050781  56139.0\n",
      "4      62273.082031  62291.0\n",
      "...             ...      ...\n",
      "11995  59869.902344  59847.0\n",
      "11996  57757.000000  57746.0\n",
      "11997  58024.812500  57982.0\n",
      "11998  57034.136719  57080.0\n",
      "11999  56621.734375  56720.0\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(0.0, 100.0]         11182\n",
      "(100.0, 200.0]         765\n",
      "(200.0, 300.0]          39\n",
      "(300.0, 400.0]           4\n",
      "(500.0, 600.0]           4\n",
      "(800.0, 900.0]           2\n",
      "(400.0, 500.0]           1\n",
      "(600.0, 700.0]           1\n",
      "(1000.0, 2000.0]         1\n",
      "(2000.0, 3000.0]         1\n",
      "(700.0, 800.0]           0\n",
      "(900.0, 1000.0]          0\n",
      "(3000.0, 4000.0]         0\n",
      "(4000.0, 5000.0]         0\n",
      "(5000.0, 10000.0]        0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57727.910156</td>\n",
       "      <td>57732.0</td>\n",
       "      <td>-4.089844</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49723.781250</td>\n",
       "      <td>49679.0</td>\n",
       "      <td>44.781250</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54576.582031</td>\n",
       "      <td>54567.0</td>\n",
       "      <td>9.582031</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56116.050781</td>\n",
       "      <td>56139.0</td>\n",
       "      <td>-22.949219</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62273.082031</td>\n",
       "      <td>62291.0</td>\n",
       "      <td>-17.917969</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>59869.902344</td>\n",
       "      <td>59847.0</td>\n",
       "      <td>22.902344</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>57757.000000</td>\n",
       "      <td>57746.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>58024.812500</td>\n",
       "      <td>57982.0</td>\n",
       "      <td>42.812500</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>57034.136719</td>\n",
       "      <td>57080.0</td>\n",
       "      <td>-45.863281</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>56621.734375</td>\n",
       "      <td>56720.0</td>\n",
       "      <td>-98.265625</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test    v_error v_error_range v_error_label\n",
       "0      57727.910156  57732.0  -4.089844  (0.0, 100.0]           1.0\n",
       "1      49723.781250  49679.0  44.781250  (0.0, 100.0]           1.0\n",
       "2      54576.582031  54567.0   9.582031  (0.0, 100.0]           1.0\n",
       "3      56116.050781  56139.0 -22.949219  (0.0, 100.0]           1.0\n",
       "4      62273.082031  62291.0 -17.917969  (0.0, 100.0]           1.0\n",
       "...             ...      ...        ...           ...           ...\n",
       "11995  59869.902344  59847.0  22.902344  (0.0, 100.0]           1.0\n",
       "11996  57757.000000  57746.0  11.000000  (0.0, 100.0]           1.0\n",
       "11997  58024.812500  57982.0  42.812500  (0.0, 100.0]           1.0\n",
       "11998  57034.136719  57080.0 -45.863281  (0.0, 100.0]           1.0\n",
       "11999  56621.734375  56720.0 -98.265625  (0.0, 100.0]           1.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxAssBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_1[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
