{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexhang/project/Nuclear'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kinf_1</th>\n",
       "      <th>kinf_2</th>\n",
       "      <th>kinf_3</th>\n",
       "      <th>kinf_4</th>\n",
       "      <th>kinf_5</th>\n",
       "      <th>kinf_6</th>\n",
       "      <th>kinf_7</th>\n",
       "      <th>kinf_8</th>\n",
       "      <th>kinf_9</th>\n",
       "      <th>kinf_10</th>\n",
       "      <th>...</th>\n",
       "      <th>NODE2DBU_95</th>\n",
       "      <th>NODE2DBU_96</th>\n",
       "      <th>NODE2DBU_97</th>\n",
       "      <th>NODE2DBU_98</th>\n",
       "      <th>NODE2DBU_99</th>\n",
       "      <th>NODE2DBU_100</th>\n",
       "      <th>NODE2DBU_101</th>\n",
       "      <th>NODE2DBU_102</th>\n",
       "      <th>NODE2DBU_103</th>\n",
       "      <th>NODE2DBU_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.16708</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.18090</td>\n",
       "      <td>1.16216</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.08632</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26837.0</td>\n",
       "      <td>26864.0</td>\n",
       "      <td>26834.0</td>\n",
       "      <td>26862.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1.15988</td>\n",
       "      <td>1.07458</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.15534</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>1.19730</td>\n",
       "      <td>1.15417</td>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.20422</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.06296</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1.06622</td>\n",
       "      <td>1.16655</td>\n",
       "      <td>1.20028</td>\n",
       "      <td>1.19485</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15935</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>43835.0</td>\n",
       "      <td>39276.0</td>\n",
       "      <td>42082.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         kinf_1   kinf_2   kinf_3   kinf_4  kinf_5  kinf_6   kinf_7  kinf_8  \\\n",
       "0       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "1       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "2       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "3       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "4       1.16708  1.08099  1.18090  1.16216  1.4279  1.1821  1.08632  1.4279   \n",
       "...         ...      ...      ...      ...     ...     ...      ...     ...   \n",
       "119995  1.15988  1.07458  1.09861  1.15534  1.4279  1.1821  1.07427  1.4279   \n",
       "119996  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119997  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119998  1.19730  1.15417  1.21509  1.20422  1.4279  1.1821  1.06296  1.4279   \n",
       "119999  1.06622  1.16655  1.20028  1.19485  1.4279  1.1821  1.15935  1.4279   \n",
       "\n",
       "        kinf_9  kinf_10  ...  NODE2DBU_95  NODE2DBU_96  NODE2DBU_97  \\\n",
       "0       1.1821   1.1821  ...          0.0          0.0      36445.0   \n",
       "1       1.1821   1.1821  ...          0.0          0.0      36809.0   \n",
       "2       1.1821   1.1821  ...          0.0          0.0      31836.0   \n",
       "3       1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "4       1.1821   1.1821  ...          0.0          0.0      26837.0   \n",
       "...        ...      ...  ...          ...          ...          ...   \n",
       "119995  1.1821   1.1821  ...          0.0          0.0      26191.0   \n",
       "119996  1.1821   1.1821  ...          0.0          0.0      43151.0   \n",
       "119997  1.1821   1.1821  ...          0.0          0.0      43091.0   \n",
       "119998  1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "119999  1.1821   1.1821  ...          0.0          0.0      42090.0   \n",
       "\n",
       "        NODE2DBU_98  NODE2DBU_99  NODE2DBU_100  NODE2DBU_101  NODE2DBU_102  \\\n",
       "0           31803.0      36809.0       31836.0       20806.0       20806.0   \n",
       "1           36445.0      31836.0       31803.0       20806.0       20806.0   \n",
       "2           36809.0      31803.0       36445.0       20806.0       20806.0   \n",
       "3           31836.0      36445.0       36809.0       20806.0       20806.0   \n",
       "4           26864.0      26834.0       26862.0       22008.0       22008.0   \n",
       "...             ...          ...           ...           ...           ...   \n",
       "119995      26191.0      26200.0       26200.0       36146.0       36146.0   \n",
       "119996      45258.0      39658.0       43091.0       36186.0       36186.0   \n",
       "119997      39658.0      45258.0       43151.0       36186.0       36186.0   \n",
       "119998      31836.0      36445.0       36809.0       25238.0       25238.0   \n",
       "119999      43835.0      39276.0       42082.0       21568.0       21568.0   \n",
       "\n",
       "        NODE2DBU_103  NODE2DBU_104  \n",
       "0            20806.0       20806.0  \n",
       "1            20806.0       20806.0  \n",
       "2            20806.0       20806.0  \n",
       "3            20806.0       20806.0  \n",
       "4            22008.0       22008.0  \n",
       "...              ...           ...  \n",
       "119995       36146.0       36146.0  \n",
       "119996       36186.0       36186.0  \n",
       "119997       36186.0       36186.0  \n",
       "119998       25238.0       25238.0  \n",
       "119999       21568.0       21568.0  \n",
       "\n",
       "[120000 rows x 156 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入处理好的数据\n",
    "file_base_name = 'nuclear_burnup_data_20201215.csv'\n",
    "file_input_name = 'df.csv'\n",
    "\n",
    "pre_data_X = pd.read_csv(os.path.join(path, file_input_name), index_col=0)\n",
    "pre_data_base = pd.read_csv(os.path.join(path, file_base_name))\n",
    "pre_data_X.columns.values.tolist()  \n",
    "pre_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAssBurnupCal</th>\n",
       "      <th>MaxPinBurnupCal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56624</td>\n",
       "      <td>62467.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56812</td>\n",
       "      <td>61980.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56724</td>\n",
       "      <td>62521.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56537</td>\n",
       "      <td>62195.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57424</td>\n",
       "      <td>64025.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>57359</td>\n",
       "      <td>63599.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>59562</td>\n",
       "      <td>64865.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>59631</td>\n",
       "      <td>63716.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>61714</td>\n",
       "      <td>65601.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>61843</td>\n",
       "      <td>66948.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAssBurnupCal  MaxPinBurnupCal\n",
       "0                 56624          62467.5\n",
       "1                 56812          61980.1\n",
       "2                 56724          62521.4\n",
       "3                 56537          62195.7\n",
       "4                 57424          64025.7\n",
       "...                 ...              ...\n",
       "119995            57359          63599.1\n",
       "119996            59562          64865.4\n",
       "119997            59631          63716.2\n",
       "119998            61714          65601.8\n",
       "119999            61843          66948.3\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_y = pre_data_base.iloc[:,-2:]\n",
    "pre_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = pd.concat([pre_data_X, pre_data_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 156)\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pre_data_array = pre_data.values\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "pre_data_Normalized = mm.fit_transform(pre_data_array)\n",
    "pre_data_Normalized = pre_data_Normalized[:,:-2]\n",
    "# pre_data_y = pre_data_Normalized[:,-2:]\n",
    "print(pre_data_Normalized.shape)\n",
    "print(pre_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "map1 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [2,6,11,16,20,23,25,26,25,23,20,16,11,6,2],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0]\n",
    "    \n",
    "]\n",
    "m = np.array(map1)\n",
    "print(m.shape)\n",
    "map2 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['2/0','2/2',\"6/0\",\"6/2\",'11/0','11/2','16/0','16/2',\"20/0\",'20/2','23/0','23/2','25/0','25/2','26/0','26/1',\"25/2\",'25/0','23/2','23/0','20/2','20/0','16/2','16/0','11/2','11/0','6/2','6/0','2/2','2/0'],\n",
    "    ['2/1','2/3',\"6/1\",\"6/3\",'11/1','11/3','16/1','16/3',\"20/1\",'20/3','23/1','23/3','25/1','25/3','26/2','26/3',\"25/3\",'25/1','23/3','23/1','20/3','20/1','16/3','16/1','11/3','11/1','6/3','6/1','2/3','2/1'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "      \n",
    "]\n",
    "m = np.array(map2)\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(a,alist):\n",
    "    res = []\n",
    "    for i in range(len(alist)):\n",
    "        for j in range(len(alist[i])):\n",
    "            if alist[i][j] == a:\n",
    "                res.append((i,j))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #排布input\n",
    "# def search(a,alist):\n",
    "#     res = []\n",
    "#     for i in range(len(alist)):\n",
    "#         for j in range(len(alist[i])):\n",
    "#             if alist[i][j] == a:\n",
    "#                 res.append((i,j))\n",
    "#     return res\n",
    "# new = []\n",
    "# for i,item in enumerate(pre_data_Normalized):\n",
    "#     print(\"正在处理第 \"+str(i)+' 个图像..')\n",
    "#     layer1 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer2 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer3 = [[0 for _ in range(30)] for _ in range(30)]\n",
    "# #     print(item)\n",
    "#     for j in range(item.size): #len = 156\n",
    "#         if j < 26:\n",
    "#             res = search(j+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer1[h][w] = item[j]\n",
    "#         elif j < 52:\n",
    "#             j_ = j-26\n",
    "#             res = search(j_+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer2[h][w] = item[j]\n",
    "            \n",
    "#         else:\n",
    "#             j_ = j-52\n",
    "#             number = str(j_ // 4 + 1)\n",
    "#             corner = str(j_ % 4)\n",
    "#             number_corner = number + '/' + corner\n",
    "#             res = search(number_corner,map2)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer3[h][w] = item[j]\n",
    "#     layer1_array = np.array(layer1)\n",
    "#     layer2_array = np.array(layer2)\n",
    "#     layer3_array = np.array(layer3)\n",
    "    \n",
    "#     #从1*1扩展成2*2\n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 1) \n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 0)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 1)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 0)\n",
    "    \n",
    "#     #channel 拼接\n",
    "#     tmp = np.stack((layer1_array,layer2_array,layer3_array),axis = 2)\n",
    "#     new.append(tmp)\n",
    "\n",
    "# pre_data_x = np.array(new)\n",
    "# print(pre_data_x.shape)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"pre_data_x_tmp.npy\",pre_data_x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_x = np.load(\"pre_data_x_tmp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pre_data_x.shape)\n",
    "pre_data_y = np.array(pre_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    " \n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 16)        16400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 30, 30, 32)        65568     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 30, 30, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 30, 30, 64)        262208    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              57601000  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,340,289\n",
      "Trainable params: 58,338,273\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.applications.resnet18 import ResNet18\n",
    "# from keras.applications.resnet18 import preprocess_input as preprocess_input_resnet\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "# import tensorflow.keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "def deeper_conv2D(h,w):\n",
    "    new_model = tf.keras.Sequential()\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\", activation=\"relu\"))\n",
    "    # Flatten will take our convolution filters and lay them out end to end so our dense layer can predict based on the outcomes of each\n",
    "    new_model.add(tf.keras.layers.Flatten())\n",
    "#     new_model.add(tf.keras.layers.Dense(1000,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "    new_model.add(tf.keras.layers.Dense(1000))\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(100))\n",
    "#     new_model.add(tf.keras.layers.Dense(100,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(1))\n",
    "    new_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")    \n",
    "    return new_model\n",
    "# m = deeper_conv2D(30,30)\n",
    "m = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m2 = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "seed = 2020\n",
    "x_pre_train, x_test, y_pre_train, y_test = train_test_split(pre_data_x, pre_data_y, \n",
    "                                                           random_state=seed, train_size=0.9, \n",
    "                                                           test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 17 20:39:15 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 29%   38C    P2    36W / 175W |   1646MiB /  7979MiB |      2%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       918      G   /usr/lib/xorg/Xorg                           201MiB |\r\n",
      "|    0      1426      G   /usr/lib/firefox/firefox                       2MiB |\r\n",
      "|    0      1950      G   /usr/bin/gnome-shell                         161MiB |\r\n",
      "|    0      6761      C   ...exhang/anaconda3/envs/tf_gpu/bin/python  1259MiB |\r\n",
      "|    0     18318      G   /usr/local/sunlogin/bin/sunloginclient         7MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    min_delta=0.0001, \n",
    "    cooldown=0, \n",
    "    min_lr=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 102s 121ms/step - loss: 47073908.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 105s 124ms/step - loss: 971069.1250 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 106s 126ms/step - loss: 805743.8750 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 106s 126ms/step - loss: 759739.1250 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 107s 126ms/step - loss: 650078.1250 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 107s 127ms/step - loss: 522885.0312 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 108s 128ms/step - loss: 540127.6875 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 109s 129ms/step - loss: 453399.7500 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 109s 130ms/step - loss: 488538.7812 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 319036.5625 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 111s 131ms/step - loss: 368957.2812 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 111s 131ms/step - loss: 399468.3438 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 112s 133ms/step - loss: 311701.7812 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 112s 133ms/step - loss: 323393.5625 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 113s 134ms/step - loss: 303827.9375 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 113s 134ms/step - loss: 283753.6250 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 318353.6875 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 251971.5781 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 201581.7344 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 248463.9062 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 230683.6094 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 216847.8594 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 249739.0781 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 249389.0000 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 201663.1875 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 196338.0781 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 197870.9688 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 198684.2812 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 200440.2500 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 165272.5781 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 158779.1094 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 169471.6094 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 144653.8125 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 168141.7812 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 179249.2656 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 174209.0000 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 168220.5469 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 120678.1328 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 144663.2031 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 155155.2344 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 136916.1719 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 142124.3281 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 141569.9844 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 123776.5781 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 136655.2031 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 133268.7969 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 120092.5781 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 133031.2969 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 120146.7734 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 123532.0156 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 115352.0625 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 114s 135ms/step - loss: 127609.8594 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 118497.8203 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 128564.2734 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 99869.3750 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 111995.9453 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 101901.6172 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 125183.4844 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 105476.1406 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 142977.9375 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 90432.5000 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 91187.4531 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 108636.2188 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 106138.6016 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 99376.1641 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 100431.7422 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 90296.5781 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 87484.0000 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 83380.3750 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 87853.7500 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 95200.7266 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 79237.2422 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 69769.7578 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 105224.3438 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 73570.3359 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 74024.8438 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 82928.6719 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 110s 130ms/step - loss: 89552.0391 - lr: 0.0010\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 104s 123ms/step - loss: 97034.4141 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 87927.8359 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 77274.8359 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 53700.6602 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 85220.8828 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 76681.8672 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 65894.0312 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 82437.4297 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 71266.9453 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 70767.6953 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 65602.7031 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 70809.6094 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 74842.6172 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 69568.7734 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 20179.5723 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 18272.3535 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 20319.9082 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 19513.8320 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 19431.1582 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 19957.6113 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 18471.0352 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 19270.0020 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 19289.5156 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 18664.3691 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 19069.7617 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 18772.1973 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12218.8477 - lr: 1.0000e-05\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12427.6660 - lr: 1.0000e-05\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12678.7588 - lr: 1.0000e-05\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12080.8633 - lr: 1.0000e-05\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12017.8789 - lr: 1.0000e-05\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12172.1279 - lr: 1.0000e-05\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12088.1768 - lr: 1.0000e-05\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11965.5186 - lr: 1.0000e-05\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11526.6094 - lr: 1.0000e-05\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11982.4346 - lr: 1.0000e-05\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12101.5889 - lr: 1.0000e-05\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12047.6416 - lr: 1.0000e-05\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12396.5566 - lr: 1.0000e-05\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12280.3398 - lr: 1.0000e-05\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12198.2988 - lr: 1.0000e-05\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12054.5674 - lr: 1.0000e-05\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11806.0098 - lr: 1.0000e-05\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11431.0918 - lr: 1.0000e-05\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12267.8125 - lr: 1.0000e-05\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11899.1230 - lr: 1.0000e-05\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11545.4854 - lr: 1.0000e-05\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12098.3779 - lr: 1.0000e-05\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11919.6494 - lr: 1.0000e-05\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11861.4375 - lr: 1.0000e-05\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 12104.3623 - lr: 1.0000e-05\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11566.4521 - lr: 1.0000e-05\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11698.1855 - lr: 1.0000e-05\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11811.7217 - lr: 1.0000e-05\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11398.8340 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10783.4238 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10997.8213 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10695.4121 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11114.9170 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11347.3916 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11032.1387 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11067.7607 - lr: 1.0000e-06\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11156.1719 - lr: 1.0000e-06\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11449.0771 - lr: 1.0000e-06\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11003.1025 - lr: 1.0000e-06\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10924.1875 - lr: 1.0000e-06\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11224.9199 - lr: 1.0000e-06\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10702.8945 - lr: 1.0000e-06\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10911.7705 - lr: 1.0000e-07\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11000.0928 - lr: 1.0000e-07\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10687.9570 - lr: 1.0000e-07\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10910.3174 - lr: 1.0000e-07\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10941.5381 - lr: 1.0000e-07\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10823.8896 - lr: 1.0000e-07\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11270.8828 - lr: 1.0000e-07\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10905.7764 - lr: 1.0000e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11043.1074 - lr: 1.0000e-07\n",
      "Epoch 156/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11057.2998 - lr: 1.0000e-07\n",
      "Epoch 157/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11151.8926 - lr: 1.0000e-07\n",
      "Epoch 158/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11014.9365 - lr: 1.0000e-07\n",
      "Epoch 159/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10805.0156 - lr: 1.0000e-07\n",
      "Epoch 160/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10752.9951 - lr: 1.0000e-08\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11181.8408 - lr: 1.0000e-08\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10959.8037 - lr: 1.0000e-08\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10818.6807 - lr: 1.0000e-08\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11209.2324 - lr: 1.0000e-08\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11132.9062 - lr: 1.0000e-08\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10800.3906 - lr: 1.0000e-08\n",
      "Epoch 167/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11212.7939 - lr: 1.0000e-08\n",
      "Epoch 168/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10847.1016 - lr: 1.0000e-08\n",
      "Epoch 169/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10786.6455 - lr: 1.0000e-08\n",
      "Epoch 170/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10791.3730 - lr: 1.0000e-09\n",
      "Epoch 171/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11195.0889 - lr: 1.0000e-09\n",
      "Epoch 172/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10953.0645 - lr: 1.0000e-09\n",
      "Epoch 173/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11123.0615 - lr: 1.0000e-09\n",
      "Epoch 174/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10854.4854 - lr: 1.0000e-09\n",
      "Epoch 175/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11077.6074 - lr: 1.0000e-09\n",
      "Epoch 176/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11132.2754 - lr: 1.0000e-09\n",
      "Epoch 177/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10986.3457 - lr: 1.0000e-09\n",
      "Epoch 178/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10829.4434 - lr: 1.0000e-09\n",
      "Epoch 179/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10818.6846 - lr: 1.0000e-09\n",
      "Epoch 180/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10895.0469 - lr: 1.0000e-10\n",
      "Epoch 181/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11042.7178 - lr: 1.0000e-10\n",
      "Epoch 182/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11074.4707 - lr: 1.0000e-10\n",
      "Epoch 183/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10733.6992 - lr: 1.0000e-10\n",
      "Epoch 184/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10810.2646 - lr: 1.0000e-10\n",
      "Epoch 185/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10785.3770 - lr: 1.0000e-10\n",
      "Epoch 186/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11054.5566 - lr: 1.0000e-10\n",
      "Epoch 187/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11122.6221 - lr: 1.0000e-10\n",
      "Epoch 188/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10875.9551 - lr: 1.0000e-10\n",
      "Epoch 189/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10871.3223 - lr: 1.0000e-10\n",
      "Epoch 190/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10790.2158 - lr: 1.0000e-11\n",
      "Epoch 191/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10898.3408 - lr: 1.0000e-11\n",
      "Epoch 192/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11249.4355 - lr: 1.0000e-11\n",
      "Epoch 193/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11143.3213 - lr: 1.0000e-11\n",
      "Epoch 194/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10795.5488 - lr: 1.0000e-11\n",
      "Epoch 195/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11119.3555 - lr: 1.0000e-11\n",
      "Epoch 196/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11283.9648 - lr: 1.0000e-11\n",
      "Epoch 197/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11066.2578 - lr: 1.0000e-11\n",
      "Epoch 198/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11115.9043 - lr: 1.0000e-11\n",
      "Epoch 199/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10670.9727 - lr: 1.0000e-11\n",
      "Epoch 200/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10941.4238 - lr: 1.0000e-11\n",
      "Epoch 201/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10718.8398 - lr: 1.0000e-11\n",
      "Epoch 202/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10856.9531 - lr: 1.0000e-11\n",
      "Epoch 203/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10914.5195 - lr: 1.0000e-11\n",
      "Epoch 204/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10984.0928 - lr: 1.0000e-11\n",
      "Epoch 205/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11007.0068 - lr: 1.0000e-11\n",
      "Epoch 206/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10878.5820 - lr: 1.0000e-11\n",
      "Epoch 207/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11318.1436 - lr: 1.0000e-11\n",
      "Epoch 208/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10863.1455 - lr: 1.0000e-11\n",
      "Epoch 209/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10971.8682 - lr: 1.0000e-11\n",
      "Epoch 210/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11114.0127 - lr: 1.0000e-12\n",
      "Epoch 211/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10786.1465 - lr: 1.0000e-12\n",
      "Epoch 212/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10809.5146 - lr: 1.0000e-12\n",
      "Epoch 213/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10978.6309 - lr: 1.0000e-12\n",
      "Epoch 214/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11165.8096 - lr: 1.0000e-12\n",
      "Epoch 215/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10919.3301 - lr: 1.0000e-12\n",
      "Epoch 216/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10902.5205 - lr: 1.0000e-12\n",
      "Epoch 217/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10902.8760 - lr: 1.0000e-12\n",
      "Epoch 218/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10932.5449 - lr: 1.0000e-12\n",
      "Epoch 219/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10862.0654 - lr: 1.0000e-12\n",
      "Epoch 220/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10832.4092 - lr: 1.0000e-13\n",
      "Epoch 221/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11253.5859 - lr: 1.0000e-13\n",
      "Epoch 222/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11200.4590 - lr: 1.0000e-13\n",
      "Epoch 223/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10956.5771 - lr: 1.0000e-13\n",
      "Epoch 224/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11045.8330 - lr: 1.0000e-13\n",
      "Epoch 225/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10936.4238 - lr: 1.0000e-13\n",
      "Epoch 226/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10955.9980 - lr: 1.0000e-13\n",
      "Epoch 227/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10850.8271 - lr: 1.0000e-13\n",
      "Epoch 228/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10873.5605 - lr: 1.0000e-13\n",
      "Epoch 229/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10724.5146 - lr: 1.0000e-13\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 104s 123ms/step - loss: 10878.3311 - lr: 1.0000e-14\n",
      "Epoch 231/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11176.4893 - lr: 1.0000e-14\n",
      "Epoch 232/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10743.8096 - lr: 1.0000e-14\n",
      "Epoch 233/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11111.7412 - lr: 1.0000e-14\n",
      "Epoch 234/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11451.8623 - lr: 1.0000e-14\n",
      "Epoch 235/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11008.0703 - lr: 1.0000e-14\n",
      "Epoch 236/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10739.7666 - lr: 1.0000e-14\n",
      "Epoch 237/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10935.5234 - lr: 1.0000e-14\n",
      "Epoch 238/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10955.1553 - lr: 1.0000e-14\n",
      "Epoch 239/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10809.5215 - lr: 1.0000e-14\n",
      "Epoch 240/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11263.6289 - lr: 1.0000e-15\n",
      "Epoch 241/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10912.2754 - lr: 1.0000e-15\n",
      "Epoch 242/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10926.3154 - lr: 1.0000e-15\n",
      "Epoch 243/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10923.2275 - lr: 1.0000e-15\n",
      "Epoch 244/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11228.9434 - lr: 1.0000e-15\n",
      "Epoch 245/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10706.7236 - lr: 1.0000e-15\n",
      "Epoch 246/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10791.6523 - lr: 1.0000e-15\n",
      "Epoch 247/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11021.1631 - lr: 1.0000e-15\n",
      "Epoch 248/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10960.4219 - lr: 1.0000e-15\n",
      "Epoch 249/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10877.2529 - lr: 1.0000e-15\n",
      "Epoch 250/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10502.4316 - lr: 1.0000e-16\n",
      "Epoch 251/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10773.5664 - lr: 1.0000e-16\n",
      "Epoch 252/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10899.6152 - lr: 1.0000e-16\n",
      "Epoch 253/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11072.5508 - lr: 1.0000e-16\n",
      "Epoch 254/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10989.9521 - lr: 1.0000e-16\n",
      "Epoch 255/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11290.7031 - lr: 1.0000e-16\n",
      "Epoch 256/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10969.5107 - lr: 1.0000e-16\n",
      "Epoch 257/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10713.9639 - lr: 1.0000e-16\n",
      "Epoch 258/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11205.3984 - lr: 1.0000e-16\n",
      "Epoch 259/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10972.5508 - lr: 1.0000e-16\n",
      "Epoch 260/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11034.8555 - lr: 1.0000e-16\n",
      "Epoch 261/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11216.1396 - lr: 1.0000e-17\n",
      "Epoch 262/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11190.0293 - lr: 1.0000e-17\n",
      "Epoch 263/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11184.4639 - lr: 1.0000e-17\n",
      "Epoch 264/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10851.8857 - lr: 1.0000e-17\n",
      "Epoch 265/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11130.4502 - lr: 1.0000e-17\n",
      "Epoch 266/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10810.5898 - lr: 1.0000e-17\n",
      "Epoch 267/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10675.0713 - lr: 1.0000e-17\n",
      "Epoch 268/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10710.0459 - lr: 1.0000e-17\n",
      "Epoch 269/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10805.7197 - lr: 1.0000e-17\n",
      "Epoch 270/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10837.2598 - lr: 1.0000e-17\n",
      "Epoch 271/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10828.5537 - lr: 1.0000e-18\n",
      "Epoch 272/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10901.7900 - lr: 1.0000e-18\n",
      "Epoch 273/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10875.5713 - lr: 1.0000e-18\n",
      "Epoch 274/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10886.6533 - lr: 1.0000e-18\n",
      "Epoch 275/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11068.9424 - lr: 1.0000e-18\n",
      "Epoch 276/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10840.6533 - lr: 1.0000e-18\n",
      "Epoch 277/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10822.7471 - lr: 1.0000e-18\n",
      "Epoch 278/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10697.5098 - lr: 1.0000e-18\n",
      "Epoch 279/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11169.9287 - lr: 1.0000e-18\n",
      "Epoch 280/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10676.3682 - lr: 1.0000e-18\n",
      "Epoch 281/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10878.3682 - lr: 1.0000e-19\n",
      "Epoch 282/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11095.3281 - lr: 1.0000e-19\n",
      "Epoch 283/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10930.6865 - lr: 1.0000e-19\n",
      "Epoch 284/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11070.9287 - lr: 1.0000e-19\n",
      "Epoch 285/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11039.1582 - lr: 1.0000e-19\n",
      "Epoch 286/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11089.8906 - lr: 1.0000e-19\n",
      "Epoch 287/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11033.6963 - lr: 1.0000e-19\n",
      "Epoch 288/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11150.2822 - lr: 1.0000e-19\n",
      "Epoch 289/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11213.6182 - lr: 1.0000e-19\n",
      "Epoch 290/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11281.3428 - lr: 1.0000e-19\n",
      "Epoch 291/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11210.7559 - lr: 1.0000e-20\n",
      "Epoch 292/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11120.9287 - lr: 1.0000e-20\n",
      "Epoch 293/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11039.2627 - lr: 1.0000e-20\n",
      "Epoch 294/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10959.6299 - lr: 1.0000e-20\n",
      "Epoch 295/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11072.6973 - lr: 1.0000e-20\n",
      "Epoch 296/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11068.1719 - lr: 1.0000e-20\n",
      "Epoch 297/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11151.8428 - lr: 1.0000e-20\n",
      "Epoch 298/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11138.7275 - lr: 1.0000e-20\n",
      "Epoch 299/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 10929.5635 - lr: 1.0000e-20\n",
      "Epoch 300/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 11104.5283 - lr: 1.0000e-20\n"
     ]
    }
   ],
   "source": [
    "model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m2.save('test_05_model_2.h5')\n",
    "\n",
    "\n",
    "#batch size: 调小\n",
    "#删除所有正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "# m2.save('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m2.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_2 = pd.DataFrame(y_test[:,1],columns=output_col)\n",
    "# y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPinBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      63589.621094  63664.2\n",
      "1      52712.257812  53156.4\n",
      "2      60865.148438  60857.7\n",
      "3      63780.664062  63804.1\n",
      "4      65976.429688  66056.1\n",
      "...             ...      ...\n",
      "11995  63253.140625  63212.1\n",
      "11996  64078.726562  63966.7\n",
      "11997  63340.250000  63355.0\n",
      "11998  62961.964844  63106.4\n",
      "11999  62767.710938  62843.1\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(0.0, 100.0]         8500\n",
      "(100.0, 200.0]       2487\n",
      "(200.0, 300.0]        653\n",
      "(300.0, 400.0]        197\n",
      "(400.0, 500.0]         71\n",
      "(500.0, 600.0]         40\n",
      "(600.0, 700.0]         18\n",
      "(700.0, 800.0]         15\n",
      "(800.0, 900.0]          7\n",
      "(1000.0, 2000.0]        7\n",
      "(900.0, 1000.0]         5\n",
      "(2000.0, 3000.0]        0\n",
      "(3000.0, 4000.0]        0\n",
      "(4000.0, 5000.0]        0\n",
      "(5000.0, 10000.0]       0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63589.621094</td>\n",
       "      <td>63664.2</td>\n",
       "      <td>-74.578906</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52712.257812</td>\n",
       "      <td>53156.4</td>\n",
       "      <td>-444.142188</td>\n",
       "      <td>(400.0, 500.0]</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60865.148438</td>\n",
       "      <td>60857.7</td>\n",
       "      <td>7.448438</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63780.664062</td>\n",
       "      <td>63804.1</td>\n",
       "      <td>-23.435937</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65976.429688</td>\n",
       "      <td>66056.1</td>\n",
       "      <td>-79.670313</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>63253.140625</td>\n",
       "      <td>63212.1</td>\n",
       "      <td>41.040625</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>64078.726562</td>\n",
       "      <td>63966.7</td>\n",
       "      <td>112.026563</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>63340.250000</td>\n",
       "      <td>63355.0</td>\n",
       "      <td>-14.750000</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>62961.964844</td>\n",
       "      <td>63106.4</td>\n",
       "      <td>-144.435156</td>\n",
       "      <td>(100.0, 200.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>62767.710938</td>\n",
       "      <td>62843.1</td>\n",
       "      <td>-75.389062</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test     v_error   v_error_range v_error_label\n",
       "0      63589.621094  63664.2  -74.578906    (0.0, 100.0]           1.0\n",
       "1      52712.257812  53156.4 -444.142188  (400.0, 500.0]           5.0\n",
       "2      60865.148438  60857.7    7.448438    (0.0, 100.0]           1.0\n",
       "3      63780.664062  63804.1  -23.435937    (0.0, 100.0]           1.0\n",
       "4      65976.429688  66056.1  -79.670313    (0.0, 100.0]           1.0\n",
       "...             ...      ...         ...             ...           ...\n",
       "11995  63253.140625  63212.1   41.040625    (0.0, 100.0]           1.0\n",
       "11996  64078.726562  63966.7  112.026563  (100.0, 200.0]           2.0\n",
       "11997  63340.250000  63355.0  -14.750000    (0.0, 100.0]           1.0\n",
       "11998  62961.964844  63106.4 -144.435156  (100.0, 200.0]           2.0\n",
       "11999  62767.710938  62843.1  -75.389062    (0.0, 100.0]           1.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxPinBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_2[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 39889452.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 447793.5312 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 387510.9062 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 420412.7500 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 433653.0312 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 367672.8438 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 321767.1250 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 308791.9062 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 266290.6250 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 337976.1562 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 262415.4688 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 273903.6562 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 265372.2500 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 205731.4844 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 192564.8906 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 228193.4531 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 215763.8281 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 157360.0625 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 168968.3750 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 145732.9844 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 146605.3594 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 160248.8281 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 165874.2656 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 144022.1406 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 130039.6250 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 141428.6719 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 136455.1719 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 123416.4531 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 147953.5156 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 131839.0000 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 109712.6797 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 88867.6484 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 100153.5312 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 114455.2969 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 89084.7266 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 106174.9062 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 82329.6250 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 100435.7422 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 79662.8359 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 86627.1562 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 89723.8828 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 86575.6094 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 75722.1562 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 90249.7578 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 79343.9062 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 87230.2109 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 70923.4688 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 68956.8281 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 79473.2344 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 55317.2773 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 82369.7891 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 58117.9648 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 69805.4141 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 58623.2969 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 63034.8789 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 56660.5938 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 59186.4336 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 53818.3945 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 71418.6797 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 64319.1797 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 46173.9375 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 54607.9570 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 53490.4336 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 58539.6875 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 49906.3672 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 46319.2539 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 55768.6758 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 63506.2148 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 44832.1094 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 54789.0547 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 44477.4688 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 62558.4883 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 41414.2656 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 42429.8008 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 44644.2383 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 48128.0430 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 44403.7461 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 46395.3359 - lr: 0.0010\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 104s 123ms/step - loss: 45482.6094 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 52946.4336 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 42173.8047 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 40307.5430 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 40955.7031 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 45969.2891 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 48031.4609 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 46049.6250 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 38705.6484 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 48471.7383 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 51949.5078 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 42738.4883 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 46080.4648 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 40256.7617 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 41065.2695 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 36646.5000 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 40329.8320 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 48289.9062 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 44893.6953 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 32397.3164 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 44523.7031 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 41962.0781 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 38967.4336 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 39517.0977 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 33371.5430 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 38227.1055 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 36875.0938 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 36274.7969 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 41072.4062 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 33951.7969 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 8393.2539 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 9184.6855 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 8441.8564 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7770.0630 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 8210.9102 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7161.3237 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6939.7280 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7956.9385 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7037.7993 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7296.6396 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6979.3320 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6746.3325 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7310.4902 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 8198.0264 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6321.8135 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6437.5400 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6445.4746 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7060.5225 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6547.0933 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6717.9702 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 7447.8594 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5987.8760 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6378.3975 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6350.7563 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6551.4683 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6090.6499 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6343.0933 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6219.4727 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6171.5645 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6564.1855 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5801.9668 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6205.3174 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5887.2480 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5790.9614 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6033.2871 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5446.3491 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6057.9351 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5349.5044 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6044.9214 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5702.3735 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5681.0757 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 6093.1187 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5905.3506 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5817.9165 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5680.6606 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5999.2344 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 5469.3374 - lr: 1.0000e-04\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 104s 123ms/step - loss: 5836.2881 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 3058.3325 - lr: 1.0000e-05\n",
      "Epoch 158/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 2911.3057 - lr: 1.0000e-05\n",
      "Epoch 159/300\n",
      "844/844 [==============================] - 106s 125ms/step - loss: 3115.1541 - lr: 1.0000e-05\n",
      "Epoch 160/300\n",
      "844/844 [==============================] - 105s 125ms/step - loss: 2963.9004 - lr: 1.0000e-05\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 3141.1936 - lr: 1.0000e-05\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 2964.9846 - lr: 1.0000e-05\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 3061.1223 - lr: 1.0000e-05\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 2989.2051 - lr: 1.0000e-05\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 3107.4929 - lr: 1.0000e-05\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 104s 123ms/step - loss: 3057.6418 - lr: 1.0000e-05\n",
      "Epoch 167/300\n",
      "175/844 [=====>........................] - ETA: 1:21 - loss: 3003.4597"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c5702f9f5f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pre_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_pre_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_05_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = m.fit(x_pre_train,  y_pre_train[:,0], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m.save('test_05_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = load_model('test_05_model.h5')\n",
    "# m2 = load_model('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_1 = pd.DataFrame(y_test[:,0],columns=output_col)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MaxAssBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_1[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
