{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexhang/project/Nuclear'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kinf_1</th>\n",
       "      <th>kinf_2</th>\n",
       "      <th>kinf_3</th>\n",
       "      <th>kinf_4</th>\n",
       "      <th>kinf_5</th>\n",
       "      <th>kinf_6</th>\n",
       "      <th>kinf_7</th>\n",
       "      <th>kinf_8</th>\n",
       "      <th>kinf_9</th>\n",
       "      <th>kinf_10</th>\n",
       "      <th>...</th>\n",
       "      <th>NODE2DBU_95</th>\n",
       "      <th>NODE2DBU_96</th>\n",
       "      <th>NODE2DBU_97</th>\n",
       "      <th>NODE2DBU_98</th>\n",
       "      <th>NODE2DBU_99</th>\n",
       "      <th>NODE2DBU_100</th>\n",
       "      <th>NODE2DBU_101</th>\n",
       "      <th>NODE2DBU_102</th>\n",
       "      <th>NODE2DBU_103</th>\n",
       "      <th>NODE2DBU_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.16708</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.18090</td>\n",
       "      <td>1.16216</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.08632</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26837.0</td>\n",
       "      <td>26864.0</td>\n",
       "      <td>26834.0</td>\n",
       "      <td>26862.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1.15988</td>\n",
       "      <td>1.07458</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.15534</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>1.19730</td>\n",
       "      <td>1.15417</td>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.20422</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.06296</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1.06622</td>\n",
       "      <td>1.16655</td>\n",
       "      <td>1.20028</td>\n",
       "      <td>1.19485</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15935</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>43835.0</td>\n",
       "      <td>39276.0</td>\n",
       "      <td>42082.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         kinf_1   kinf_2   kinf_3   kinf_4  kinf_5  kinf_6   kinf_7  kinf_8  \\\n",
       "0       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "1       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "2       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "3       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "4       1.16708  1.08099  1.18090  1.16216  1.4279  1.1821  1.08632  1.4279   \n",
       "...         ...      ...      ...      ...     ...     ...      ...     ...   \n",
       "119995  1.15988  1.07458  1.09861  1.15534  1.4279  1.1821  1.07427  1.4279   \n",
       "119996  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119997  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119998  1.19730  1.15417  1.21509  1.20422  1.4279  1.1821  1.06296  1.4279   \n",
       "119999  1.06622  1.16655  1.20028  1.19485  1.4279  1.1821  1.15935  1.4279   \n",
       "\n",
       "        kinf_9  kinf_10  ...  NODE2DBU_95  NODE2DBU_96  NODE2DBU_97  \\\n",
       "0       1.1821   1.1821  ...          0.0          0.0      36445.0   \n",
       "1       1.1821   1.1821  ...          0.0          0.0      36809.0   \n",
       "2       1.1821   1.1821  ...          0.0          0.0      31836.0   \n",
       "3       1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "4       1.1821   1.1821  ...          0.0          0.0      26837.0   \n",
       "...        ...      ...  ...          ...          ...          ...   \n",
       "119995  1.1821   1.1821  ...          0.0          0.0      26191.0   \n",
       "119996  1.1821   1.1821  ...          0.0          0.0      43151.0   \n",
       "119997  1.1821   1.1821  ...          0.0          0.0      43091.0   \n",
       "119998  1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "119999  1.1821   1.1821  ...          0.0          0.0      42090.0   \n",
       "\n",
       "        NODE2DBU_98  NODE2DBU_99  NODE2DBU_100  NODE2DBU_101  NODE2DBU_102  \\\n",
       "0           31803.0      36809.0       31836.0       20806.0       20806.0   \n",
       "1           36445.0      31836.0       31803.0       20806.0       20806.0   \n",
       "2           36809.0      31803.0       36445.0       20806.0       20806.0   \n",
       "3           31836.0      36445.0       36809.0       20806.0       20806.0   \n",
       "4           26864.0      26834.0       26862.0       22008.0       22008.0   \n",
       "...             ...          ...           ...           ...           ...   \n",
       "119995      26191.0      26200.0       26200.0       36146.0       36146.0   \n",
       "119996      45258.0      39658.0       43091.0       36186.0       36186.0   \n",
       "119997      39658.0      45258.0       43151.0       36186.0       36186.0   \n",
       "119998      31836.0      36445.0       36809.0       25238.0       25238.0   \n",
       "119999      43835.0      39276.0       42082.0       21568.0       21568.0   \n",
       "\n",
       "        NODE2DBU_103  NODE2DBU_104  \n",
       "0            20806.0       20806.0  \n",
       "1            20806.0       20806.0  \n",
       "2            20806.0       20806.0  \n",
       "3            20806.0       20806.0  \n",
       "4            22008.0       22008.0  \n",
       "...              ...           ...  \n",
       "119995       36146.0       36146.0  \n",
       "119996       36186.0       36186.0  \n",
       "119997       36186.0       36186.0  \n",
       "119998       25238.0       25238.0  \n",
       "119999       21568.0       21568.0  \n",
       "\n",
       "[120000 rows x 156 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入处理好的数据\n",
    "file_base_name = 'nuclear_burnup_data_20201215.csv'\n",
    "file_input_name = 'df.csv'\n",
    "\n",
    "pre_data_X = pd.read_csv(os.path.join(path, file_input_name), index_col=0)\n",
    "pre_data_base = pd.read_csv(os.path.join(path, file_base_name))\n",
    "pre_data_X.columns.values.tolist()  \n",
    "pre_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAssBurnupCal</th>\n",
       "      <th>MaxPinBurnupCal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56624</td>\n",
       "      <td>62467.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56812</td>\n",
       "      <td>61980.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56724</td>\n",
       "      <td>62521.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56537</td>\n",
       "      <td>62195.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57424</td>\n",
       "      <td>64025.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>57359</td>\n",
       "      <td>63599.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>59562</td>\n",
       "      <td>64865.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>59631</td>\n",
       "      <td>63716.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>61714</td>\n",
       "      <td>65601.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>61843</td>\n",
       "      <td>66948.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAssBurnupCal  MaxPinBurnupCal\n",
       "0                 56624          62467.5\n",
       "1                 56812          61980.1\n",
       "2                 56724          62521.4\n",
       "3                 56537          62195.7\n",
       "4                 57424          64025.7\n",
       "...                 ...              ...\n",
       "119995            57359          63599.1\n",
       "119996            59562          64865.4\n",
       "119997            59631          63716.2\n",
       "119998            61714          65601.8\n",
       "119999            61843          66948.3\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_y = pre_data_base.iloc[:,-2:]\n",
    "pre_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kinf_1</th>\n",
       "      <th>kinf_2</th>\n",
       "      <th>kinf_3</th>\n",
       "      <th>kinf_4</th>\n",
       "      <th>kinf_5</th>\n",
       "      <th>kinf_6</th>\n",
       "      <th>kinf_7</th>\n",
       "      <th>kinf_8</th>\n",
       "      <th>kinf_9</th>\n",
       "      <th>kinf_10</th>\n",
       "      <th>...</th>\n",
       "      <th>NODE2DBU_97</th>\n",
       "      <th>NODE2DBU_98</th>\n",
       "      <th>NODE2DBU_99</th>\n",
       "      <th>NODE2DBU_100</th>\n",
       "      <th>NODE2DBU_101</th>\n",
       "      <th>NODE2DBU_102</th>\n",
       "      <th>NODE2DBU_103</th>\n",
       "      <th>NODE2DBU_104</th>\n",
       "      <th>MaxAssBurnupCal</th>\n",
       "      <th>MaxPinBurnupCal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>56624</td>\n",
       "      <td>62467.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>56812</td>\n",
       "      <td>61980.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>56724</td>\n",
       "      <td>62521.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>56537</td>\n",
       "      <td>62195.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.16708</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.18090</td>\n",
       "      <td>1.16216</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.08632</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>26837.0</td>\n",
       "      <td>26864.0</td>\n",
       "      <td>26834.0</td>\n",
       "      <td>26862.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>57424</td>\n",
       "      <td>64025.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1.15988</td>\n",
       "      <td>1.07458</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.15534</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>57359</td>\n",
       "      <td>63599.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>59562</td>\n",
       "      <td>64865.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>59631</td>\n",
       "      <td>63716.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>1.19730</td>\n",
       "      <td>1.15417</td>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.20422</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.06296</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>61714</td>\n",
       "      <td>65601.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1.06622</td>\n",
       "      <td>1.16655</td>\n",
       "      <td>1.20028</td>\n",
       "      <td>1.19485</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15935</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>43835.0</td>\n",
       "      <td>39276.0</td>\n",
       "      <td>42082.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>61843</td>\n",
       "      <td>66948.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         kinf_1   kinf_2   kinf_3   kinf_4  kinf_5  kinf_6   kinf_7  kinf_8  \\\n",
       "0       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "1       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "2       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "3       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "4       1.16708  1.08099  1.18090  1.16216  1.4279  1.1821  1.08632  1.4279   \n",
       "...         ...      ...      ...      ...     ...     ...      ...     ...   \n",
       "119995  1.15988  1.07458  1.09861  1.15534  1.4279  1.1821  1.07427  1.4279   \n",
       "119996  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119997  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119998  1.19730  1.15417  1.21509  1.20422  1.4279  1.1821  1.06296  1.4279   \n",
       "119999  1.06622  1.16655  1.20028  1.19485  1.4279  1.1821  1.15935  1.4279   \n",
       "\n",
       "        kinf_9  kinf_10  ...  NODE2DBU_97  NODE2DBU_98  NODE2DBU_99  \\\n",
       "0       1.1821   1.1821  ...      36445.0      31803.0      36809.0   \n",
       "1       1.1821   1.1821  ...      36809.0      36445.0      31836.0   \n",
       "2       1.1821   1.1821  ...      31836.0      36809.0      31803.0   \n",
       "3       1.1821   1.1821  ...      31803.0      31836.0      36445.0   \n",
       "4       1.1821   1.1821  ...      26837.0      26864.0      26834.0   \n",
       "...        ...      ...  ...          ...          ...          ...   \n",
       "119995  1.1821   1.1821  ...      26191.0      26191.0      26200.0   \n",
       "119996  1.1821   1.1821  ...      43151.0      45258.0      39658.0   \n",
       "119997  1.1821   1.1821  ...      43091.0      39658.0      45258.0   \n",
       "119998  1.1821   1.1821  ...      31803.0      31836.0      36445.0   \n",
       "119999  1.1821   1.1821  ...      42090.0      43835.0      39276.0   \n",
       "\n",
       "        NODE2DBU_100  NODE2DBU_101  NODE2DBU_102  NODE2DBU_103  NODE2DBU_104  \\\n",
       "0            31836.0       20806.0       20806.0       20806.0       20806.0   \n",
       "1            31803.0       20806.0       20806.0       20806.0       20806.0   \n",
       "2            36445.0       20806.0       20806.0       20806.0       20806.0   \n",
       "3            36809.0       20806.0       20806.0       20806.0       20806.0   \n",
       "4            26862.0       22008.0       22008.0       22008.0       22008.0   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "119995       26200.0       36146.0       36146.0       36146.0       36146.0   \n",
       "119996       43091.0       36186.0       36186.0       36186.0       36186.0   \n",
       "119997       43151.0       36186.0       36186.0       36186.0       36186.0   \n",
       "119998       36809.0       25238.0       25238.0       25238.0       25238.0   \n",
       "119999       42082.0       21568.0       21568.0       21568.0       21568.0   \n",
       "\n",
       "        MaxAssBurnupCal  MaxPinBurnupCal  \n",
       "0                 56624          62467.5  \n",
       "1                 56812          61980.1  \n",
       "2                 56724          62521.4  \n",
       "3                 56537          62195.7  \n",
       "4                 57424          64025.7  \n",
       "...                 ...              ...  \n",
       "119995            57359          63599.1  \n",
       "119996            59562          64865.4  \n",
       "119997            59631          63716.2  \n",
       "119998            61714          65601.8  \n",
       "119999            61843          66948.3  \n",
       "\n",
       "[120000 rows x 158 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data = pd.concat([pre_data_X, pre_data_y], axis=1)\n",
    "pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 156)\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pre_data_array = pre_data.values\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "pre_data_Normalized = mm.fit_transform(pre_data_array)\n",
    "pre_data_Normalized = pre_data_Normalized[:,:-2]  # 删除y目标被均一化的列\n",
    "# pre_data_y = pre_data_Normalized[:,-2:]\n",
    "print(pre_data_Normalized.shape)\n",
    "print(pre_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "map1 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [2,6,11,16,20,23,25,26,25,23,20,16,11,6,2],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0]\n",
    "    \n",
    "]\n",
    "m = np.array(map1)\n",
    "print(m.shape)\n",
    "map2 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['2/0','2/2',\"6/0\",\"6/2\",'11/0','11/2','16/0','16/2',\"20/0\",'20/2','23/0','23/2','25/0','25/2','26/0','26/1',\"25/2\",'25/0','23/2','23/0','20/2','20/0','16/2','16/0','11/2','11/0','6/2','6/0','2/2','2/0'],\n",
    "    ['2/1','2/3',\"6/1\",\"6/3\",'11/1','11/3','16/1','16/3',\"20/1\",'20/3','23/1','23/3','25/1','25/3','26/2','26/3',\"25/3\",'25/1','23/3','23/1','20/3','20/1','16/3','16/1','11/3','11/1','6/3','6/1','2/3','2/1'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "      \n",
    "]\n",
    "m = np.array(map2)\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(a,alist):\n",
    "    res = []\n",
    "    for i in range(len(alist)):\n",
    "        for j in range(len(alist[i])):\n",
    "            if alist[i][j] == a:\n",
    "                res.append((i,j))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#排布input\n",
    "# def search(a,alist):\n",
    "#     res = []\n",
    "#     for i in range(len(alist)):\n",
    "#         for j in range(len(alist[i])):\n",
    "#             if alist[i][j] == a:\n",
    "#                 res.append((i,j))\n",
    "#     return res\n",
    "# new = []\n",
    "# for i,item in enumerate(pre_data_Normalized):\n",
    "#     print(\"正在处理第 \"+str(i)+' 个图像..')\n",
    "#     layer1 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer2 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer3 = [[0 for _ in range(30)] for _ in range(30)]\n",
    "# #     print(item)\n",
    "#     for j in range(item.size): #len = 156\n",
    "#         if j < 26:\n",
    "#             res = search(j+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer1[h][w] = item[j]\n",
    "#         elif j < 52:\n",
    "#             j_ = j-26\n",
    "#             res = search(j_+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer2[h][w] = item[j]\n",
    "            \n",
    "#         else:\n",
    "#             j_ = j-52\n",
    "#             number = str(j_ // 4 + 1)\n",
    "#             corner = str(j_ % 4)\n",
    "#             number_corner = number + '/' + corner\n",
    "#             res = search(number_corner,map2)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer3[h][w] = item[j]\n",
    "#     layer1_array = np.array(layer1)\n",
    "#     layer2_array = np.array(layer2)\n",
    "#     layer3_array = np.array(layer3)\n",
    "    \n",
    "#     #从1*1扩展成2*2\n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 1) \n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 0)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 1)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 0)\n",
    "    \n",
    "#     #channel 拼接\n",
    "#     tmp = np.stack((layer1_array,layer2_array,layer3_array),axis = 2)\n",
    "#     new.append(tmp)\n",
    "\n",
    "# pre_data_x = np.array(new)\n",
    "# print(pre_data_x.shape)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"pre_data_x_tmp.npy\",pre_data_x) \n",
    "pre_data_x = np.load(\"pre_data_x_tmp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pre_data_x.shape)\n",
    "# print(pre_data_y)\n",
    "pre_data_y = np.array(pre_data_y)\n",
    "# print(pre_data_y)\n",
    "# pre_data_y = pre_data_y[0:100,:]\n",
    "# print(pre_data_y)\n",
    "# data = (pre_data_x,pre_data_y)\n",
    "# pre_data_y.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 19 09:57:43 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 29%   36C    P8     2W / 175W |    318MiB /  7979MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       918      G   /usr/lib/xorg/Xorg                           176MiB |\r\n",
      "|    0      1950      G   /usr/bin/gnome-shell                         125MiB |\r\n",
      "|    0     18318      G   /usr/local/sunlogin/bin/sunloginclient         7MiB |\r\n",
      "|    0     23353      G   /usr/lib/firefox/firefox                       2MiB |\r\n",
      "|    0     32188      G   /usr/lib/firefox/firefox                       2MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    " \n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 16)        16400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 30, 30, 32)        65568     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 30, 30, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 30, 30, 64)        262208    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              57601000  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,274,097\n",
      "Trainable params: 58,272,753\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.applications.resnet18 import ResNet18\n",
    "# from keras.applications.resnet18 import preprocess_input as preprocess_input_resnet\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# import tensorflow.keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "def deeper_conv2D(h,w):\n",
    "    new_model = tf.keras.Sequential()\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\", activation=\"relu\"))\n",
    "    # Flatten will take our convolution filters and lay them out end to end so our dense layer can predict based on the outcomes of each\n",
    "    new_model.add(tf.keras.layers.Flatten())\n",
    "#     new_model.add(tf.keras.layers.Dense(1000,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "    new_model.add(tf.keras.layers.Dense(1000))\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(100))\n",
    "#     new_model.add(tf.keras.layers.Dense(100,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(1))\n",
    "    new_model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")    \n",
    "    return new_model\n",
    "\n",
    "m = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m2 = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "seed = 2020\n",
    "X_pre_train, X_test, y_pre_train, y_test = train_test_split(pre_data_x, pre_data_y, \n",
    "                                                           random_state=seed, train_size=0.9, \n",
    "                                                           test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    min_delta=0.0001, \n",
    "    cooldown=0, \n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=200, \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义绘制history的函数\n",
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-19\n",
      "fold 1\n",
      "Epoch 1/300\n",
      "1688/1688 [==============================] - 68s 40ms/step - loss: 1638.1965 - val_loss: 5374.6909 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "1688/1688 [==============================] - 69s 41ms/step - loss: 962.8853 - val_loss: 2129.2793 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "1688/1688 [==============================] - 70s 42ms/step - loss: 826.9444 - val_loss: 3377.4258 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 813.6362 - val_loss: 1851.7732 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 641.3144 - val_loss: 806.5145 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 609.6444 - val_loss: 923.6963 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 528.0492 - val_loss: 2004.3490 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 482.2118 - val_loss: 2120.9636 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 492.7764 - val_loss: 1304.7866 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 441.8519 - val_loss: 2370.7180 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 406.6136 - val_loss: 240.0272 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 411.7629 - val_loss: 244.6326 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 397.0351 - val_loss: 223.3292 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 384.5051 - val_loss: 2824.1680 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 351.2980 - val_loss: 227.8419 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 380.4943 - val_loss: 1554.5764 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 324.3144 - val_loss: 1009.6556 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 325.0670 - val_loss: 1540.6588 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 354.1628 - val_loss: 2738.4724 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 308.9570 - val_loss: 1286.1417 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 313.0057 - val_loss: 666.8803 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 318.4270 - val_loss: 300.1390 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 274.9186 - val_loss: 445.2269 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 140.2863 - val_loss: 160.0191 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 123.3130 - val_loss: 191.2245 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 121.3197 - val_loss: 172.6103 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 114.5759 - val_loss: 99.6232 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 113.9996 - val_loss: 165.5135 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 106.3423 - val_loss: 78.1943 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 103.3807 - val_loss: 111.9497 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 102.4340 - val_loss: 255.2711 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 102.8504 - val_loss: 75.8186 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 100.4321 - val_loss: 103.8608 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 98.7398 - val_loss: 102.3741 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 98.4995 - val_loss: 170.6502 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 96.7603 - val_loss: 288.2474 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 92.3763 - val_loss: 102.0911 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 95.3935 - val_loss: 93.2090 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 93.7712 - val_loss: 348.4966 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 90.0814 - val_loss: 68.7109 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 91.5281 - val_loss: 248.6818 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 86.8840 - val_loss: 83.6451 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 88.5717 - val_loss: 142.7436 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 86.5468 - val_loss: 362.0005 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 85.6938 - val_loss: 449.8583 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 84.0136 - val_loss: 75.4744 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 84.4926 - val_loss: 76.7341 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 83.8884 - val_loss: 67.8811 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 81.9538 - val_loss: 113.1008 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 80.7738 - val_loss: 64.3608 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 81.4014 - val_loss: 379.9294 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 82.3770 - val_loss: 66.0315 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 79.1137 - val_loss: 315.1494 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 82.1926 - val_loss: 97.8088 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 79.9309 - val_loss: 104.0203 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 79.1508 - val_loss: 121.6218 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 76.7127 - val_loss: 108.9170 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 81.8796 - val_loss: 324.8147 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 75.2295 - val_loss: 152.7478 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 75.1380 - val_loss: 105.3827 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 59.7739 - val_loss: 58.8579 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.5967 - val_loss: 59.3307 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 59.2362 - val_loss: 58.6736 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 59.8696 - val_loss: 67.2839 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.5911 - val_loss: 62.8714 - lr: 1.0000e-05\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 72s 42ms/step - loss: 59.0350 - val_loss: 115.0466 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.8697 - val_loss: 62.8897 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.4367 - val_loss: 59.0628 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.1153 - val_loss: 68.5423 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.4709 - val_loss: 89.1887 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 57.4905 - val_loss: 89.8344 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.2567 - val_loss: 58.6675 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.2295 - val_loss: 57.4921 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.4619 - val_loss: 93.1609 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.6178 - val_loss: 89.3767 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 57.7388 - val_loss: 67.1901 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 57.2078 - val_loss: 57.2677 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 57.4467 - val_loss: 62.1271 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 57.1244 - val_loss: 60.2928 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 57.7139 - val_loss: 57.5318 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 57.3947 - val_loss: 68.2874 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 58.0578 - val_loss: 67.3567 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 56.8430 - val_loss: 58.1160 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 56.7648 - val_loss: 58.1911 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 56.6644 - val_loss: 65.0360 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 56.7125 - val_loss: 57.6240 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 57.1528 - val_loss: 98.5549 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.4077 - val_loss: 61.8240 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3491 - val_loss: 69.1576 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.5204 - val_loss: 65.2922 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.4504 - val_loss: 60.1191 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.8455 - val_loss: 69.4706 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.4420 - val_loss: 65.4488 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3159 - val_loss: 71.3643 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3539 - val_loss: 77.8625 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1627 - val_loss: 58.6715 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2886 - val_loss: 64.3814 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2685 - val_loss: 63.7646 - lr: 1.0000e-07\n",
      "Epoch 99/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.6384 - val_loss: 62.9912 - lr: 1.0000e-07\n",
      "Epoch 100/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8436 - val_loss: 64.1891 - lr: 1.0000e-07\n",
      "Epoch 101/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 54.1586 - val_loss: 63.1654 - lr: 1.0000e-07\n",
      "Epoch 102/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0723 - val_loss: 62.8973 - lr: 1.0000e-07\n",
      "Epoch 103/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 53.5905 - val_loss: 62.2812 - lr: 1.0000e-07\n",
      "Epoch 104/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8270 - val_loss: 64.3259 - lr: 1.0000e-07\n",
      "Epoch 105/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1705 - val_loss: 62.9104 - lr: 1.0000e-07\n",
      "Epoch 106/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 53.9714 - val_loss: 64.4188 - lr: 1.0000e-07\n",
      "Epoch 107/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2497 - val_loss: 63.9572 - lr: 1.0000e-07\n",
      "Epoch 108/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9552 - val_loss: 62.6115 - lr: 1.0000e-08\n",
      "Epoch 109/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0683 - val_loss: 62.2882 - lr: 1.0000e-08\n",
      "Epoch 110/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9351 - val_loss: 62.5650 - lr: 1.0000e-08\n",
      "Epoch 111/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1148 - val_loss: 63.9761 - lr: 1.0000e-08\n",
      "Epoch 112/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0044 - val_loss: 63.7332 - lr: 1.0000e-08\n",
      "Epoch 113/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1687 - val_loss: 62.4772 - lr: 1.0000e-08\n",
      "Epoch 114/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1838 - val_loss: 63.8051 - lr: 1.0000e-08\n",
      "Epoch 115/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2721 - val_loss: 63.1267 - lr: 1.0000e-08\n",
      "Epoch 116/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2085 - val_loss: 63.3443 - lr: 1.0000e-08\n",
      "Epoch 117/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1403 - val_loss: 63.5783 - lr: 1.0000e-08\n",
      "Epoch 118/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0066 - val_loss: 63.0153 - lr: 1.0000e-09\n",
      "Epoch 119/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2482 - val_loss: 64.8485 - lr: 1.0000e-09\n",
      "Epoch 120/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1049 - val_loss: 62.7498 - lr: 1.0000e-09\n",
      "Epoch 121/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7982 - val_loss: 62.6713 - lr: 1.0000e-09\n",
      "Epoch 122/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0245 - val_loss: 63.6479 - lr: 1.0000e-09\n",
      "Epoch 123/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9463 - val_loss: 63.9967 - lr: 1.0000e-09\n",
      "Epoch 124/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 54.2399 - val_loss: 62.4506 - lr: 1.0000e-09\n",
      "Epoch 125/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 54.2246 - val_loss: 62.0362 - lr: 1.0000e-09\n",
      "Epoch 126/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2318 - val_loss: 63.7363 - lr: 1.0000e-09\n",
      "Epoch 127/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1672 - val_loss: 63.1587 - lr: 1.0000e-09\n",
      "Epoch 128/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0689 - val_loss: 64.5325 - lr: 1.0000e-10\n",
      "Epoch 129/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3789 - val_loss: 64.6180 - lr: 1.0000e-10\n",
      "Epoch 130/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3006 - val_loss: 62.7561 - lr: 1.0000e-10\n",
      "Epoch 131/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0080 - val_loss: 64.1887 - lr: 1.0000e-10\n",
      "Epoch 132/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2838 - val_loss: 62.0779 - lr: 1.0000e-10\n",
      "Epoch 133/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8641 - val_loss: 62.7340 - lr: 1.0000e-10\n",
      "Epoch 134/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 54.1169 - val_loss: 63.5611 - lr: 1.0000e-10\n",
      "Epoch 135/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0135 - val_loss: 64.3787 - lr: 1.0000e-10\n",
      "Epoch 136/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1648 - val_loss: 64.7310 - lr: 1.0000e-10\n",
      "Epoch 137/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2741 - val_loss: 62.6543 - lr: 1.0000e-10\n",
      "Epoch 138/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8336 - val_loss: 64.3088 - lr: 1.0000e-11\n",
      "Epoch 139/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1745 - val_loss: 66.0390 - lr: 1.0000e-11\n",
      "Epoch 140/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2301 - val_loss: 63.9862 - lr: 1.0000e-11\n",
      "Epoch 141/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9893 - val_loss: 64.6830 - lr: 1.0000e-11\n",
      "Epoch 142/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1583 - val_loss: 64.2460 - lr: 1.0000e-11\n",
      "Epoch 143/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 53.8007 - val_loss: 65.1404 - lr: 1.0000e-11\n",
      "Epoch 144/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3318 - val_loss: 62.5931 - lr: 1.0000e-11\n",
      "Epoch 145/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1652 - val_loss: 63.5115 - lr: 1.0000e-11\n",
      "Epoch 146/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9946 - val_loss: 63.4288 - lr: 1.0000e-11\n",
      "Epoch 147/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 53.7502 - val_loss: 62.1896 - lr: 1.0000e-11\n",
      "Epoch 148/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9565 - val_loss: 63.0445 - lr: 1.0000e-12\n",
      "Epoch 149/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7336 - val_loss: 62.6682 - lr: 1.0000e-12\n",
      "Epoch 150/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2494 - val_loss: 65.5788 - lr: 1.0000e-12\n",
      "Epoch 151/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9026 - val_loss: 63.8169 - lr: 1.0000e-12\n",
      "Epoch 152/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9402 - val_loss: 62.3458 - lr: 1.0000e-12\n",
      "Epoch 153/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7399 - val_loss: 62.1406 - lr: 1.0000e-12\n",
      "Epoch 154/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9871 - val_loss: 63.1839 - lr: 1.0000e-12\n",
      "Epoch 155/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1855 - val_loss: 64.2311 - lr: 1.0000e-12\n",
      "Epoch 156/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9223 - val_loss: 63.6082 - lr: 1.0000e-12\n",
      "Epoch 157/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2222 - val_loss: 64.3441 - lr: 1.0000e-12\n",
      "Epoch 158/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.6581 - val_loss: 64.4983 - lr: 1.0000e-13\n",
      "Epoch 159/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.4591 - val_loss: 62.4546 - lr: 1.0000e-13\n",
      "Epoch 160/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0571 - val_loss: 64.1740 - lr: 1.0000e-13\n",
      "Epoch 161/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8571 - val_loss: 63.2567 - lr: 1.0000e-13\n",
      "Epoch 162/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 54.0465 - val_loss: 63.4539 - lr: 1.0000e-13\n",
      "Epoch 163/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8364 - val_loss: 63.4051 - lr: 1.0000e-13\n",
      "Epoch 164/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1587 - val_loss: 63.5097 - lr: 1.0000e-13\n",
      "Epoch 165/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0040 - val_loss: 64.4713 - lr: 1.0000e-13\n",
      "Epoch 166/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0493 - val_loss: 62.4396 - lr: 1.0000e-13\n",
      "Epoch 167/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3603 - val_loss: 63.1029 - lr: 1.0000e-13\n",
      "Epoch 168/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7505 - val_loss: 63.9811 - lr: 1.0000e-14\n",
      "Epoch 169/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1050 - val_loss: 63.1939 - lr: 1.0000e-14\n",
      "Epoch 170/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0041 - val_loss: 64.9888 - lr: 1.0000e-14\n",
      "Epoch 171/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8059 - val_loss: 62.5531 - lr: 1.0000e-14\n",
      "Epoch 172/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9860 - val_loss: 62.1655 - lr: 1.0000e-14\n",
      "Epoch 173/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2405 - val_loss: 63.2676 - lr: 1.0000e-14\n",
      "Epoch 174/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.6808 - val_loss: 62.6052 - lr: 1.0000e-14\n",
      "Epoch 175/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9478 - val_loss: 63.2480 - lr: 1.0000e-14\n",
      "Epoch 176/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0331 - val_loss: 63.7185 - lr: 1.0000e-14\n",
      "Epoch 177/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 54.1694 - val_loss: 63.4946 - lr: 1.0000e-14\n",
      "Epoch 178/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1502 - val_loss: 64.7341 - lr: 1.0000e-15\n",
      "Epoch 179/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.6011 - val_loss: 62.4999 - lr: 1.0000e-15\n",
      "Epoch 180/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9297 - val_loss: 62.8924 - lr: 1.0000e-15\n",
      "Epoch 181/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7575 - val_loss: 63.5946 - lr: 1.0000e-15\n",
      "Epoch 182/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 53.6436 - val_loss: 63.5581 - lr: 1.0000e-15\n",
      "Epoch 183/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3291 - val_loss: 62.9758 - lr: 1.0000e-15\n",
      "Epoch 184/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8644 - val_loss: 62.5485 - lr: 1.0000e-15\n",
      "Epoch 185/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 53.7668 - val_loss: 63.6226 - lr: 1.0000e-15\n",
      "Epoch 186/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 53.5909 - val_loss: 64.0060 - lr: 1.0000e-15\n",
      "Epoch 187/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7398 - val_loss: 61.9591 - lr: 1.0000e-15\n",
      "Epoch 188/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.4037 - val_loss: 64.1546 - lr: 1.0000e-16\n",
      "Epoch 189/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1814 - val_loss: 63.3073 - lr: 1.0000e-16\n",
      "Epoch 190/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0743 - val_loss: 62.8773 - lr: 1.0000e-16\n",
      "Epoch 191/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 53.8258 - val_loss: 63.5690 - lr: 1.0000e-16\n",
      "Epoch 192/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7595 - val_loss: 61.5054 - lr: 1.0000e-16\n",
      "Epoch 193/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 54.1223 - val_loss: 64.4231 - lr: 1.0000e-16\n",
      "Epoch 194/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3002 - val_loss: 63.4629 - lr: 1.0000e-16\n",
      "Epoch 195/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3015 - val_loss: 62.1972 - lr: 1.0000e-16\n",
      "Epoch 196/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1923 - val_loss: 63.5667 - lr: 1.0000e-16\n",
      "Epoch 197/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1335 - val_loss: 63.6552 - lr: 1.0000e-16\n",
      "Epoch 198/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 53.6292 - val_loss: 64.6310 - lr: 1.0000e-17\n",
      "Epoch 199/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7422 - val_loss: 65.4504 - lr: 1.0000e-17\n",
      "Epoch 200/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 54.1486 - val_loss: 62.4717 - lr: 1.0000e-17\n",
      "Epoch 201/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8337 - val_loss: 63.2014 - lr: 1.0000e-17\n",
      "Epoch 202/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0415 - val_loss: 64.1435 - lr: 1.0000e-17\n",
      "Epoch 203/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7639 - val_loss: 64.0854 - lr: 1.0000e-17\n",
      "Epoch 204/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9314 - val_loss: 64.1391 - lr: 1.0000e-17\n",
      "Epoch 205/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8054 - val_loss: 63.0338 - lr: 1.0000e-17\n",
      "Epoch 206/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2418 - val_loss: 64.0477 - lr: 1.0000e-17\n",
      "Epoch 207/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 54.2779 - val_loss: 63.9150 - lr: 1.0000e-17\n",
      "Epoch 208/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2042 - val_loss: 63.4330 - lr: 1.0000e-18\n",
      "Epoch 209/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9976 - val_loss: 63.2412 - lr: 1.0000e-18\n",
      "Epoch 210/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1388 - val_loss: 63.7635 - lr: 1.0000e-18\n",
      "Epoch 211/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.9837 - val_loss: 64.2466 - lr: 1.0000e-18\n",
      "Epoch 212/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8220 - val_loss: 62.7573 - lr: 1.0000e-18\n",
      "Epoch 213/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7856 - val_loss: 62.6524 - lr: 1.0000e-18\n",
      "Epoch 214/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.3316 - val_loss: 64.8003 - lr: 1.0000e-18\n",
      "Epoch 215/300\n",
      "1688/1688 [==============================] - 71s 42ms/step - loss: 54.2306 - val_loss: 65.3504 - lr: 1.0000e-18\n",
      "Epoch 216/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.8437 - val_loss: 63.7610 - lr: 1.0000e-18\n",
      "Epoch 217/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7534 - val_loss: 63.9216 - lr: 1.0000e-18\n",
      "Epoch 218/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 53.7586 - val_loss: 64.5549 - lr: 1.0000e-19\n",
      "Epoch 219/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.0628 - val_loss: 63.4871 - lr: 1.0000e-19\n",
      "Epoch 220/300\n",
      "1688/1688 [==============================] - 72s 43ms/step - loss: 53.7520 - val_loss: 62.0872 - lr: 1.0000e-19\n",
      "Epoch 221/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7734 - val_loss: 62.0141 - lr: 1.0000e-19\n",
      "Epoch 222/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7383 - val_loss: 63.8416 - lr: 1.0000e-19\n",
      "Epoch 223/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.7184 - val_loss: 65.3304 - lr: 1.0000e-19\n",
      "Epoch 224/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 53.5776 - val_loss: 64.5341 - lr: 1.0000e-19\n",
      "Epoch 225/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.1113 - val_loss: 62.8574 - lr: 1.0000e-19\n",
      "Epoch 226/300\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 54.2222 - val_loss: 63.6572 - lr: 1.0000e-19\n",
      "Epoch 227/300\n",
      " 829/1688 [=============>................] - ETA: 28s - loss: 54.4892"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_stamp = time.time()\n",
    "time_local = time.localtime(time_stamp)\n",
    "dt = time.strftime(\"%Y-%m-%d\", time_local)\n",
    "print(dt)\n",
    "\n",
    "K = 2\n",
    "seed = 2020\n",
    "\n",
    "ass_prediction = np.zeros((len(y_test),1))\n",
    "pin_prediction = np.zeros((len(y_test),1))\n",
    "ass_val_prediction = np.zeros((len(y_pre_train),1))\n",
    "pin_val_prediction = np.zeros((len(y_pre_train),1))\n",
    "\n",
    "skf = KFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X_pre_train,y_pre_train)):\n",
    "        print(\"fold {}\".format(i+1))\n",
    "        X_tr, X_val = X_pre_train[train_index], X_pre_train[val_index]\n",
    "        y_tr, y_val = y_pre_train[train_index], y_pre_train[val_index]\n",
    "        \n",
    "        history1 = m.fit(X_tr,  y_tr[:,0], epochs=300, batch_size=32, callbacks=[learning_rate, early_stopping] ,\n",
    "                     validation_data = (X_val, y_val[:,0]))\n",
    "        \n",
    "        m.summary()\n",
    "        \n",
    "        plot_learning_curves(history1)\n",
    "        \n",
    "        ass_prediction += m.predict(X_test) / skf.n_splits\n",
    "        ass_val_prediction += m.predict(X_pre_train) / skf.n_splits\n",
    "\n",
    "        model_name = 'NuclearFuelBurnupCal_CNN_' + 'ASS' + '_'+ str(i+1)+ '_fold_'+dt + '.h5'\n",
    "        print('正在被保存的模型是 ', model_name)\n",
    "        m.save(model_name)\n",
    "        print('模型已经被保存。')\n",
    "        \n",
    "        \n",
    "        history2 = m2.fit(X_tr,  y_tr[:,1], epochs=300, batch_size=32, callbacks=[learning_rate, early_stopping] ,\n",
    "                     validation_data = (X_val, y_val[:,1]))\n",
    "        \n",
    "        m2.summary()\n",
    "        \n",
    "        plot_learning_curves(history2)\n",
    "        \n",
    "        pin_prediction += m2.predict(X_test) / skf.n_splits\n",
    "        pin_val_prediction += m2.predict(X_pre_train) / skf.n_splits\n",
    "\n",
    "        model_name2 = 'NuclearFuelBurnupCal_CNN_' + 'Pin' + '_'+ str(i+1)+ '_fold_'+dt + '.h5'\n",
    "        print('正在被保存的模型是 ', model_name2)\n",
    "        m2.save(model_name2)\n",
    "        print('模型已经被保存。')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre_train1, X_test1, y_pre_train1, y_test1 = train_test_split(pre_data_x, pre_data_y, \n",
    "                                                           random_state=seed, train_size=0.9, \n",
    "                                                           test_size=0.1)\n",
    "y_test_pred = y_test1.copy()\n",
    "y_test_pred['MaxAssBurnupCal'] = ass_prediction\n",
    "y_test_pred['MaxPinBurnupCal'] = pin_prediction\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN_pred_error = CalError(y_test_pred, y_test1)\n",
    "CNN_pred_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN_pred_error.to_csv('nuclear_cnn_pred_error_test03_20210115.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ass_pred_outlier_list = []\n",
    "pin_pred_outlier_list = []\n",
    "threshold_value = 1000\n",
    "\n",
    "for index,row in CNN_pred_error.iterrows():\n",
    "    if abs(CNN_pred_error.loc[index, 'MaxAssBurnupCal_error']) >= threshold_value:\n",
    "        ass_pred_outlier_list.append(index)\n",
    "        print('MaxAss误差大于{}的行号是{}。'.format(threshold_value, index))\n",
    "        \n",
    "    elif abs(CNN_pred_error.loc[index, 'MaxPinBurnupCal_error']) >= threshold_value:\n",
    "        pin_pred_outlier_list.append(index)\n",
    "        print('MaxPin误差大于{}的行号是{}。'.format(threshold_value, index))\n",
    "            \n",
    "ass_pred_outlier = pre_data.loc[ass_pred_outlier_list, :]\n",
    "pin_pred_outlier = pre_data.loc[pin_pred_outlier_list, :]\n",
    "ass_pred_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pin_pred_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pin_pred_outlier_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
