{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexhang/project/Nuclear'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kinf_1</th>\n",
       "      <th>kinf_2</th>\n",
       "      <th>kinf_3</th>\n",
       "      <th>kinf_4</th>\n",
       "      <th>kinf_5</th>\n",
       "      <th>kinf_6</th>\n",
       "      <th>kinf_7</th>\n",
       "      <th>kinf_8</th>\n",
       "      <th>kinf_9</th>\n",
       "      <th>kinf_10</th>\n",
       "      <th>...</th>\n",
       "      <th>NODE2DBU_95</th>\n",
       "      <th>NODE2DBU_96</th>\n",
       "      <th>NODE2DBU_97</th>\n",
       "      <th>NODE2DBU_98</th>\n",
       "      <th>NODE2DBU_99</th>\n",
       "      <th>NODE2DBU_100</th>\n",
       "      <th>NODE2DBU_101</th>\n",
       "      <th>NODE2DBU_102</th>\n",
       "      <th>NODE2DBU_103</th>\n",
       "      <th>NODE2DBU_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.16662</td>\n",
       "      <td>1.07459</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15078</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "      <td>20806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.16708</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.18090</td>\n",
       "      <td>1.16216</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.08632</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26837.0</td>\n",
       "      <td>26864.0</td>\n",
       "      <td>26834.0</td>\n",
       "      <td>26862.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "      <td>22008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1.15988</td>\n",
       "      <td>1.07458</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.15534</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26191.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "      <td>36146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>1.09092</td>\n",
       "      <td>1.08161</td>\n",
       "      <td>1.07427</td>\n",
       "      <td>1.07583</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.09178</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43091.0</td>\n",
       "      <td>39658.0</td>\n",
       "      <td>45258.0</td>\n",
       "      <td>43151.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>1.19730</td>\n",
       "      <td>1.15417</td>\n",
       "      <td>1.21509</td>\n",
       "      <td>1.20422</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.06296</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31803.0</td>\n",
       "      <td>31836.0</td>\n",
       "      <td>36445.0</td>\n",
       "      <td>36809.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "      <td>25238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1.06622</td>\n",
       "      <td>1.16655</td>\n",
       "      <td>1.20028</td>\n",
       "      <td>1.19485</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.15935</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>43835.0</td>\n",
       "      <td>39276.0</td>\n",
       "      <td>42082.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>21568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         kinf_1   kinf_2   kinf_3   kinf_4  kinf_5  kinf_6   kinf_7  kinf_8  \\\n",
       "0       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "1       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "2       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "3       1.21509  1.16662  1.07459  1.21975  1.4279  1.1821  1.15078  1.4279   \n",
       "4       1.16708  1.08099  1.18090  1.16216  1.4279  1.1821  1.08632  1.4279   \n",
       "...         ...      ...      ...      ...     ...     ...      ...     ...   \n",
       "119995  1.15988  1.07458  1.09861  1.15534  1.4279  1.1821  1.07427  1.4279   \n",
       "119996  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119997  1.09092  1.08161  1.07427  1.07583  1.4279  1.1821  1.09178  1.4279   \n",
       "119998  1.19730  1.15417  1.21509  1.20422  1.4279  1.1821  1.06296  1.4279   \n",
       "119999  1.06622  1.16655  1.20028  1.19485  1.4279  1.1821  1.15935  1.4279   \n",
       "\n",
       "        kinf_9  kinf_10  ...  NODE2DBU_95  NODE2DBU_96  NODE2DBU_97  \\\n",
       "0       1.1821   1.1821  ...          0.0          0.0      36445.0   \n",
       "1       1.1821   1.1821  ...          0.0          0.0      36809.0   \n",
       "2       1.1821   1.1821  ...          0.0          0.0      31836.0   \n",
       "3       1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "4       1.1821   1.1821  ...          0.0          0.0      26837.0   \n",
       "...        ...      ...  ...          ...          ...          ...   \n",
       "119995  1.1821   1.1821  ...          0.0          0.0      26191.0   \n",
       "119996  1.1821   1.1821  ...          0.0          0.0      43151.0   \n",
       "119997  1.1821   1.1821  ...          0.0          0.0      43091.0   \n",
       "119998  1.1821   1.1821  ...          0.0          0.0      31803.0   \n",
       "119999  1.1821   1.1821  ...          0.0          0.0      42090.0   \n",
       "\n",
       "        NODE2DBU_98  NODE2DBU_99  NODE2DBU_100  NODE2DBU_101  NODE2DBU_102  \\\n",
       "0           31803.0      36809.0       31836.0       20806.0       20806.0   \n",
       "1           36445.0      31836.0       31803.0       20806.0       20806.0   \n",
       "2           36809.0      31803.0       36445.0       20806.0       20806.0   \n",
       "3           31836.0      36445.0       36809.0       20806.0       20806.0   \n",
       "4           26864.0      26834.0       26862.0       22008.0       22008.0   \n",
       "...             ...          ...           ...           ...           ...   \n",
       "119995      26191.0      26200.0       26200.0       36146.0       36146.0   \n",
       "119996      45258.0      39658.0       43091.0       36186.0       36186.0   \n",
       "119997      39658.0      45258.0       43151.0       36186.0       36186.0   \n",
       "119998      31836.0      36445.0       36809.0       25238.0       25238.0   \n",
       "119999      43835.0      39276.0       42082.0       21568.0       21568.0   \n",
       "\n",
       "        NODE2DBU_103  NODE2DBU_104  \n",
       "0            20806.0       20806.0  \n",
       "1            20806.0       20806.0  \n",
       "2            20806.0       20806.0  \n",
       "3            20806.0       20806.0  \n",
       "4            22008.0       22008.0  \n",
       "...              ...           ...  \n",
       "119995       36146.0       36146.0  \n",
       "119996       36186.0       36186.0  \n",
       "119997       36186.0       36186.0  \n",
       "119998       25238.0       25238.0  \n",
       "119999       21568.0       21568.0  \n",
       "\n",
       "[120000 rows x 156 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入处理好的数据\n",
    "file_base_name = 'nuclear_burnup_data_20201215.csv'\n",
    "file_input_name = 'df.csv'\n",
    "\n",
    "pre_data_X = pd.read_csv(os.path.join(path, file_input_name), index_col=0)\n",
    "pre_data_base = pd.read_csv(os.path.join(path, file_base_name))\n",
    "pre_data_X.columns.values.tolist()  \n",
    "pre_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAssBurnupCal</th>\n",
       "      <th>MaxPinBurnupCal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56624</td>\n",
       "      <td>62467.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56812</td>\n",
       "      <td>61980.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56724</td>\n",
       "      <td>62521.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56537</td>\n",
       "      <td>62195.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57424</td>\n",
       "      <td>64025.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>57359</td>\n",
       "      <td>63599.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>59562</td>\n",
       "      <td>64865.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>59631</td>\n",
       "      <td>63716.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>61714</td>\n",
       "      <td>65601.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>61843</td>\n",
       "      <td>66948.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MaxAssBurnupCal  MaxPinBurnupCal\n",
       "0                 56624          62467.5\n",
       "1                 56812          61980.1\n",
       "2                 56724          62521.4\n",
       "3                 56537          62195.7\n",
       "4                 57424          64025.7\n",
       "...                 ...              ...\n",
       "119995            57359          63599.1\n",
       "119996            59562          64865.4\n",
       "119997            59631          63716.2\n",
       "119998            61714          65601.8\n",
       "119999            61843          66948.3\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_y = pre_data_base.iloc[:,-2:]\n",
    "pre_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = pd.concat([pre_data_X, pre_data_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 156)\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pre_data_array = pre_data.values\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "pre_data_Normalized = mm.fit_transform(pre_data_array)\n",
    "pre_data_Normalized = pre_data_Normalized[:,:-2]\n",
    "# pre_data_y = pre_data_Normalized[:,-2:]\n",
    "print(pre_data_Normalized.shape)\n",
    "print(pre_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "map1 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [2,6,11,16,20,23,25,26,25,23,20,16,11,6,2],\n",
    "    [1,5,10,15,19,22,24,25,24,22,19,15,10,5,1],\n",
    "    [0,4,9,14,18,21,22,23,22,21,18,14,9,4,0],\n",
    "    [0,3,8,13,17,18,19,20,19,18,17,13,8,3,0],\n",
    "    [0,0,7,12,13,14,15,16,15,14,13,12,7,0,0],\n",
    "    [0,0,0,7,8,9,10,11,10,9,8,7,0,0,0],\n",
    "    [0,0,0,0,3,4,5,6,5,4,3,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,2,1,0,0,0,0,0,0]\n",
    "    \n",
    "]\n",
    "m = np.array(map1)\n",
    "print(m.shape)\n",
    "map2 = \\\n",
    "[\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['2/0','2/2',\"6/0\",\"6/2\",'11/0','11/2','16/0','16/2',\"20/0\",'20/2','23/0','23/2','25/0','25/2','26/0','26/1',\"25/2\",'25/0','23/2','23/0','20/2','20/0','16/2','16/0','11/2','11/0','6/2','6/0','2/2','2/0'],\n",
    "    ['2/1','2/3',\"6/1\",\"6/3\",'11/1','11/3','16/1','16/3',\"20/1\",'20/3','23/1','23/3','25/1','25/3','26/2','26/3',\"25/3\",'25/1','23/3','23/1','20/3','20/1','16/3','16/1','11/3','11/1','6/3','6/1','2/3','2/1'],\n",
    "    ['1/1','1/3',\"5/1\",\"5/3\",'10/1','10/3','15/1','15/3',\"19/1\",'19/3','22/1','22/3','24/2','24/3','25/2','25/3',\"24/3\",'24/2','22/3','22/1','19/3','19/1','15/3','15/1','10/3','10/1','5/3','5/1','1/3','1/1'],\n",
    "    ['1/0','1/2',\"5/0\",\"5/2\",'10/0','10/2','15/0','15/2',\"19/0\",'19/2','22/0','22/2','24/0','24/1','25/0','25/1',\"24/1\",'24/0','22/2','22/0','19/2','19/0','15/2','15/0','10/2','10/0','5/2','5/0','1/2','1/0'],\n",
    "    [0,0,\"4/1\",\"4/3\",'9/1','9/3','14/1','14/3',\"18/1\",'18/3','21/2','21/3','22/2','22/3','23/2','23/3',\"22/3\",'22/2','21/3','21/2','18/3','18/1','14/3','14/1','9/3','9/1','4/3','4/1',0,0],\n",
    "    [0,0,\"4/0\",\"4/2\",'9/0','9/2','14/0','14/2',\"18/0\",'18/2','21/0','21/1','22/0','22/1','23/0','23/1',\"22/1\",'22/0','21/1','21/0','18/2','18/0','14/2','14/0','9/2','9/0','4/2','4/0',0,0],\n",
    "    [0,0,\"3/1\",\"3/3\",'8/1','8/3','13/1','13/3',\"17/2\",'17/3','18/2','18/3','19/2','19/3','20/2','20/3',\"19/3\",'19/2','18/3','18/2','17/3','17/2','13/3','13/1','8/3','8/1','3/3','3/1',0,0],\n",
    "    [0,0,\"3/0\",\"3/2\",'8/0','8/2','13/0','13/2',\"17/0\",'17/1','18/0','18/1','19/0','19/1','20/0','20/1',\"19/1\",'19/0','18/1','18/0','17/1','17/0','13/2','13/0','8/2','8/0','3/2','3/0',0,0],\n",
    "    [0,0,0,0,'7/1','7/3','12/2','12/3',\"13/2\",'13/3','14/2','14/3','15/2','15/3','16/2','16/3',\"15/3\",'15/2','14/3','14/2','13/3','13/2','12/3','12/2','7/3','7/1',0,0,0,0],\n",
    "    [0,0,0,0,'7/0','7/2','12/0','12/1',\"13/0\",'13/1','14/0','14/1','15/0','15/1','16/0','16/1',\"15/1\",'15/0','14/1','14/0','13/1','13/0','12/1','12/0','7/2','7/0',0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/2','7/3',\"8/2\",'8/3','9/2','9/3','10/2','10/3','11/2','11/3',\"10/3\",'10/2','9/3','9/2','8/3','8/2','7/3','7/2',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,'7/0','7/1',\"8/0\",'8/1','9/0','9/1','10/0','10/1','11/0','11/1',\"10/1\",'10/0','9/1','9/0','8/1','8/0','7/1','7/0',0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/2\",'3/3','4/2','4/3','5/2','5/3','6/2','6/3',\"3/3\",'3/2','4/3','4/2','5/3','5/2',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,\"3/0\",'3/1','4/0','4/1','5/0','5/1','6/0','6/1',\"3/1\",'3/0','4/1','4/0','5/1','5/0',0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/2\",'1/3','2/2','2/3','1/3','1/2',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,\"1/0\",'1/1','2/0','2/1','1/1','1/0',0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "      \n",
    "]\n",
    "m = np.array(map2)\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(a,alist):\n",
    "    res = []\n",
    "    for i in range(len(alist)):\n",
    "        for j in range(len(alist[i])):\n",
    "            if alist[i][j] == a:\n",
    "                res.append((i,j))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #排布input\n",
    "# def search(a,alist):\n",
    "#     res = []\n",
    "#     for i in range(len(alist)):\n",
    "#         for j in range(len(alist[i])):\n",
    "#             if alist[i][j] == a:\n",
    "#                 res.append((i,j))\n",
    "#     return res\n",
    "# new = []\n",
    "# for i,item in enumerate(pre_data_Normalized):\n",
    "#     print(\"正在处理第 \"+str(i)+' 个图像..')\n",
    "#     layer1 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer2 = [[0 for _ in range(15)] for _ in range(15)]\n",
    "#     layer3 = [[0 for _ in range(30)] for _ in range(30)]\n",
    "# #     print(item)\n",
    "#     for j in range(item.size): #len = 156\n",
    "#         if j < 26:\n",
    "#             res = search(j+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer1[h][w] = item[j]\n",
    "#         elif j < 52:\n",
    "#             j_ = j-26\n",
    "#             res = search(j_+1,map1)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer2[h][w] = item[j]\n",
    "            \n",
    "#         else:\n",
    "#             j_ = j-52\n",
    "#             number = str(j_ // 4 + 1)\n",
    "#             corner = str(j_ % 4)\n",
    "#             number_corner = number + '/' + corner\n",
    "#             res = search(number_corner,map2)\n",
    "#             for u in res:\n",
    "#                 h,w = u[0],u[1]\n",
    "#                 layer3[h][w] = item[j]\n",
    "#     layer1_array = np.array(layer1)\n",
    "#     layer2_array = np.array(layer2)\n",
    "#     layer3_array = np.array(layer3)\n",
    "    \n",
    "#     #从1*1扩展成2*2\n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 1) \n",
    "#     layer1_array = np.repeat(layer1_array,2,axis = 0)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 1)\n",
    "#     layer2_array = np.repeat(layer2_array,2,axis = 0)\n",
    "    \n",
    "#     #channel 拼接\n",
    "#     tmp = np.stack((layer1_array,layer2_array,layer3_array),axis = 2)\n",
    "#     new.append(tmp)\n",
    "\n",
    "# pre_data_x = np.array(new)\n",
    "# print(pre_data_x.shape)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"pre_data_x_tmp.npy\",pre_data_x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_x = np.load(\"pre_data_x_tmp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pre_data_x.shape)\n",
    "pre_data_y = np.array(pre_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    " \n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 16)        4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 16)        16400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 32)        16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 30, 30, 32)        65568     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 30, 30, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 30, 30, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 30, 30, 64)        262208    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              57601000  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 58,274,097\n",
      "Trainable params: 58,272,753\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.applications.resnet18 import ResNet18\n",
    "# from keras.applications.resnet18 import preprocess_input as preprocess_input_resnet\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "# import tensorflow.keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "def deeper_conv2D(h,w):\n",
    "    new_model = tf.keras.Sequential()\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, strides=1, padding=\"same\",input_shape=(h, w, 3)))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4,strides=1, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\"))\n",
    "    new_model.add(tf.keras.layers.BatchNormalization())\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=8, padding=\"same\"))\n",
    "#     new_model.add(tf.keras.layers.BatchNormalization())\n",
    "#     new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    \n",
    "#     new_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=8, padding=\"same\", activation=\"relu\"))\n",
    "    # Flatten will take our convolution filters and lay them out end to end so our dense layer can predict based on the outcomes of each\n",
    "    new_model.add(tf.keras.layers.Flatten())\n",
    "#     new_model.add(tf.keras.layers.Dense(1000,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "    new_model.add(tf.keras.layers.Dense(1000))\n",
    "    new_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(100))\n",
    "#     new_model.add(tf.keras.layers.Dense(100,kernel_regularizer=regularizers.l2(0.01),\\\n",
    "#                 activity_regularizer=regularizers.l1(0.01)))\n",
    "#     new_model.add(tf.keras.layers.Dropout(0.02))\n",
    "    new_model.add(tf.keras.layers.Dense(1))\n",
    "    new_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")    \n",
    "    return new_model\n",
    "# m = deeper_conv2D(30,30)\n",
    "m = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m2 = deeper_conv2D(pre_data_x.shape[1],pre_data_x.shape[2])\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "seed = 2020\n",
    "x_pre_train, x_test, y_pre_train, y_test = train_test_split(pre_data_x, pre_data_y, \n",
    "                                                           random_state=seed, train_size=0.9, \n",
    "                                                           test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 17 00:51:14 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 29%   39C    P2    37W / 175W |   1646MiB /  7979MiB |     10%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       918      G   /usr/lib/xorg/Xorg                           201MiB |\r\n",
      "|    0      1426      G   /usr/lib/firefox/firefox                       2MiB |\r\n",
      "|    0      1950      G   /usr/bin/gnome-shell                         161MiB |\r\n",
      "|    0      4809      C   ...exhang/anaconda3/envs/tf_gpu/bin/python  1259MiB |\r\n",
      "|    0     18318      G   /usr/local/sunlogin/bin/sunloginclient         7MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    min_delta=0.0001, \n",
    "    cooldown=0, \n",
    "    min_lr=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 80s 95ms/step - loss: 44184732.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 82s 97ms/step - loss: 726929.8750 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 83s 99ms/step - loss: 576419.4375 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 84s 100ms/step - loss: 565910.5625 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 84s 100ms/step - loss: 524742.1875 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 85s 101ms/step - loss: 426575.5000 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 86s 101ms/step - loss: 383777.0938 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 86s 102ms/step - loss: 2896148.7500 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 1372447.1250 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 646632.8125 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 88s 104ms/step - loss: 466475.3125 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 88s 104ms/step - loss: 502916.6562 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 88s 105ms/step - loss: 375187.6875 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 439892.9688 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 311122.2812 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 325222.6250 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 315395.5312 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 333967.5000 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 320129.0000 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 317468.5312 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 255925.2031 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 278918.8750 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 229982.2031 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 231670.2500 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 240716.5156 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 284890.8750 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 212679.4219 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 221190.1406 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 213327.7344 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 186878.8750 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 167598.1719 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 194342.8594 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 165749.7812 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 164618.8125 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 198792.5156 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 138730.1406 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 188350.6562 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 164932.5312 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 146745.0625 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 149555.5312 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 149088.5469 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 135806.2031 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 146918.7500 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 120296.8359 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 140440.2031 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 122849.6719 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 121425.5781 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 112559.0078 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 143923.2031 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 114462.4141 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 102377.4688 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 89s 105ms/step - loss: 129821.4688 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 119611.0391 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 114397.3750 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 102305.1953 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 107069.3984 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 99364.6250 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 104061.1953 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 105067.7578 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 103222.1875 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 87649.7266 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 83132.0156 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 89665.0391 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 91153.9688 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 102332.7266 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 91894.9844 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 85702.0938 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 102994.6484 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 77214.4141 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 88027.2188 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 95070.9688 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 88056.6719 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 84165.6484 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 86554.7969 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 85965.9688 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 86438.5781 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 78497.8281 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 77989.3594 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "844/844 [==============================] - 87s 103ms/step - loss: 69194.3203 - lr: 0.0010\n",
      "Epoch 80/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 81s 96ms/step - loss: 94373.8672 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 85786.6797 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 65458.0664 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 86927.9375 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 66571.9453 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 69372.2969 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 72947.6016 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 68975.7422 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 60610.9922 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 74470.0938 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 71405.8281 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 76651.6094 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 60565.1172 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 59717.4531 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 74320.1719 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 63292.6719 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 62471.7305 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 63006.8828 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 56930.0820 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 63181.5977 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 78808.6484 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 65346.6172 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 66353.0547 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 62628.1719 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 63447.7930 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 61444.8203 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 64207.0859 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 66036.4688 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 52850.6641 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 67095.7891 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 70194.9453 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 57514.3789 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 58702.0625 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 61399.4766 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 59095.5859 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 65261.3945 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 68248.6094 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 65833.5078 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 52807.1875 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 53555.0352 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 59332.4219 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 65557.8047 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 55721.3008 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 67831.4219 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 51941.8984 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 48731.6484 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 54738.9141 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 59101.7227 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 69211.9297 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 54449.0430 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 49002.4570 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 50191.9141 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 56493.3164 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 44949.4297 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 50095.1406 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 43150.5547 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 55267.2539 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 50600.7891 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 47223.4180 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 58870.2109 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 44663.3984 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 51148.0664 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 40529.5156 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 59627.0117 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 52427.7031 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 41399.3594 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 51664.4922 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 46765.5859 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 50159.7969 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 47412.7461 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 47845.8203 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 47648.3047 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 38404.3867 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 40919.9844 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 48157.7891 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 42692.9297 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 46961.3242 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 46042.1016 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 48671.6445 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 52251.4805 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 41337.7578 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 41991.1719 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 60224.5078 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 13337.7109 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11921.9648 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12249.5654 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12370.3975 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 13862.9121 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12263.4502 - lr: 1.0000e-04\n",
      "Epoch 169/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12669.8164 - lr: 1.0000e-04\n",
      "Epoch 170/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12239.0615 - lr: 1.0000e-04\n",
      "Epoch 171/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12047.1621 - lr: 1.0000e-04\n",
      "Epoch 172/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12703.8096 - lr: 1.0000e-04\n",
      "Epoch 173/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11936.9424 - lr: 1.0000e-04\n",
      "Epoch 174/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11212.4600 - lr: 1.0000e-04\n",
      "Epoch 175/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11236.7744 - lr: 1.0000e-04\n",
      "Epoch 176/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12463.8779 - lr: 1.0000e-04\n",
      "Epoch 177/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11635.8018 - lr: 1.0000e-04\n",
      "Epoch 178/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11232.7949 - lr: 1.0000e-04\n",
      "Epoch 179/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10890.3545 - lr: 1.0000e-04\n",
      "Epoch 180/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10289.8477 - lr: 1.0000e-04\n",
      "Epoch 181/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11484.5371 - lr: 1.0000e-04\n",
      "Epoch 182/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10170.4785 - lr: 1.0000e-04\n",
      "Epoch 183/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11643.3223 - lr: 1.0000e-04\n",
      "Epoch 184/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10421.4990 - lr: 1.0000e-04\n",
      "Epoch 185/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10469.2100 - lr: 1.0000e-04\n",
      "Epoch 186/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11019.1191 - lr: 1.0000e-04\n",
      "Epoch 187/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10409.6816 - lr: 1.0000e-04\n",
      "Epoch 188/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10047.0439 - lr: 1.0000e-04\n",
      "Epoch 189/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10633.4834 - lr: 1.0000e-04\n",
      "Epoch 190/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9878.7539 - lr: 1.0000e-04\n",
      "Epoch 191/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10104.0049 - lr: 1.0000e-04\n",
      "Epoch 192/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10429.6523 - lr: 1.0000e-04\n",
      "Epoch 193/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10575.4326 - lr: 1.0000e-04\n",
      "Epoch 194/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11219.2021 - lr: 1.0000e-04\n",
      "Epoch 195/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9935.9844 - lr: 1.0000e-04\n",
      "Epoch 196/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10310.6250 - lr: 1.0000e-04\n",
      "Epoch 197/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10159.3408 - lr: 1.0000e-04\n",
      "Epoch 198/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10107.5898 - lr: 1.0000e-04\n",
      "Epoch 199/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10845.0771 - lr: 1.0000e-04\n",
      "Epoch 200/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9902.9912 - lr: 1.0000e-04\n",
      "Epoch 201/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7410.9434 - lr: 1.0000e-05\n",
      "Epoch 202/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7380.5996 - lr: 1.0000e-05\n",
      "Epoch 203/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7456.5718 - lr: 1.0000e-05\n",
      "Epoch 204/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7529.2988 - lr: 1.0000e-05\n",
      "Epoch 205/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7482.2861 - lr: 1.0000e-05\n",
      "Epoch 206/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7312.1245 - lr: 1.0000e-05\n",
      "Epoch 207/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7371.9722 - lr: 1.0000e-05\n",
      "Epoch 208/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7320.4634 - lr: 1.0000e-05\n",
      "Epoch 209/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7302.4263 - lr: 1.0000e-05\n",
      "Epoch 210/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7245.7539 - lr: 1.0000e-05\n",
      "Epoch 211/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7236.7017 - lr: 1.0000e-05\n",
      "Epoch 212/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7343.3433 - lr: 1.0000e-05\n",
      "Epoch 213/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7311.7588 - lr: 1.0000e-05\n",
      "Epoch 214/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7255.6172 - lr: 1.0000e-05\n",
      "Epoch 215/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7641.4536 - lr: 1.0000e-05\n",
      "Epoch 216/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7046.7896 - lr: 1.0000e-05\n",
      "Epoch 217/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7267.3198 - lr: 1.0000e-05\n",
      "Epoch 218/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7222.2803 - lr: 1.0000e-05\n",
      "Epoch 219/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7223.7280 - lr: 1.0000e-05\n",
      "Epoch 220/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7340.3110 - lr: 1.0000e-05\n",
      "Epoch 221/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7604.7690 - lr: 1.0000e-05\n",
      "Epoch 222/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7304.7075 - lr: 1.0000e-05\n",
      "Epoch 223/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7316.5068 - lr: 1.0000e-05\n",
      "Epoch 224/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7265.3872 - lr: 1.0000e-05\n",
      "Epoch 225/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7306.4907 - lr: 1.0000e-05\n",
      "Epoch 226/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7069.6763 - lr: 1.0000e-05\n",
      "Epoch 227/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6930.6167 - lr: 1.0000e-06\n",
      "Epoch 228/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6762.2993 - lr: 1.0000e-06\n",
      "Epoch 229/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6825.2363 - lr: 1.0000e-06\n",
      "Epoch 230/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6990.0005 - lr: 1.0000e-06\n",
      "Epoch 231/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6801.7744 - lr: 1.0000e-06\n",
      "Epoch 232/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6917.3374 - lr: 1.0000e-06\n",
      "Epoch 233/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6830.6792 - lr: 1.0000e-06\n",
      "Epoch 234/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6809.4282 - lr: 1.0000e-06\n",
      "Epoch 235/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6924.2666 - lr: 1.0000e-06\n",
      "Epoch 236/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6773.8408 - lr: 1.0000e-06\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 81s 96ms/step - loss: 6830.5439 - lr: 1.0000e-06\n",
      "Epoch 238/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6874.7153 - lr: 1.0000e-06\n",
      "Epoch 239/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6870.3179 - lr: 1.0000e-07\n",
      "Epoch 240/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6868.6431 - lr: 1.0000e-07\n",
      "Epoch 241/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6749.4072 - lr: 1.0000e-07\n",
      "Epoch 242/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6786.4644 - lr: 1.0000e-07\n",
      "Epoch 243/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6823.9800 - lr: 1.0000e-07\n",
      "Epoch 244/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6892.1924 - lr: 1.0000e-07\n",
      "Epoch 245/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6795.3276 - lr: 1.0000e-07\n",
      "Epoch 246/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6774.0669 - lr: 1.0000e-07\n",
      "Epoch 247/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6914.9746 - lr: 1.0000e-07\n",
      "Epoch 248/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6873.2959 - lr: 1.0000e-07\n",
      "Epoch 249/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6850.8247 - lr: 1.0000e-07\n",
      "Epoch 250/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6950.4155 - lr: 1.0000e-07\n",
      "Epoch 251/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6838.2852 - lr: 1.0000e-07\n",
      "Epoch 252/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6812.5854 - lr: 1.0000e-08\n",
      "Epoch 253/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6853.0703 - lr: 1.0000e-08\n",
      "Epoch 254/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6780.7988 - lr: 1.0000e-08\n",
      "Epoch 255/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6789.7910 - lr: 1.0000e-08\n",
      "Epoch 256/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6751.3970 - lr: 1.0000e-08\n",
      "Epoch 257/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6908.5391 - lr: 1.0000e-08\n",
      "Epoch 258/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7068.5879 - lr: 1.0000e-08\n",
      "Epoch 259/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6745.3369 - lr: 1.0000e-08\n",
      "Epoch 260/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6713.3359 - lr: 1.0000e-08\n",
      "Epoch 261/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6954.8804 - lr: 1.0000e-08\n",
      "Epoch 262/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6852.7407 - lr: 1.0000e-08\n",
      "Epoch 263/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6899.4380 - lr: 1.0000e-08\n",
      "Epoch 264/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6830.4482 - lr: 1.0000e-08\n",
      "Epoch 265/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6927.8228 - lr: 1.0000e-08\n",
      "Epoch 266/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6882.4116 - lr: 1.0000e-08\n",
      "Epoch 267/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6822.5151 - lr: 1.0000e-08\n",
      "Epoch 268/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6805.8691 - lr: 1.0000e-08\n",
      "Epoch 269/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6898.8013 - lr: 1.0000e-08\n",
      "Epoch 270/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6739.7524 - lr: 1.0000e-08\n",
      "Epoch 271/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6792.7876 - lr: 1.0000e-09\n",
      "Epoch 272/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6941.3340 - lr: 1.0000e-09\n",
      "Epoch 273/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6700.0278 - lr: 1.0000e-09\n",
      "Epoch 274/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6797.2686 - lr: 1.0000e-09\n",
      "Epoch 275/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6850.1416 - lr: 1.0000e-09\n",
      "Epoch 276/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6857.3369 - lr: 1.0000e-09\n",
      "Epoch 277/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6799.3848 - lr: 1.0000e-09\n",
      "Epoch 278/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6782.5430 - lr: 1.0000e-09\n",
      "Epoch 279/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6810.6582 - lr: 1.0000e-09\n",
      "Epoch 280/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6888.4497 - lr: 1.0000e-09\n",
      "Epoch 281/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6818.5581 - lr: 1.0000e-09\n",
      "Epoch 282/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6768.7964 - lr: 1.0000e-09\n",
      "Epoch 283/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6813.5063 - lr: 1.0000e-09\n",
      "Epoch 284/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6912.0063 - lr: 1.0000e-10\n",
      "Epoch 285/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6727.5254 - lr: 1.0000e-10\n",
      "Epoch 286/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6867.3184 - lr: 1.0000e-10\n",
      "Epoch 287/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7018.1748 - lr: 1.0000e-10\n",
      "Epoch 288/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6780.2979 - lr: 1.0000e-10\n",
      "Epoch 289/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6828.6069 - lr: 1.0000e-10\n",
      "Epoch 290/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6832.4365 - lr: 1.0000e-10\n",
      "Epoch 291/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6837.8721 - lr: 1.0000e-10\n",
      "Epoch 292/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6833.1230 - lr: 1.0000e-10\n",
      "Epoch 293/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6642.0215 - lr: 1.0000e-10\n",
      "Epoch 294/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6870.4683 - lr: 1.0000e-10\n",
      "Epoch 295/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6880.8647 - lr: 1.0000e-10\n",
      "Epoch 296/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6773.3408 - lr: 1.0000e-10\n",
      "Epoch 297/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6799.4785 - lr: 1.0000e-10\n",
      "Epoch 298/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6763.7529 - lr: 1.0000e-10\n",
      "Epoch 299/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6889.1436 - lr: 1.0000e-10\n",
      "Epoch 300/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6892.6611 - lr: 1.0000e-10\n"
     ]
    }
   ],
   "source": [
    "model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m2.save('test_07_model_2.h5')\n",
    "\n",
    "\n",
    "#batch size: 调小\n",
    "#删除所有正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = m2.fit(x_pre_train,  y_pre_train[:,1], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "# m2.save('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m2.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_2 = pd.DataFrame(y_test[:,1],columns=output_col)\n",
    "# y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPinBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      63585.046875  63664.2\n",
      "1      52950.996094  53156.4\n",
      "2      60884.644531  60857.7\n",
      "3      63831.679688  63804.1\n",
      "4      66071.554688  66056.1\n",
      "...             ...      ...\n",
      "11995  63237.707031  63212.1\n",
      "11996  63965.433594  63966.7\n",
      "11997  63338.414062  63355.0\n",
      "11998  62887.312500  63106.4\n",
      "11999  62754.527344  62843.1\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(0.0, 100.0]         9340\n",
      "(100.0, 200.0]       1956\n",
      "(200.0, 300.0]        482\n",
      "(300.0, 400.0]        129\n",
      "(400.0, 500.0]         44\n",
      "(500.0, 600.0]         24\n",
      "(600.0, 700.0]         12\n",
      "(700.0, 800.0]          4\n",
      "(800.0, 900.0]          4\n",
      "(900.0, 1000.0]         4\n",
      "(1000.0, 2000.0]        1\n",
      "(2000.0, 3000.0]        0\n",
      "(3000.0, 4000.0]        0\n",
      "(4000.0, 5000.0]        0\n",
      "(5000.0, 10000.0]       0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63585.046875</td>\n",
       "      <td>63664.2</td>\n",
       "      <td>-79.153125</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52950.996094</td>\n",
       "      <td>53156.4</td>\n",
       "      <td>-205.403906</td>\n",
       "      <td>(200.0, 300.0]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60884.644531</td>\n",
       "      <td>60857.7</td>\n",
       "      <td>26.944531</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63831.679688</td>\n",
       "      <td>63804.1</td>\n",
       "      <td>27.579688</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66071.554688</td>\n",
       "      <td>66056.1</td>\n",
       "      <td>15.454687</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>63237.707031</td>\n",
       "      <td>63212.1</td>\n",
       "      <td>25.607031</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>63965.433594</td>\n",
       "      <td>63966.7</td>\n",
       "      <td>-1.266406</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>63338.414062</td>\n",
       "      <td>63355.0</td>\n",
       "      <td>-16.585938</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>62887.312500</td>\n",
       "      <td>63106.4</td>\n",
       "      <td>-219.087500</td>\n",
       "      <td>(200.0, 300.0]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>62754.527344</td>\n",
       "      <td>62843.1</td>\n",
       "      <td>-88.572656</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test     v_error   v_error_range v_error_label\n",
       "0      63585.046875  63664.2  -79.153125    (0.0, 100.0]           1.0\n",
       "1      52950.996094  53156.4 -205.403906  (200.0, 300.0]           3.0\n",
       "2      60884.644531  60857.7   26.944531    (0.0, 100.0]           1.0\n",
       "3      63831.679688  63804.1   27.579688    (0.0, 100.0]           1.0\n",
       "4      66071.554688  66056.1   15.454687    (0.0, 100.0]           1.0\n",
       "...             ...      ...         ...             ...           ...\n",
       "11995  63237.707031  63212.1   25.607031    (0.0, 100.0]           1.0\n",
       "11996  63965.433594  63966.7   -1.266406    (0.0, 100.0]           1.0\n",
       "11997  63338.414062  63355.0  -16.585938    (0.0, 100.0]           1.0\n",
       "11998  62887.312500  63106.4 -219.087500  (200.0, 300.0]           3.0\n",
       "11999  62754.527344  62843.1  -88.572656    (0.0, 100.0]           1.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxPinBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_2[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 39360088.0000 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 525548.6250 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 486623.7188 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 488185.3750 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 407323.5312 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 541812.6250 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 429521.8125 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 463006.5312 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 414775.1562 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 308854.0312 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 318474.5312 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 330399.1250 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 299811.6250 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 252578.2969 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 225231.3438 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 212307.2812 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 180981.3125 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 157288.2812 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 197244.9219 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 136622.2344 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 164151.0938 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 151437.6719 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 123362.6953 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 177647.2031 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 103072.4844 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 127453.7344 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 96504.1172 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 121059.8203 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 99484.8828 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 96475.1328 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 103021.5469 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 100120.6719 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 88383.8984 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 116259.9531 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 105632.3438 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 77762.1172 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 67841.6094 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 71391.5703 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 97669.9844 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 73326.1719 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 62992.4922 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 102909.8125 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 62460.3125 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 75084.5391 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 64481.0625 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 94725.1406 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 68230.4531 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 74067.3359 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 68908.7578 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 67389.4531 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 64473.8086 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 63530.3672 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 70072.0000 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 13561.3447 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12982.9385 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12743.9980 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 14823.3613 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12668.8389 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 14188.0459 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 13179.0361 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11550.3037 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 12367.4209 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11703.1523 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11823.9502 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11413.2998 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11232.4395 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11151.4141 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11307.1836 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10710.9863 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 11522.7021 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 10737.6299 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9377.7500 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9117.7041 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9363.4932 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9666.6230 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9634.1934 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8208.7148 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9265.8301 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8632.9590 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8703.2939 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8848.9775 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 9530.1904 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8422.9980 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8242.5068 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8566.9004 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8512.8027 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7579.5249 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8328.1104 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8218.4014 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7481.5552 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7534.5288 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7699.4399 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7543.9810 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8382.0508 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7013.9316 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7976.6914 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7279.9526 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7579.8716 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 8298.3408 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6337.5122 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7079.9268 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6979.3853 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6988.0962 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7112.5513 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6850.6621 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6147.7178 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6680.3682 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6660.7959 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6704.4937 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5974.6211 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6364.7827 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6473.8867 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5826.6030 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5941.1934 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6003.0503 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6791.3506 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5768.9321 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5783.2153 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6154.9717 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6385.4600 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6845.8447 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5410.0259 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6301.8008 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5622.2681 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5732.7095 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5863.6304 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5624.7124 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5101.2539 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5387.3408 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 7359.5308 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5358.9175 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5418.2749 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6486.7793 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5062.7617 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5710.7148 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5358.6797 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6309.2500 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5297.4614 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5929.1597 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5896.7773 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5369.9106 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 6207.3442 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4631.5674 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5690.9238 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5208.2495 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4985.7026 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5227.0713 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5475.6860 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5326.4136 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5271.2129 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4522.1016 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4752.1904 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4842.7637 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5284.2568 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5426.9385 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5479.9741 - lr: 1.0000e-04\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 81s 96ms/step - loss: 4436.8813 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5650.7378 - lr: 1.0000e-04\n",
      "Epoch 159/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4640.7134 - lr: 1.0000e-04\n",
      "Epoch 160/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4602.7095 - lr: 1.0000e-04\n",
      "Epoch 161/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4906.7192 - lr: 1.0000e-04\n",
      "Epoch 162/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4410.4946 - lr: 1.0000e-04\n",
      "Epoch 163/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5045.7891 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5174.7769 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4984.1924 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5115.0107 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4249.4185 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4836.1006 - lr: 1.0000e-04\n",
      "Epoch 169/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5711.5332 - lr: 1.0000e-04\n",
      "Epoch 170/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5368.9243 - lr: 1.0000e-04\n",
      "Epoch 171/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4880.7827 - lr: 1.0000e-04\n",
      "Epoch 172/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4864.3032 - lr: 1.0000e-04\n",
      "Epoch 173/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4327.4722 - lr: 1.0000e-04\n",
      "Epoch 174/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4954.1182 - lr: 1.0000e-04\n",
      "Epoch 175/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5243.1313 - lr: 1.0000e-04\n",
      "Epoch 176/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 4500.5405 - lr: 1.0000e-04\n",
      "Epoch 177/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 5302.4116 - lr: 1.0000e-04\n",
      "Epoch 178/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2096.8013 - lr: 1.0000e-05\n",
      "Epoch 179/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2052.1277 - lr: 1.0000e-05\n",
      "Epoch 180/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2116.0166 - lr: 1.0000e-05\n",
      "Epoch 181/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2021.2241 - lr: 1.0000e-05\n",
      "Epoch 182/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2090.8489 - lr: 1.0000e-05\n",
      "Epoch 183/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2028.7705 - lr: 1.0000e-05\n",
      "Epoch 184/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2062.8777 - lr: 1.0000e-05\n",
      "Epoch 185/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2077.0708 - lr: 1.0000e-05\n",
      "Epoch 186/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2169.0425 - lr: 1.0000e-05\n",
      "Epoch 187/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2063.7620 - lr: 1.0000e-05\n",
      "Epoch 188/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2103.2126 - lr: 1.0000e-05\n",
      "Epoch 189/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2022.3341 - lr: 1.0000e-05\n",
      "Epoch 190/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1912.9917 - lr: 1.0000e-05\n",
      "Epoch 191/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1956.2462 - lr: 1.0000e-05\n",
      "Epoch 192/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1966.7491 - lr: 1.0000e-05\n",
      "Epoch 193/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1977.9554 - lr: 1.0000e-05\n",
      "Epoch 194/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1950.2433 - lr: 1.0000e-05\n",
      "Epoch 195/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1968.6406 - lr: 1.0000e-05\n",
      "Epoch 196/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2027.4626 - lr: 1.0000e-05\n",
      "Epoch 197/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2017.7885 - lr: 1.0000e-05\n",
      "Epoch 198/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1986.6593 - lr: 1.0000e-05\n",
      "Epoch 199/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 2027.4369 - lr: 1.0000e-05\n",
      "Epoch 200/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1984.1715 - lr: 1.0000e-05\n",
      "Epoch 201/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1645.9451 - lr: 1.0000e-06\n",
      "Epoch 202/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1647.7017 - lr: 1.0000e-06\n",
      "Epoch 203/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1686.0696 - lr: 1.0000e-06\n",
      "Epoch 204/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1647.3300 - lr: 1.0000e-06\n",
      "Epoch 205/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1637.6920 - lr: 1.0000e-06\n",
      "Epoch 206/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1639.1587 - lr: 1.0000e-06\n",
      "Epoch 207/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1637.3026 - lr: 1.0000e-06\n",
      "Epoch 208/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1610.8717 - lr: 1.0000e-06\n",
      "Epoch 209/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1651.5645 - lr: 1.0000e-06\n",
      "Epoch 210/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1608.9739 - lr: 1.0000e-06\n",
      "Epoch 211/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1596.9514 - lr: 1.0000e-06\n",
      "Epoch 212/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1611.9297 - lr: 1.0000e-06\n",
      "Epoch 213/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1695.0688 - lr: 1.0000e-06\n",
      "Epoch 214/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1612.1145 - lr: 1.0000e-06\n",
      "Epoch 215/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1610.1267 - lr: 1.0000e-06\n",
      "Epoch 216/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1672.2838 - lr: 1.0000e-06\n",
      "Epoch 217/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1635.7529 - lr: 1.0000e-06\n",
      "Epoch 218/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1640.7321 - lr: 1.0000e-06\n",
      "Epoch 219/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1607.2596 - lr: 1.0000e-06\n",
      "Epoch 220/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1640.4012 - lr: 1.0000e-06\n",
      "Epoch 221/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1657.3422 - lr: 1.0000e-06\n",
      "Epoch 222/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1613.3341 - lr: 1.0000e-07\n",
      "Epoch 223/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1596.9634 - lr: 1.0000e-07\n",
      "Epoch 224/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1626.9822 - lr: 1.0000e-07\n",
      "Epoch 225/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1545.9354 - lr: 1.0000e-07\n",
      "Epoch 226/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1588.5186 - lr: 1.0000e-07\n",
      "Epoch 227/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1554.4694 - lr: 1.0000e-07\n",
      "Epoch 228/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1493.1887 - lr: 1.0000e-07\n",
      "Epoch 229/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1554.9506 - lr: 1.0000e-07\n",
      "Epoch 230/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1618.5602 - lr: 1.0000e-07\n",
      "Epoch 231/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1634.0813 - lr: 1.0000e-07\n",
      "Epoch 232/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1551.3839 - lr: 1.0000e-07\n",
      "Epoch 233/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1580.1288 - lr: 1.0000e-07\n",
      "Epoch 234/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1525.7689 - lr: 1.0000e-07\n",
      "Epoch 235/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1553.8483 - lr: 1.0000e-07\n",
      "Epoch 236/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1589.8348 - lr: 1.0000e-07\n",
      "Epoch 237/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1612.8488 - lr: 1.0000e-07\n",
      "Epoch 238/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1612.9916 - lr: 1.0000e-07\n",
      "Epoch 239/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1608.1920 - lr: 1.0000e-08\n",
      "Epoch 240/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1532.9480 - lr: 1.0000e-08\n",
      "Epoch 241/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1631.1995 - lr: 1.0000e-08\n",
      "Epoch 242/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1589.8727 - lr: 1.0000e-08\n",
      "Epoch 243/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1568.3274 - lr: 1.0000e-08\n",
      "Epoch 244/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1565.2053 - lr: 1.0000e-08\n",
      "Epoch 245/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1559.2767 - lr: 1.0000e-08\n",
      "Epoch 246/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1556.4417 - lr: 1.0000e-08\n",
      "Epoch 247/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1541.0557 - lr: 1.0000e-08\n",
      "Epoch 248/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1543.6041 - lr: 1.0000e-08\n",
      "Epoch 249/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1592.3352 - lr: 1.0000e-09\n",
      "Epoch 250/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1563.3302 - lr: 1.0000e-09\n",
      "Epoch 251/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1598.8427 - lr: 1.0000e-09\n",
      "Epoch 252/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1601.8788 - lr: 1.0000e-09\n",
      "Epoch 253/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1538.6527 - lr: 1.0000e-09\n",
      "Epoch 254/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1545.7515 - lr: 1.0000e-09\n",
      "Epoch 255/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1589.4946 - lr: 1.0000e-09\n",
      "Epoch 256/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1607.4775 - lr: 1.0000e-09\n",
      "Epoch 257/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1588.0513 - lr: 1.0000e-09\n",
      "Epoch 258/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1576.0458 - lr: 1.0000e-09\n",
      "Epoch 259/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1549.9355 - lr: 1.0000e-10\n",
      "Epoch 260/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1564.3616 - lr: 1.0000e-10\n",
      "Epoch 261/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1596.1260 - lr: 1.0000e-10\n",
      "Epoch 262/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1580.7010 - lr: 1.0000e-10\n",
      "Epoch 263/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1563.9293 - lr: 1.0000e-10\n",
      "Epoch 264/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1588.7434 - lr: 1.0000e-10\n",
      "Epoch 265/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1568.0939 - lr: 1.0000e-10\n",
      "Epoch 266/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1575.5602 - lr: 1.0000e-10\n",
      "Epoch 267/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1617.7898 - lr: 1.0000e-10\n",
      "Epoch 268/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1582.3839 - lr: 1.0000e-10\n",
      "Epoch 269/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1564.6060 - lr: 1.0000e-11\n",
      "Epoch 270/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1626.9091 - lr: 1.0000e-11\n",
      "Epoch 271/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1626.3323 - lr: 1.0000e-11\n",
      "Epoch 272/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1541.7998 - lr: 1.0000e-11\n",
      "Epoch 273/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1630.7842 - lr: 1.0000e-11\n",
      "Epoch 274/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1572.2635 - lr: 1.0000e-11\n",
      "Epoch 275/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1594.9374 - lr: 1.0000e-11\n",
      "Epoch 276/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1563.0841 - lr: 1.0000e-11\n",
      "Epoch 277/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1521.6942 - lr: 1.0000e-11\n",
      "Epoch 278/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1581.4183 - lr: 1.0000e-11\n",
      "Epoch 279/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1523.3809 - lr: 1.0000e-12\n",
      "Epoch 280/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1568.0604 - lr: 1.0000e-12\n",
      "Epoch 281/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1580.8362 - lr: 1.0000e-12\n",
      "Epoch 282/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1622.3606 - lr: 1.0000e-12\n",
      "Epoch 283/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1589.1470 - lr: 1.0000e-12\n",
      "Epoch 284/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1561.7213 - lr: 1.0000e-12\n",
      "Epoch 285/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1571.0563 - lr: 1.0000e-12\n",
      "Epoch 286/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1587.4144 - lr: 1.0000e-12\n",
      "Epoch 287/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1564.0923 - lr: 1.0000e-12\n",
      "Epoch 288/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1492.5658 - lr: 1.0000e-12\n",
      "Epoch 289/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1593.3119 - lr: 1.0000e-12\n",
      "Epoch 290/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1634.3461 - lr: 1.0000e-12\n",
      "Epoch 291/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1588.2672 - lr: 1.0000e-12\n",
      "Epoch 292/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1612.8329 - lr: 1.0000e-12\n",
      "Epoch 293/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1622.1808 - lr: 1.0000e-12\n",
      "Epoch 294/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1573.3342 - lr: 1.0000e-12\n",
      "Epoch 295/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1594.4128 - lr: 1.0000e-12\n",
      "Epoch 296/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1570.1364 - lr: 1.0000e-12\n",
      "Epoch 297/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1603.6722 - lr: 1.0000e-12\n",
      "Epoch 298/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1551.6761 - lr: 1.0000e-12\n",
      "Epoch 299/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1532.2300 - lr: 1.0000e-13\n",
      "Epoch 300/300\n",
      "844/844 [==============================] - 81s 96ms/step - loss: 1537.9929 - lr: 1.0000e-13\n"
     ]
    }
   ],
   "source": [
    "model = m.fit(x_pre_train,  y_pre_train[:,0], epochs=300,batch_size=128,callbacks=[learning_rate])\n",
    "m.save('test_07_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = load_model('test_05_model.h5')\n",
    "# m2 = load_model('test_05_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57732. , 63664.2],\n",
       "       [49679. , 53156.4],\n",
       "       [54567. , 60857.7],\n",
       "       ...,\n",
       "       [57982. , 63355. ],\n",
       "       [57080. , 63106.4],\n",
       "       [56720. , 62843.1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_col = ['v']\n",
    "columns=output_col\n",
    "pred_test = m.predict(x_test)\n",
    "pred_test = pd.DataFrame(pred_test,columns=output_col)\n",
    "y_test_1 = pd.DataFrame(y_test[:,0],columns=output_col)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAssBurnupCal\n",
      "\n",
      "                  v        v\n",
      "0      57734.843750  57732.0\n",
      "1      49700.640625  49679.0\n",
      "2      54594.113281  54567.0\n",
      "3      56127.300781  56139.0\n",
      "4      62322.390625  62291.0\n",
      "...             ...      ...\n",
      "11995  59880.917969  59847.0\n",
      "11996  57777.601562  57746.0\n",
      "11997  58016.804688  57982.0\n",
      "11998  57104.230469  57080.0\n",
      "11999  56720.238281  56720.0\n",
      "\n",
      "[12000 rows x 2 columns]\n",
      "v_error_range特征误差范围及统计个数\n",
      "(0.0, 100.0]         11732\n",
      "(100.0, 200.0]         247\n",
      "(200.0, 300.0]          14\n",
      "(300.0, 400.0]           3\n",
      "(500.0, 600.0]           2\n",
      "(400.0, 500.0]           1\n",
      "(2000.0, 3000.0]         1\n",
      "(600.0, 700.0]           0\n",
      "(700.0, 800.0]           0\n",
      "(800.0, 900.0]           0\n",
      "(900.0, 1000.0]          0\n",
      "(1000.0, 2000.0]         0\n",
      "(3000.0, 4000.0]         0\n",
      "(4000.0, 5000.0]         0\n",
      "(5000.0, 10000.0]        0\n",
      "Name: v_error_range, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_pred</th>\n",
       "      <th>v_test</th>\n",
       "      <th>v_error</th>\n",
       "      <th>v_error_range</th>\n",
       "      <th>v_error_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57734.843750</td>\n",
       "      <td>57732.0</td>\n",
       "      <td>2.843750</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49700.640625</td>\n",
       "      <td>49679.0</td>\n",
       "      <td>21.640625</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54594.113281</td>\n",
       "      <td>54567.0</td>\n",
       "      <td>27.113281</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56127.300781</td>\n",
       "      <td>56139.0</td>\n",
       "      <td>-11.699219</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62322.390625</td>\n",
       "      <td>62291.0</td>\n",
       "      <td>31.390625</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>59880.917969</td>\n",
       "      <td>59847.0</td>\n",
       "      <td>33.917969</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>57777.601562</td>\n",
       "      <td>57746.0</td>\n",
       "      <td>31.601562</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>58016.804688</td>\n",
       "      <td>57982.0</td>\n",
       "      <td>34.804688</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>57104.230469</td>\n",
       "      <td>57080.0</td>\n",
       "      <td>24.230469</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>56720.238281</td>\n",
       "      <td>56720.0</td>\n",
       "      <td>0.238281</td>\n",
       "      <td>(0.0, 100.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v_pred   v_test    v_error v_error_range v_error_label\n",
       "0      57734.843750  57732.0   2.843750  (0.0, 100.0]           1.0\n",
       "1      49700.640625  49679.0  21.640625  (0.0, 100.0]           1.0\n",
       "2      54594.113281  54567.0  27.113281  (0.0, 100.0]           1.0\n",
       "3      56127.300781  56139.0 -11.699219  (0.0, 100.0]           1.0\n",
       "4      62322.390625  62291.0  31.390625  (0.0, 100.0]           1.0\n",
       "...             ...      ...        ...           ...           ...\n",
       "11995  59880.917969  59847.0  33.917969  (0.0, 100.0]           1.0\n",
       "11996  57777.601562  57746.0  31.601562  (0.0, 100.0]           1.0\n",
       "11997  58016.804688  57982.0  34.804688  (0.0, 100.0]           1.0\n",
       "11998  57104.230469  57080.0  24.230469  (0.0, 100.0]           1.0\n",
       "11999  56720.238281  56720.0   0.238281  (0.0, 100.0]           1.0\n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MaxAssBurnupCal')\n",
    "print(\"\")\n",
    "def CalError(pred, test):\n",
    "    \n",
    "    col_name = []\n",
    "    for col in pred.columns:\n",
    "        col_name.append(col+'_pred')\n",
    "    for col in test.columns:\n",
    "        col_name.append(col+'_test')  \n",
    "    \n",
    "    pred.index = test.index\n",
    "    pred_error = pd.concat([pred,test], axis=1)\n",
    "    print(pred_error)\n",
    "    pred_error.columns = col_name\n",
    "    \n",
    "    error_col = []\n",
    "    for col in pred.columns:\n",
    "        pred_error[col + '_error'] = np.array(pred[col]) - np.array(test[col])\n",
    "        error_col.append(col + '_error')\n",
    "    bin_range = np.linspace(0,1000,11).tolist() + [2000, 3000, 4000, 5000, 10000]\n",
    "    bin_label = np.linspace(1,15,15).tolist()\n",
    "    \n",
    "    range_col = []\n",
    "    for col in error_col:\n",
    "        pred_error[col+'_range'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range\n",
    "                                        )\n",
    "        range_col.append(col+'_range')\n",
    "        pred_error[col+'_label'] = pd.cut(\n",
    "                                        abs(np.array(pred_error[col])),\n",
    "                                        bins=bin_range,\n",
    "                                        labels=bin_label\n",
    "                                        )\n",
    "        range_col.append(col+'_label')\n",
    "        \n",
    "    for col in [c for c in range_col if '_label' not in c]:\n",
    "        print('{}特征误差范围及统计个数'.format(col))\n",
    "        print(pred_error[col].value_counts())\n",
    "        \n",
    "    return pred_error\n",
    "        \n",
    "pred_error = CalError(pred_test[output_col], y_test_1[output_col])\n",
    "pred_error\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 19 11:26:30 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 66%   82C    P2   139W / 175W |   2996MiB /  7979MiB |     89%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       918      G   /usr/lib/xorg/Xorg                           176MiB |\r\n",
      "|    0      1950      G   /usr/bin/gnome-shell                         125MiB |\r\n",
      "|    0      3977      C   ...exhang/anaconda3/envs/tf_gpu/bin/python  2667MiB |\r\n",
      "|    0     18318      G   /usr/local/sunlogin/bin/sunloginclient         7MiB |\r\n",
      "|    0     23353      G   /usr/lib/firefox/firefox                       2MiB |\r\n",
      "|    0     32188      G   /usr/lib/firefox/firefox                       2MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
